{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#LSTM-Lecture\" data-toc-modified-id=\"LSTM-Lecture-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>LSTM Lecture</a></div><div class=\"lev1 toc-item\"><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Reshaping-the-Data-2-(X-:-Multiple,-Y-:-Multiple)\" data-toc-modified-id=\"Reshaping-the-Data-2-(X-:-Multiple,-Y-:-Multiple)-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reshaping the Data 2 (X : Multiple, Y : Multiple)</a></div><div class=\"lev3 toc-item\"><a href=\"#Dimension\" data-toc-modified-id=\"Dimension-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Dimension</a></div><div class=\"lev3 toc-item\"><a href=\"#Assignment\" data-toc-modified-id=\"Assignment-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Assignment</a></div><div class=\"lev3 toc-item\"><a href=\"#Reshaping-:-1D-to-2D-(for-MinMaxScaler)\" data-toc-modified-id=\"Reshaping-:-1D-to-2D-(for-MinMaxScaler)-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Reshaping : 1D to 2D (for <code>MinMaxScaler</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#Scaling-:-MinMax,-0-~-1\" data-toc-modified-id=\"Scaling-:-MinMax,-0-~-1-214\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Scaling : <code>MinMax</code>, 0 ~ 1</a></div><div class=\"lev3 toc-item\"><a href=\"#Reshaping-X:-2D-to-3D,-(Samples,-Timestep-Sequence,-Features)\" data-toc-modified-id=\"Reshaping-X:-2D-to-3D,-(Samples,-Timestep-Sequence,-Features)-215\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Reshaping <code>X</code>: 2D to 3D, (Samples, Timestep-Sequence, Features)</a></div><div class=\"lev3 toc-item\"><a href=\"#Padding\" data-toc-modified-id=\"Padding-216\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Padding</a></div><div class=\"lev3 toc-item\"><a href=\"#Start-&amp;-End-Marking\" data-toc-modified-id=\"Start-&amp;-End-Marking-217\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span>Start &amp; End Marking</a></div><div class=\"lev3 toc-item\"><a href=\"#Splitting-(Train-&amp;-Test)\" data-toc-modified-id=\"Splitting-(Train-&amp;-Test)-218\"><span class=\"toc-item-num\">2.1.8&nbsp;&nbsp;</span>Splitting (Train &amp; Test)</a></div><div class=\"lev1 toc-item\"><a href=\"#Encoder-Decoder-1-(RepeatVector)\" data-toc-modified-id=\"Encoder-Decoder-1-(RepeatVector)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Encoder-Decoder 1 (RepeatVector)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-303\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-304\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#Encoder-Decoder-2-(Seq2Seq)---It-can-be-used-for-Text-Summarization\" data-toc-modified-id=\"Encoder-Decoder-2-(Seq2Seq)---It-can-be-used-for-Text-Summarization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Encoder-Decoder 2 (Seq2Seq) - It can be used for Text Summarization</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-401\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-402\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-403\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-404\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model\" data-toc-modified-id=\"Inference-Model-4041\"><span class=\"toc-item-num\">4.0.4.1&nbsp;&nbsp;</span>Inference Model</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model-within-a-Function\" data-toc-modified-id=\"Inference-Model-within-a-Function-4042\"><span class=\"toc-item-num\">4.0.4.2&nbsp;&nbsp;</span>Inference Model within a Function</a></div><div class=\"lev1 toc-item\"><a href=\"#Stateful\" data-toc-modified-id=\"Stateful-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stateful</a></div><div class=\"lev1 toc-item\"><a href=\"#RNN-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"RNN-Decoder-with-Attention-(encoder:-return_sequence=True)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>RNN Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyRNNAttention-(Feed-Forward)\" data-toc-modified-id=\"MyRNNAttention-(Feed-Forward)-601\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>MyRNNAttention (Feed-Forward)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-602\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-603\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-604\"><span class=\"toc-item-num\">6.0.4&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-605\"><span class=\"toc-item-num\">6.0.5&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#LSTM-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"LSTM-Decoder-with-Attention-(encoder:-return_sequence=True)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>LSTM Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyLSTMAttention-(Feed-Forward)\" data-toc-modified-id=\"MyLSTMAttention-(Feed-Forward)-701\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>MyLSTMAttention (Feed-Forward)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-702\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-703\"><span class=\"toc-item-num\">7.0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-704\"><span class=\"toc-item-num\">7.0.4&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-705\"><span class=\"toc-item-num\">7.0.5&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#GRU-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"GRU-Decoder-with-Attention-(encoder:-return_sequence=True)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>GRU Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyGRUAttention-(Feed-Forward,-Not-Recurrent)\" data-toc-modified-id=\"MyGRUAttention-(Feed-Forward,-Not-Recurrent)-801\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span>MyGRUAttention (Feed-Forward, Not Recurrent)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-802\"><span class=\"toc-item-num\">8.0.2&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-803\"><span class=\"toc-item-num\">8.0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-804\"><span class=\"toc-item-num\">8.0.4&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-805\"><span class=\"toc-item-num\">8.0.5&nbsp;&nbsp;</span>Testing</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "\n",
    "```txt\n",
    "xt = xt\n",
    "ht = fw(ht-1, xt)  \n",
    "   = tanh(Whh * ht-1 + Wxh * xt)\n",
    "\n",
    "ot = Why * ht\n",
    "   = Why * tanh(Whh * ht-1 + Wxh * xt)\n",
    "   \n",
    "yt = argmax(softmax(ot))\n",
    "or\n",
    "yt = sigmoid(ot)\n",
    "   \n",
    "```\n",
    "\n",
    "```txt\n",
    "weights : Wxh, Whh, Wyh\n",
    "biases  : bxh, bhh, byh\n",
    "```\n",
    "\n",
    "* LSTM\n",
    "\n",
    "```txt\n",
    "xt = xt\n",
    "ht = fw(ht-1, xt)  \n",
    "   = tanh(Whh * ht-1 + Wxh * xt)\n",
    "\n",
    "yt = Why * ht\n",
    "   = Why * tanh(Whh * ht-1 + Wxh * xt)\n",
    "```\n",
    "\n",
    "```txt\n",
    "weights : Wxh, Whh, Wyh\n",
    "biases  : bxh, bhh, byh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recurrent Neural Networks, we are quickly confronted to the so-called __gradient vanishing problem__:\n",
    "\n",
    "In machine learning, __the vanishing gradient problem__ is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation.  \n",
    "In such methods, each of the neural network’s weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training.   \n",
    "_Traditional activation functions such as the hyperbolic tangent function have gradients in the range `(−1,1)` or `(0,1)`_, and backpropagation computes gradients by the chain rule.  \n",
    "This has the effect of multiplying n of these small numbers to compute gradients of the “front” layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.\n",
    "\n",
    "One solution is __to consider *adding the updates* instead of multiplying them__, and this is exactly what the LSTM does. The state of every cell is updated in an additive way (Equation 9) such that the gradient hardly vanishes.\n",
    "\n",
    ">선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다.  \n",
    ">그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "* Input  Gate  \n",
    "* Forget Gate  \n",
    "* Cell   State\n",
    "* Output Gate \n",
    "* Hidden State \n",
    "\n",
    "![lstm](keras_stateful_lstm_2.png)\n",
    "![lstm](lstm_basic.png)\n",
    "![lstm](lstm_module.jpg)\n",
    "![](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm'](lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime as dt\n",
    "import itertools as it\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import SimpleRNN, RNN, LSTM, GRU, Dense, Flatten\n",
    "from keras.callbacks import Callback, LambdaCallback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (TimeDistributed, Embedding, RepeatVector,\n",
    "                          Permute, Lambda, Bidirectional)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Reshape, dot, multiply, concatenate, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.weights = []\n",
    "        self.states = []\n",
    "\n",
    "#    def on_batch_begin(self, batch, logs={}):\n",
    "#        self.weights.append([{'begin_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.weights.append([{'end_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "        \n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[0].get_weights()))\n",
    "#print_outputs = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].output))\n",
    "#print_states = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>22.5</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>25.5</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>31.5</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>34.5</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>43.5</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>106.5</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.87</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>109.5</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.08</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>112.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.36</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>115.5</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.47</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>118.5</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.61</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>121.5</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>124.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>126.0</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>127.5</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>130.5</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>133.5</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>136.5</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>139.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.07</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>144.0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>145.5</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>147.0</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.34</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>148.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    col1   col2  col3  col4  col5\n",
       "0      0    0.0  1.00  1.00   0.0\n",
       "1      1    1.5  0.54 -0.42   0.2\n",
       "2      2    3.0 -0.42 -0.65   0.4\n",
       "3      3    4.5 -0.99  0.96   0.6\n",
       "4      4    6.0 -0.65 -0.15   0.8\n",
       "5      5    7.5  0.28 -0.84   1.0\n",
       "6      6    9.0  0.96  0.84   1.2\n",
       "7      7   10.5  0.75  0.14   1.4\n",
       "8      8   12.0 -0.15 -0.96   1.6\n",
       "9      9   13.5 -0.91  0.66   1.8\n",
       "10    10   15.0 -0.84  0.41   2.0\n",
       "11    11   16.5  0.00 -1.00   2.2\n",
       "12    12   18.0  0.84  0.42   2.4\n",
       "13    13   19.5  0.91  0.65   2.6\n",
       "14    14   21.0  0.14 -0.96   2.8\n",
       "15    15   22.5 -0.76  0.15   3.0\n",
       "16    16   24.0 -0.96  0.83   3.2\n",
       "17    17   25.5 -0.28 -0.85   3.4\n",
       "18    18   27.0  0.66 -0.13   3.6\n",
       "19    19   28.5  0.99  0.96   3.8\n",
       "20    20   30.0  0.41 -0.67   4.0\n",
       "21    21   31.5 -0.55 -0.40   4.2\n",
       "22    22   33.0 -1.00  1.00   4.4\n",
       "23    23   34.5 -0.53 -0.43   4.6\n",
       "24    24   36.0  0.42 -0.64   4.8\n",
       "25    25   37.5  0.99  0.96   5.0\n",
       "26    26   39.0  0.65 -0.16   5.2\n",
       "27    27   40.5 -0.29 -0.83   5.4\n",
       "28    28   42.0 -0.96  0.85   5.6\n",
       "29    29   43.5 -0.75  0.12   5.8\n",
       "..   ...    ...   ...   ...   ...\n",
       "70    70  105.0  0.63 -0.20  14.0\n",
       "71    71  106.5 -0.31 -0.81  14.2\n",
       "72    72  108.0 -0.97  0.87  14.4\n",
       "73    73  109.5 -0.74  0.08  14.6\n",
       "74    74  111.0  0.17 -0.94  14.8\n",
       "75    75  112.5  0.92  0.70  15.0\n",
       "76    76  114.0  0.82  0.36  15.2\n",
       "77    77  115.5 -0.03 -1.00  15.4\n",
       "78    78  117.0 -0.86  0.47  15.6\n",
       "79    79  118.5 -0.90  0.61  15.8\n",
       "80    80  120.0 -0.11 -0.98  16.0\n",
       "81    81  121.5  0.78  0.21  16.2\n",
       "82    82  123.0  0.95  0.80  16.4\n",
       "83    83  124.5  0.25 -0.88  16.6\n",
       "84    84  126.0 -0.68 -0.08  16.8\n",
       "85    85  127.5 -0.98  0.94  17.0\n",
       "86    86  129.0 -0.38 -0.71  17.2\n",
       "87    87  130.5  0.57 -0.35  17.4\n",
       "88    88  132.0  1.00  1.00  17.6\n",
       "89    89  133.5  0.51 -0.48  17.8\n",
       "90    90  135.0 -0.45 -0.60  18.0\n",
       "91    91  136.5 -0.99  0.98  18.2\n",
       "92    92  138.0 -0.63 -0.22  18.4\n",
       "93    93  139.5  0.32 -0.80  18.6\n",
       "94    94  141.0  0.97  0.88  18.8\n",
       "95    95  142.5  0.73  0.07  19.0\n",
       "96    96  144.0 -0.18 -0.93  19.2\n",
       "97    97  145.5 -0.93  0.71  19.4\n",
       "98    98  147.0 -0.82  0.34  19.6\n",
       "99    99  148.5  0.04 -1.00  19.8\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = 100\n",
    "data = pd.DataFrame({'col1': np.arange(data_len),\n",
    "                     'col2': np.arange(data_len) * 1.5,\n",
    "                     'col3': np.round(np.cos(np.arange(data_len)), 2),\n",
    "                     'col4': np.round(np.cos(np.arange(data_len) * 2), 2),\n",
    "                     'col5': np.arange(0, data_len*.2, .2)})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f21e4699588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21e4925b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Data 2 (X : Multiple, Y : Multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfeature = 3\n",
    "yfeature = 2\n",
    "look_back = timestep = xlen = 4\n",
    "foresight = 1\n",
    "ylen = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  1.5       ,  0.54000002],\n",
       "       [ 2.        ,  3.        , -0.41999999],\n",
       "       [ 3.        ,  4.5       , -0.99000001],\n",
       "       [ 4.        ,  6.        , -0.64999998]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_1d = data.iloc[:, :xfeature].values.astype('float32')\n",
    "print(data_X_1d.shape)\n",
    "data_X_1d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ],\n",
       "       [-0.41999999,  0.2       ],\n",
       "       [-0.64999998,  0.40000001],\n",
       "       [ 0.95999998,  0.60000002],\n",
       "       [-0.15000001,  0.80000001]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y_1d = data.iloc[:, -yfeature:].values.astype('float32')\n",
    "print(data_Y_1d.shape)\n",
    "data_Y_1d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping : 1D to 2D (for `MinMaxScaler`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  1.5       ,  0.54000002],\n",
       "       [ 2.        ,  3.        , -0.41999999],\n",
       "       [ 3.        ,  4.5       , -0.99000001],\n",
       "       [ 4.        ,  6.        , -0.64999998]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_2d = data_X_1d.reshape(-1, xfeature)\n",
    "print(data_X_2d.shape)\n",
    "data_X_2d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ],\n",
       "       [-0.41999999,  0.2       ],\n",
       "       [-0.64999998,  0.40000001],\n",
       "       [ 0.95999998,  0.60000002],\n",
       "       [-0.15000001,  0.80000001]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y_2d = data_Y_1d.reshape(-1, yfeature)\n",
    "print(data_Y_2d.shape)\n",
    "data_Y_2d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling : `MinMax`, 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.        ]\n",
      " [ 0.01010101  0.01010101  0.76999998]\n",
      " [ 0.02020202  0.02020202  0.29000002]\n",
      " [ 0.03030303  0.03030303  0.005     ]\n",
      " [ 0.04040404  0.04040404  0.17500001]]\n",
      "[[ 1.          0.        ]\n",
      " [ 0.29000002  0.01010101]\n",
      " [ 0.17500001  0.02020202]\n",
      " [ 0.98000002  0.03030303]\n",
      " [ 0.42500001  0.04040404]]\n"
     ]
    }
   ],
   "source": [
    "scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "scalerY = MinMaxScaler(feature_range=(0, 1))  # sigmoid(0, 1), tanh(-1, 1)\n",
    "\n",
    "scaled_X = scalerX.fit_transform(data_X_2d)\n",
    "scaled_Y = scalerY.fit_transform(data_Y_2d)\n",
    "\n",
    "print(scaled_X[:5])\n",
    "print(scaled_Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping `X`: 2D to 3D, (Samples, Timestep-Sequence, Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scaled_X) - xlen - foresight - ylen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (94, 4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  1.        ],\n",
       "        [ 0.01010101,  0.01010101,  0.76999998],\n",
       "        [ 0.02020202,  0.02020202,  0.29000002],\n",
       "        [ 0.03030303,  0.03030303,  0.005     ]],\n",
       "\n",
       "       [[ 0.01010101,  0.01010101,  0.76999998],\n",
       "        [ 0.02020202,  0.02020202,  0.29000002],\n",
       "        [ 0.03030303,  0.03030303,  0.005     ],\n",
       "        [ 0.04040404,  0.04040404,  0.17500001]],\n",
       "\n",
       "       [[ 0.02020202,  0.02020202,  0.29000002],\n",
       "        [ 0.03030303,  0.03030303,  0.005     ],\n",
       "        [ 0.04040404,  0.04040404,  0.17500001],\n",
       "        [ 0.05050505,  0.05050505,  0.63999999]],\n",
       "\n",
       "       [[ 0.03030303,  0.03030303,  0.005     ],\n",
       "        [ 0.04040404,  0.04040404,  0.17500001],\n",
       "        [ 0.05050505,  0.05050505,  0.63999999],\n",
       "        [ 0.06060606,  0.06060606,  0.98000002]],\n",
       "\n",
       "       [[ 0.04040404,  0.04040404,  0.17500001],\n",
       "        [ 0.05050505,  0.05050505,  0.63999999],\n",
       "        [ 0.06060606,  0.06060606,  0.98000002],\n",
       "        [ 0.07070707,  0.07070707,  0.875     ]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_X = np.array([scaled_X[i:i+xlen] for i in range(0, len(scaled_X) - xlen - (foresight - 1) - (ylen - 1))])\n",
    "print('X Shape:', seq_X.shape)\n",
    "seq_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Shape: (94, 3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.42500001,  0.04040404],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607]],\n",
       "\n",
       "       [[ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708]],\n",
       "\n",
       "       [[ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809]],\n",
       "\n",
       "       [[ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909]],\n",
       "\n",
       "       [[ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909],\n",
       "        [ 0.70499998,  0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_Y = np.array([scaled_Y[i:i+ylen] for i in range(xlen + (foresight - 1), len(scaled_Y) - (ylen - 1))])\n",
    "print('Y Shape:', seq_Y.shape)\n",
    "seq_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(xlen, ylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_X = pad_sequences(seq_X, maxlen=max_len, dtype='float32', padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ],\n",
       "        [ 0.42500001,  0.04040404],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909],\n",
       "        [ 0.70499998,  0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq_Y = pad_sequences(seq_Y, maxlen=max_len, dtype='float32', padding='pre')\n",
    "padded_seq_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start & End Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 3, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_Y  = pad_sequences(seq_Y, maxlen=1+ylen, dtype='float32', padding='pre', value=0.)\n",
    "decoder_input_Y  = pad_sequences(decoder_input_Y, maxlen=max_len, dtype='float32', padding='post', value=0.)\n",
    "decoder_target_Y = pad_sequences(seq_Y, maxlen=max_len, dtype='float32', padding='post', value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ],\n",
       "        [ 0.42500001,  0.04040404],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909]],\n",
       "\n",
       "       [[ 0.        ,  0.        ],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909],\n",
       "        [ 0.70499998,  0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.42500001,  0.04040404],\n",
       "        [ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.08000001,  0.05050505],\n",
       "        [ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.91999996,  0.06060607],\n",
       "        [ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.56999999,  0.07070708],\n",
       "        [ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.02000001,  0.08080809],\n",
       "        [ 0.83000004,  0.09090909],\n",
       "        [ 0.70499998,  0.10101011],\n",
       "        [ 0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting (Train & Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 4, 3) (65, 4, 2)\n",
      "Train_X\t: (65, 4, 3)\n",
      "Train_Y\t: (65, 4, 2)\n",
      "Test_X\t: (29, 4, 3)\n",
      "Test_Y\t: (29, 4, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = .3\n",
    "\n",
    "(train_X, test_X,\n",
    " train_Y, test_Y) = train_test_split(seq_X, padded_seq_Y,\n",
    "                                    test_size=test_size,\n",
    "                                    shuffle=False,\n",
    "                                    random_state=99)\n",
    "\n",
    "(train_decoder_input_Y,\n",
    " test_decoder_input_Y,\n",
    " train_decoder_target_Y,\n",
    " test_decoder_target_Y) = train_test_split(decoder_input_Y,\n",
    "                                           decoder_target_Y,\n",
    "                                           test_size=test_size,\n",
    "                                           shuffle=False,\n",
    "                                           random_state=99)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print('Train_X\\t: %s\\nTrain_Y\\t: %s\\nTest_X\\t: %s\\nTest_Y\\t: %s\\n' % \n",
    "      (train_X.shape, train_Y.shape, test_X.shape, test_Y.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# apt-get install -y graphviz libgraphviz-dev\n",
    "keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder-Decoder 1 (RepeatVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_seq_to_seq_1'](lstm_encdec_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_seq_to_seq_1'](seq2seq_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 16)                1280      \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 16)             2112      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 2)              34        \n",
      "=================================================================\n",
      "Total params: 3,426\n",
      "Trainable params: 3,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "# simple lstm network learning\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "2D: (batch_size, units)\n",
    "3D: (batch_size, timesteps, input_dim)\n",
    "\"\"\"\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=False,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "model.add(RepeatVector(timestepX))\n",
    "\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=True,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "# TimeDistributed to compare the predicted with the real one, sequence by sequence\n",
    "model.add(TimeDistributed(Dense(ndimY,  # Network Node\n",
    "                                input_shape=(ylen, ndimY),\n",
    "                                activation='linear'),))\n",
    "\n",
    "#model.add(Dense(ndimY,  # Network Node\n",
    "#                input_shape=(ylen, ndimY),  # Time-step, Feature Number\n",
    "#                activation='linear',\n",
    "#                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),\n",
    "#                bias_initializer=keras.initializers.Constant(value=0),))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/300\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.1884 - mean_absolute_error: 0.3066 - val_loss: 0.2900 - val_mean_absolute_error: 0.4262\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.1841 - mean_absolute_error: 0.3015 - val_loss: 0.2838 - val_mean_absolute_error: 0.4207\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.1793 - mean_absolute_error: 0.2962 - val_loss: 0.2771 - val_mean_absolute_error: 0.4156\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.1742 - mean_absolute_error: 0.2917 - val_loss: 0.2701 - val_mean_absolute_error: 0.4106\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.1688 - mean_absolute_error: 0.2870 - val_loss: 0.2628 - val_mean_absolute_error: 0.4054\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.1634 - mean_absolute_error: 0.2826 - val_loss: 0.2550 - val_mean_absolute_error: 0.3999\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.1577 - mean_absolute_error: 0.2780 - val_loss: 0.2469 - val_mean_absolute_error: 0.3941\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.1518 - mean_absolute_error: 0.2732 - val_loss: 0.2384 - val_mean_absolute_error: 0.3882\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.1458 - mean_absolute_error: 0.2683 - val_loss: 0.2295 - val_mean_absolute_error: 0.3819\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.1395 - mean_absolute_error: 0.2634 - val_loss: 0.2200 - val_mean_absolute_error: 0.3752\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.1330 - mean_absolute_error: 0.2584 - val_loss: 0.2100 - val_mean_absolute_error: 0.3677\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.1262 - mean_absolute_error: 0.2533 - val_loss: 0.1994 - val_mean_absolute_error: 0.3598\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.1192 - mean_absolute_error: 0.2480 - val_loss: 0.1883 - val_mean_absolute_error: 0.3520\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.1120 - mean_absolute_error: 0.2426 - val_loss: 0.1766 - val_mean_absolute_error: 0.3438\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.1047 - mean_absolute_error: 0.2371 - val_loss: 0.1645 - val_mean_absolute_error: 0.3357\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0973 - mean_absolute_error: 0.2314 - val_loss: 0.1521 - val_mean_absolute_error: 0.3274\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0899 - mean_absolute_error: 0.2259 - val_loss: 0.1397 - val_mean_absolute_error: 0.3197\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0829 - mean_absolute_error: 0.2206 - val_loss: 0.1279 - val_mean_absolute_error: 0.3120\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0766 - mean_absolute_error: 0.2158 - val_loss: 0.1171 - val_mean_absolute_error: 0.3058\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0713 - mean_absolute_error: 0.2117 - val_loss: 0.1081 - val_mean_absolute_error: 0.2998\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0675 - mean_absolute_error: 0.2086 - val_loss: 0.1018 - val_mean_absolute_error: 0.2940\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0657 - mean_absolute_error: 0.2072 - val_loss: 0.0985 - val_mean_absolute_error: 0.2892\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0658 - mean_absolute_error: 0.2078 - val_loss: 0.0976 - val_mean_absolute_error: 0.2848\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 469us/step - loss: 0.0672 - mean_absolute_error: 0.2101 - val_loss: 0.0976 - val_mean_absolute_error: 0.2809\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0687 - mean_absolute_error: 0.2126 - val_loss: 0.0970 - val_mean_absolute_error: 0.2777\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0693 - mean_absolute_error: 0.2141 - val_loss: 0.0952 - val_mean_absolute_error: 0.2755\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0688 - mean_absolute_error: 0.2141 - val_loss: 0.0930 - val_mean_absolute_error: 0.2744\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0678 - mean_absolute_error: 0.2132 - val_loss: 0.0912 - val_mean_absolute_error: 0.2740\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0668 - mean_absolute_error: 0.2120 - val_loss: 0.0902 - val_mean_absolute_error: 0.2744\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0661 - mean_absolute_error: 0.2108 - val_loss: 0.0900 - val_mean_absolute_error: 0.2751\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0658 - mean_absolute_error: 0.2098 - val_loss: 0.0903 - val_mean_absolute_error: 0.2758\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0658 - mean_absolute_error: 0.2093 - val_loss: 0.0907 - val_mean_absolute_error: 0.2763\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0659 - mean_absolute_error: 0.2090 - val_loss: 0.0910 - val_mean_absolute_error: 0.2767\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0660 - mean_absolute_error: 0.2087 - val_loss: 0.0911 - val_mean_absolute_error: 0.2767\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0661 - mean_absolute_error: 0.2085 - val_loss: 0.0909 - val_mean_absolute_error: 0.2763\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0661 - mean_absolute_error: 0.2084 - val_loss: 0.0905 - val_mean_absolute_error: 0.2757\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0660 - mean_absolute_error: 0.2083 - val_loss: 0.0900 - val_mean_absolute_error: 0.2748\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 410us/step - loss: 0.0659 - mean_absolute_error: 0.2082 - val_loss: 0.0894 - val_mean_absolute_error: 0.2739\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0658 - mean_absolute_error: 0.2082 - val_loss: 0.0888 - val_mean_absolute_error: 0.2730\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0657 - mean_absolute_error: 0.2083 - val_loss: 0.0884 - val_mean_absolute_error: 0.2722\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0657 - mean_absolute_error: 0.2084 - val_loss: 0.0881 - val_mean_absolute_error: 0.2716\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0657 - mean_absolute_error: 0.2086 - val_loss: 0.0879 - val_mean_absolute_error: 0.2711\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0657 - mean_absolute_error: 0.2087 - val_loss: 0.0879 - val_mean_absolute_error: 0.2708\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0657 - mean_absolute_error: 0.2088 - val_loss: 0.0879 - val_mean_absolute_error: 0.2708\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0657 - mean_absolute_error: 0.2088 - val_loss: 0.0880 - val_mean_absolute_error: 0.2709\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0656 - mean_absolute_error: 0.2087 - val_loss: 0.0881 - val_mean_absolute_error: 0.2711\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0656 - mean_absolute_error: 0.2085 - val_loss: 0.0882 - val_mean_absolute_error: 0.2715\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0655 - mean_absolute_error: 0.2082 - val_loss: 0.0884 - val_mean_absolute_error: 0.2719\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0654 - mean_absolute_error: 0.2080 - val_loss: 0.0885 - val_mean_absolute_error: 0.2722\n",
      "Epoch 50/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0654 - mean_absolute_error: 0.2077 - val_loss: 0.0886 - val_mean_absolute_error: 0.2725\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0653 - mean_absolute_error: 0.2075 - val_loss: 0.0887 - val_mean_absolute_error: 0.2727\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0653 - mean_absolute_error: 0.2073 - val_loss: 0.0887 - val_mean_absolute_error: 0.2728\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0652 - mean_absolute_error: 0.2071 - val_loss: 0.0887 - val_mean_absolute_error: 0.2727\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0652 - mean_absolute_error: 0.2069 - val_loss: 0.0886 - val_mean_absolute_error: 0.2726\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0652 - mean_absolute_error: 0.2068 - val_loss: 0.0885 - val_mean_absolute_error: 0.2723\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0651 - mean_absolute_error: 0.2066 - val_loss: 0.0884 - val_mean_absolute_error: 0.2720\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0651 - mean_absolute_error: 0.2065 - val_loss: 0.0882 - val_mean_absolute_error: 0.2717\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0651 - mean_absolute_error: 0.2065 - val_loss: 0.0881 - val_mean_absolute_error: 0.2713\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0651 - mean_absolute_error: 0.2064 - val_loss: 0.0879 - val_mean_absolute_error: 0.2709\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0650 - mean_absolute_error: 0.2063 - val_loss: 0.0878 - val_mean_absolute_error: 0.2706\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0650 - mean_absolute_error: 0.2062 - val_loss: 0.0876 - val_mean_absolute_error: 0.2702\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0650 - mean_absolute_error: 0.2061 - val_loss: 0.0875 - val_mean_absolute_error: 0.2699\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0650 - mean_absolute_error: 0.2060 - val_loss: 0.0874 - val_mean_absolute_error: 0.2697\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0650 - mean_absolute_error: 0.2059 - val_loss: 0.0873 - val_mean_absolute_error: 0.2694\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0649 - mean_absolute_error: 0.2057 - val_loss: 0.0872 - val_mean_absolute_error: 0.2692\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0649 - mean_absolute_error: 0.2056 - val_loss: 0.0871 - val_mean_absolute_error: 0.2690\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0649 - mean_absolute_error: 0.2054 - val_loss: 0.0870 - val_mean_absolute_error: 0.2688\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0648 - mean_absolute_error: 0.2053 - val_loss: 0.0869 - val_mean_absolute_error: 0.2686\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0648 - mean_absolute_error: 0.2051 - val_loss: 0.0868 - val_mean_absolute_error: 0.2684\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0648 - mean_absolute_error: 0.2050 - val_loss: 0.0867 - val_mean_absolute_error: 0.2681\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0648 - mean_absolute_error: 0.2049 - val_loss: 0.0866 - val_mean_absolute_error: 0.2679\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0647 - mean_absolute_error: 0.2047 - val_loss: 0.0865 - val_mean_absolute_error: 0.2676\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0647 - mean_absolute_error: 0.2046 - val_loss: 0.0864 - val_mean_absolute_error: 0.2673\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 402us/step - loss: 0.0647 - mean_absolute_error: 0.2045 - val_loss: 0.0863 - val_mean_absolute_error: 0.2670\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0646 - mean_absolute_error: 0.2043 - val_loss: 0.0862 - val_mean_absolute_error: 0.2668\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0646 - mean_absolute_error: 0.2042 - val_loss: 0.0860 - val_mean_absolute_error: 0.2665\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0646 - mean_absolute_error: 0.2041 - val_loss: 0.0859 - val_mean_absolute_error: 0.2662\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0646 - mean_absolute_error: 0.2039 - val_loss: 0.0858 - val_mean_absolute_error: 0.2660\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0645 - mean_absolute_error: 0.2038 - val_loss: 0.0857 - val_mean_absolute_error: 0.2657\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0645 - mean_absolute_error: 0.2037 - val_loss: 0.0856 - val_mean_absolute_error: 0.2655\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0645 - mean_absolute_error: 0.2035 - val_loss: 0.0856 - val_mean_absolute_error: 0.2652\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0644 - mean_absolute_error: 0.2034 - val_loss: 0.0855 - val_mean_absolute_error: 0.2650\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0644 - mean_absolute_error: 0.2032 - val_loss: 0.0854 - val_mean_absolute_error: 0.2647\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0644 - mean_absolute_error: 0.2031 - val_loss: 0.0853 - val_mean_absolute_error: 0.2645\n",
      "Epoch 85/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0643 - mean_absolute_error: 0.2029 - val_loss: 0.0852 - val_mean_absolute_error: 0.2642\n",
      "Epoch 86/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0643 - mean_absolute_error: 0.2028 - val_loss: 0.0851 - val_mean_absolute_error: 0.2640\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0643 - mean_absolute_error: 0.2026 - val_loss: 0.0850 - val_mean_absolute_error: 0.2637\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0643 - mean_absolute_error: 0.2025 - val_loss: 0.0849 - val_mean_absolute_error: 0.2634\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0642 - mean_absolute_error: 0.2024 - val_loss: 0.0848 - val_mean_absolute_error: 0.2632\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0642 - mean_absolute_error: 0.2022 - val_loss: 0.0847 - val_mean_absolute_error: 0.2629\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0642 - mean_absolute_error: 0.2021 - val_loss: 0.0846 - val_mean_absolute_error: 0.2626\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0641 - mean_absolute_error: 0.2019 - val_loss: 0.0845 - val_mean_absolute_error: 0.2623\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0641 - mean_absolute_error: 0.2018 - val_loss: 0.0844 - val_mean_absolute_error: 0.2620\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0641 - mean_absolute_error: 0.2016 - val_loss: 0.0843 - val_mean_absolute_error: 0.2617\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0640 - mean_absolute_error: 0.2015 - val_loss: 0.0842 - val_mean_absolute_error: 0.2615\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0640 - mean_absolute_error: 0.2013 - val_loss: 0.0841 - val_mean_absolute_error: 0.2612\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0640 - mean_absolute_error: 0.2012 - val_loss: 0.0840 - val_mean_absolute_error: 0.2609\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0640 - mean_absolute_error: 0.2010 - val_loss: 0.0838 - val_mean_absolute_error: 0.2606\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 378us/step - loss: 0.0639 - mean_absolute_error: 0.2009 - val_loss: 0.0837 - val_mean_absolute_error: 0.2603\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0639 - mean_absolute_error: 0.2007 - val_loss: 0.0836 - val_mean_absolute_error: 0.2600\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0639 - mean_absolute_error: 0.2006 - val_loss: 0.0835 - val_mean_absolute_error: 0.2597\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0638 - mean_absolute_error: 0.2004 - val_loss: 0.0834 - val_mean_absolute_error: 0.2594\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0638 - mean_absolute_error: 0.2003 - val_loss: 0.0833 - val_mean_absolute_error: 0.2591\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0638 - mean_absolute_error: 0.2001 - val_loss: 0.0832 - val_mean_absolute_error: 0.2588\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0637 - mean_absolute_error: 0.2000 - val_loss: 0.0831 - val_mean_absolute_error: 0.2585\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0637 - mean_absolute_error: 0.1998 - val_loss: 0.0830 - val_mean_absolute_error: 0.2582\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0637 - mean_absolute_error: 0.1997 - val_loss: 0.0829 - val_mean_absolute_error: 0.2579\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0637 - mean_absolute_error: 0.1995 - val_loss: 0.0828 - val_mean_absolute_error: 0.2576\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0636 - mean_absolute_error: 0.1994 - val_loss: 0.0827 - val_mean_absolute_error: 0.2573\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 425us/step - loss: 0.0636 - mean_absolute_error: 0.1992 - val_loss: 0.0826 - val_mean_absolute_error: 0.2570\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0636 - mean_absolute_error: 0.1990 - val_loss: 0.0825 - val_mean_absolute_error: 0.2567\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0635 - mean_absolute_error: 0.1989 - val_loss: 0.0824 - val_mean_absolute_error: 0.2564\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0635 - mean_absolute_error: 0.1987 - val_loss: 0.0823 - val_mean_absolute_error: 0.2561\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0635 - mean_absolute_error: 0.1986 - val_loss: 0.0822 - val_mean_absolute_error: 0.2558\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0634 - mean_absolute_error: 0.1984 - val_loss: 0.0821 - val_mean_absolute_error: 0.2555\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0634 - mean_absolute_error: 0.1983 - val_loss: 0.0820 - val_mean_absolute_error: 0.2552\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0634 - mean_absolute_error: 0.1981 - val_loss: 0.0819 - val_mean_absolute_error: 0.2549\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0633 - mean_absolute_error: 0.1979 - val_loss: 0.0818 - val_mean_absolute_error: 0.2546\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0633 - mean_absolute_error: 0.1978 - val_loss: 0.0817 - val_mean_absolute_error: 0.2543\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0633 - mean_absolute_error: 0.1976 - val_loss: 0.0816 - val_mean_absolute_error: 0.2539\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0632 - mean_absolute_error: 0.1975 - val_loss: 0.0815 - val_mean_absolute_error: 0.2536\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0632 - mean_absolute_error: 0.1973 - val_loss: 0.0814 - val_mean_absolute_error: 0.2533\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0632 - mean_absolute_error: 0.1972 - val_loss: 0.0813 - val_mean_absolute_error: 0.2530\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0632 - mean_absolute_error: 0.1970 - val_loss: 0.0812 - val_mean_absolute_error: 0.2527\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0631 - mean_absolute_error: 0.1968 - val_loss: 0.0811 - val_mean_absolute_error: 0.2524\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0631 - mean_absolute_error: 0.1967 - val_loss: 0.0810 - val_mean_absolute_error: 0.2520\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0631 - mean_absolute_error: 0.1965 - val_loss: 0.0809 - val_mean_absolute_error: 0.2517\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0630 - mean_absolute_error: 0.1964 - val_loss: 0.0808 - val_mean_absolute_error: 0.2514\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0630 - mean_absolute_error: 0.1962 - val_loss: 0.0807 - val_mean_absolute_error: 0.2511\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0630 - mean_absolute_error: 0.1961 - val_loss: 0.0806 - val_mean_absolute_error: 0.2507\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0629 - mean_absolute_error: 0.1959 - val_loss: 0.0805 - val_mean_absolute_error: 0.2504\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0629 - mean_absolute_error: 0.1957 - val_loss: 0.0804 - val_mean_absolute_error: 0.2501\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0629 - mean_absolute_error: 0.1956 - val_loss: 0.0803 - val_mean_absolute_error: 0.2498\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0628 - mean_absolute_error: 0.1954 - val_loss: 0.0802 - val_mean_absolute_error: 0.2494\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0628 - mean_absolute_error: 0.1953 - val_loss: 0.0801 - val_mean_absolute_error: 0.2491\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0628 - mean_absolute_error: 0.1951 - val_loss: 0.0800 - val_mean_absolute_error: 0.2488\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0627 - mean_absolute_error: 0.1950 - val_loss: 0.0799 - val_mean_absolute_error: 0.2484\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0627 - mean_absolute_error: 0.1948 - val_loss: 0.0798 - val_mean_absolute_error: 0.2481\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0627 - mean_absolute_error: 0.1947 - val_loss: 0.0797 - val_mean_absolute_error: 0.2478\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0626 - mean_absolute_error: 0.1945 - val_loss: 0.0796 - val_mean_absolute_error: 0.2474\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0626 - mean_absolute_error: 0.1944 - val_loss: 0.0795 - val_mean_absolute_error: 0.2471\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0626 - mean_absolute_error: 0.1942 - val_loss: 0.0794 - val_mean_absolute_error: 0.2468\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0625 - mean_absolute_error: 0.1941 - val_loss: 0.0793 - val_mean_absolute_error: 0.2464\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0625 - mean_absolute_error: 0.1939 - val_loss: 0.0792 - val_mean_absolute_error: 0.2461\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0625 - mean_absolute_error: 0.1938 - val_loss: 0.0791 - val_mean_absolute_error: 0.2458\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0624 - mean_absolute_error: 0.1936 - val_loss: 0.0790 - val_mean_absolute_error: 0.2454\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0624 - mean_absolute_error: 0.1935 - val_loss: 0.0789 - val_mean_absolute_error: 0.2451\n",
      "Epoch 148/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0624 - mean_absolute_error: 0.1933 - val_loss: 0.0788 - val_mean_absolute_error: 0.2447\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0624 - mean_absolute_error: 0.1932 - val_loss: 0.0787 - val_mean_absolute_error: 0.2444\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0623 - mean_absolute_error: 0.1930 - val_loss: 0.0786 - val_mean_absolute_error: 0.2441\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0623 - mean_absolute_error: 0.1929 - val_loss: 0.0785 - val_mean_absolute_error: 0.2437\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0623 - mean_absolute_error: 0.1927 - val_loss: 0.0784 - val_mean_absolute_error: 0.2434\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0622 - mean_absolute_error: 0.1926 - val_loss: 0.0783 - val_mean_absolute_error: 0.2430\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0622 - mean_absolute_error: 0.1924 - val_loss: 0.0782 - val_mean_absolute_error: 0.2427\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0622 - mean_absolute_error: 0.1923 - val_loss: 0.0781 - val_mean_absolute_error: 0.2423\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0621 - mean_absolute_error: 0.1921 - val_loss: 0.0780 - val_mean_absolute_error: 0.2420\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0621 - mean_absolute_error: 0.1920 - val_loss: 0.0779 - val_mean_absolute_error: 0.2416\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0621 - mean_absolute_error: 0.1918 - val_loss: 0.0778 - val_mean_absolute_error: 0.2413\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0620 - mean_absolute_error: 0.1917 - val_loss: 0.0777 - val_mean_absolute_error: 0.2409\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0620 - mean_absolute_error: 0.1915 - val_loss: 0.0776 - val_mean_absolute_error: 0.2406\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0620 - mean_absolute_error: 0.1914 - val_loss: 0.0775 - val_mean_absolute_error: 0.2402\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0619 - mean_absolute_error: 0.1912 - val_loss: 0.0774 - val_mean_absolute_error: 0.2399\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0619 - mean_absolute_error: 0.1911 - val_loss: 0.0773 - val_mean_absolute_error: 0.2395\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0618 - mean_absolute_error: 0.1909 - val_loss: 0.0771 - val_mean_absolute_error: 0.2392\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0618 - mean_absolute_error: 0.1908 - val_loss: 0.0770 - val_mean_absolute_error: 0.2388\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0618 - mean_absolute_error: 0.1906 - val_loss: 0.0769 - val_mean_absolute_error: 0.2385\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0617 - mean_absolute_error: 0.1905 - val_loss: 0.0768 - val_mean_absolute_error: 0.2381\n",
      "Epoch 168/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0617 - mean_absolute_error: 0.1903 - val_loss: 0.0767 - val_mean_absolute_error: 0.2377\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0617 - mean_absolute_error: 0.1902 - val_loss: 0.0766 - val_mean_absolute_error: 0.2374\n",
      "Epoch 170/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0616 - mean_absolute_error: 0.1900 - val_loss: 0.0765 - val_mean_absolute_error: 0.2370\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0616 - mean_absolute_error: 0.1899 - val_loss: 0.0764 - val_mean_absolute_error: 0.2367\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0616 - mean_absolute_error: 0.1897 - val_loss: 0.0763 - val_mean_absolute_error: 0.2363\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0615 - mean_absolute_error: 0.1896 - val_loss: 0.0762 - val_mean_absolute_error: 0.2359\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0615 - mean_absolute_error: 0.1895 - val_loss: 0.0761 - val_mean_absolute_error: 0.2356\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0615 - mean_absolute_error: 0.1893 - val_loss: 0.0760 - val_mean_absolute_error: 0.2352\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0614 - mean_absolute_error: 0.1892 - val_loss: 0.0759 - val_mean_absolute_error: 0.2348\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0614 - mean_absolute_error: 0.1890 - val_loss: 0.0758 - val_mean_absolute_error: 0.2344\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0614 - mean_absolute_error: 0.1889 - val_loss: 0.0757 - val_mean_absolute_error: 0.2341\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0613 - mean_absolute_error: 0.1887 - val_loss: 0.0756 - val_mean_absolute_error: 0.2337\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 418us/step - loss: 0.0613 - mean_absolute_error: 0.1886 - val_loss: 0.0755 - val_mean_absolute_error: 0.2333\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0613 - mean_absolute_error: 0.1885 - val_loss: 0.0754 - val_mean_absolute_error: 0.2330\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0612 - mean_absolute_error: 0.1883 - val_loss: 0.0753 - val_mean_absolute_error: 0.2326\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0612 - mean_absolute_error: 0.1882 - val_loss: 0.0752 - val_mean_absolute_error: 0.2322\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0611 - mean_absolute_error: 0.1880 - val_loss: 0.0751 - val_mean_absolute_error: 0.2318\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0611 - mean_absolute_error: 0.1879 - val_loss: 0.0749 - val_mean_absolute_error: 0.2315\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0611 - mean_absolute_error: 0.1877 - val_loss: 0.0748 - val_mean_absolute_error: 0.2311\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0610 - mean_absolute_error: 0.1876 - val_loss: 0.0747 - val_mean_absolute_error: 0.2307\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0610 - mean_absolute_error: 0.1875 - val_loss: 0.0746 - val_mean_absolute_error: 0.2304\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0610 - mean_absolute_error: 0.1873 - val_loss: 0.0745 - val_mean_absolute_error: 0.2300\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0609 - mean_absolute_error: 0.1872 - val_loss: 0.0744 - val_mean_absolute_error: 0.2297\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0609 - mean_absolute_error: 0.1871 - val_loss: 0.0743 - val_mean_absolute_error: 0.2294\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0608 - mean_absolute_error: 0.1869 - val_loss: 0.0742 - val_mean_absolute_error: 0.2291\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0608 - mean_absolute_error: 0.1868 - val_loss: 0.0741 - val_mean_absolute_error: 0.2287\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0608 - mean_absolute_error: 0.1866 - val_loss: 0.0740 - val_mean_absolute_error: 0.2285\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0607 - mean_absolute_error: 0.1865 - val_loss: 0.0739 - val_mean_absolute_error: 0.2282\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0607 - mean_absolute_error: 0.1864 - val_loss: 0.0738 - val_mean_absolute_error: 0.2279\n",
      "Epoch 197/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 383us/step - loss: 0.0607 - mean_absolute_error: 0.1862 - val_loss: 0.0736 - val_mean_absolute_error: 0.2276\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0606 - mean_absolute_error: 0.1861 - val_loss: 0.0735 - val_mean_absolute_error: 0.2273\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0606 - mean_absolute_error: 0.1860 - val_loss: 0.0734 - val_mean_absolute_error: 0.2271\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0605 - mean_absolute_error: 0.1858 - val_loss: 0.0733 - val_mean_absolute_error: 0.2268\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0605 - mean_absolute_error: 0.1857 - val_loss: 0.0732 - val_mean_absolute_error: 0.2266\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0605 - mean_absolute_error: 0.1856 - val_loss: 0.0731 - val_mean_absolute_error: 0.2264\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0604 - mean_absolute_error: 0.1854 - val_loss: 0.0730 - val_mean_absolute_error: 0.2262\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0604 - mean_absolute_error: 0.1853 - val_loss: 0.0729 - val_mean_absolute_error: 0.2260\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0603 - mean_absolute_error: 0.1852 - val_loss: 0.0728 - val_mean_absolute_error: 0.2258\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0603 - mean_absolute_error: 0.1850 - val_loss: 0.0727 - val_mean_absolute_error: 0.2257\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0603 - mean_absolute_error: 0.1849 - val_loss: 0.0725 - val_mean_absolute_error: 0.2255\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0602 - mean_absolute_error: 0.1848 - val_loss: 0.0724 - val_mean_absolute_error: 0.2254\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0602 - mean_absolute_error: 0.1846 - val_loss: 0.0723 - val_mean_absolute_error: 0.2253\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0601 - mean_absolute_error: 0.1845 - val_loss: 0.0722 - val_mean_absolute_error: 0.2251\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0601 - mean_absolute_error: 0.1844 - val_loss: 0.0721 - val_mean_absolute_error: 0.2250\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0601 - mean_absolute_error: 0.1842 - val_loss: 0.0720 - val_mean_absolute_error: 0.2249\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0600 - mean_absolute_error: 0.1841 - val_loss: 0.0719 - val_mean_absolute_error: 0.2247\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0600 - mean_absolute_error: 0.1839 - val_loss: 0.0718 - val_mean_absolute_error: 0.2246\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0599 - mean_absolute_error: 0.1838 - val_loss: 0.0717 - val_mean_absolute_error: 0.2244\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0599 - mean_absolute_error: 0.1837 - val_loss: 0.0716 - val_mean_absolute_error: 0.2243\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0599 - mean_absolute_error: 0.1835 - val_loss: 0.0714 - val_mean_absolute_error: 0.2241\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0598 - mean_absolute_error: 0.1834 - val_loss: 0.0713 - val_mean_absolute_error: 0.2240\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0598 - mean_absolute_error: 0.1832 - val_loss: 0.0712 - val_mean_absolute_error: 0.2238\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0597 - mean_absolute_error: 0.1831 - val_loss: 0.0711 - val_mean_absolute_error: 0.2237\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0597 - mean_absolute_error: 0.1830 - val_loss: 0.0710 - val_mean_absolute_error: 0.2235\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0597 - mean_absolute_error: 0.1828 - val_loss: 0.0709 - val_mean_absolute_error: 0.2234\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0596 - mean_absolute_error: 0.1827 - val_loss: 0.0708 - val_mean_absolute_error: 0.2232\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0596 - mean_absolute_error: 0.1825 - val_loss: 0.0707 - val_mean_absolute_error: 0.2231\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0595 - mean_absolute_error: 0.1824 - val_loss: 0.0706 - val_mean_absolute_error: 0.2229\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0595 - mean_absolute_error: 0.1823 - val_loss: 0.0705 - val_mean_absolute_error: 0.2227\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0595 - mean_absolute_error: 0.1821 - val_loss: 0.0703 - val_mean_absolute_error: 0.2226\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0594 - mean_absolute_error: 0.1820 - val_loss: 0.0702 - val_mean_absolute_error: 0.2224\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0594 - mean_absolute_error: 0.1818 - val_loss: 0.0701 - val_mean_absolute_error: 0.2222\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0593 - mean_absolute_error: 0.1817 - val_loss: 0.0700 - val_mean_absolute_error: 0.2221\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0593 - mean_absolute_error: 0.1815 - val_loss: 0.0699 - val_mean_absolute_error: 0.2219\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0592 - mean_absolute_error: 0.1814 - val_loss: 0.0698 - val_mean_absolute_error: 0.2217\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0592 - mean_absolute_error: 0.1813 - val_loss: 0.0697 - val_mean_absolute_error: 0.2215\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0592 - mean_absolute_error: 0.1811 - val_loss: 0.0696 - val_mean_absolute_error: 0.2213\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0591 - mean_absolute_error: 0.1810 - val_loss: 0.0695 - val_mean_absolute_error: 0.2211\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0591 - mean_absolute_error: 0.1808 - val_loss: 0.0694 - val_mean_absolute_error: 0.2210\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0590 - mean_absolute_error: 0.1807 - val_loss: 0.0693 - val_mean_absolute_error: 0.2208\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0590 - mean_absolute_error: 0.1806 - val_loss: 0.0692 - val_mean_absolute_error: 0.2206\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0590 - mean_absolute_error: 0.1804 - val_loss: 0.0691 - val_mean_absolute_error: 0.2204\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0589 - mean_absolute_error: 0.1803 - val_loss: 0.0690 - val_mean_absolute_error: 0.2202\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0589 - mean_absolute_error: 0.1801 - val_loss: 0.0689 - val_mean_absolute_error: 0.2200\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0588 - mean_absolute_error: 0.1800 - val_loss: 0.0688 - val_mean_absolute_error: 0.2198\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0588 - mean_absolute_error: 0.1799 - val_loss: 0.0687 - val_mean_absolute_error: 0.2196\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 412us/step - loss: 0.0587 - mean_absolute_error: 0.1797 - val_loss: 0.0686 - val_mean_absolute_error: 0.2194\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0587 - mean_absolute_error: 0.1796 - val_loss: 0.0685 - val_mean_absolute_error: 0.2192\n",
      "Epoch 246/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0587 - mean_absolute_error: 0.1794 - val_loss: 0.0684 - val_mean_absolute_error: 0.2190\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 402us/step - loss: 0.0586 - mean_absolute_error: 0.1793 - val_loss: 0.0683 - val_mean_absolute_error: 0.2188\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0586 - mean_absolute_error: 0.1792 - val_loss: 0.0682 - val_mean_absolute_error: 0.2185\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0585 - mean_absolute_error: 0.1790 - val_loss: 0.0681 - val_mean_absolute_error: 0.2183\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0585 - mean_absolute_error: 0.1789 - val_loss: 0.0680 - val_mean_absolute_error: 0.2181\n",
      "Epoch 251/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0585 - mean_absolute_error: 0.1787 - val_loss: 0.0679 - val_mean_absolute_error: 0.2179\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0584 - mean_absolute_error: 0.1786 - val_loss: 0.0678 - val_mean_absolute_error: 0.2177\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0584 - mean_absolute_error: 0.1785 - val_loss: 0.0677 - val_mean_absolute_error: 0.2174\n",
      "Epoch 254/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0583 - mean_absolute_error: 0.1783 - val_loss: 0.0676 - val_mean_absolute_error: 0.2172\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0583 - mean_absolute_error: 0.1782 - val_loss: 0.0676 - val_mean_absolute_error: 0.2170\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0583 - mean_absolute_error: 0.1780 - val_loss: 0.0675 - val_mean_absolute_error: 0.2168\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0582 - mean_absolute_error: 0.1779 - val_loss: 0.0674 - val_mean_absolute_error: 0.2165\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0582 - mean_absolute_error: 0.1778 - val_loss: 0.0673 - val_mean_absolute_error: 0.2163\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0581 - mean_absolute_error: 0.1776 - val_loss: 0.0672 - val_mean_absolute_error: 0.2160\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0581 - mean_absolute_error: 0.1775 - val_loss: 0.0671 - val_mean_absolute_error: 0.2158\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0581 - mean_absolute_error: 0.1773 - val_loss: 0.0671 - val_mean_absolute_error: 0.2156\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0580 - mean_absolute_error: 0.1772 - val_loss: 0.0670 - val_mean_absolute_error: 0.2153\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0580 - mean_absolute_error: 0.1771 - val_loss: 0.0669 - val_mean_absolute_error: 0.2151\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0580 - mean_absolute_error: 0.1769 - val_loss: 0.0668 - val_mean_absolute_error: 0.2148\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0579 - mean_absolute_error: 0.1768 - val_loss: 0.0667 - val_mean_absolute_error: 0.2146\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0579 - mean_absolute_error: 0.1766 - val_loss: 0.0667 - val_mean_absolute_error: 0.2143\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0578 - mean_absolute_error: 0.1765 - val_loss: 0.0666 - val_mean_absolute_error: 0.2141\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0578 - mean_absolute_error: 0.1764 - val_loss: 0.0665 - val_mean_absolute_error: 0.2139\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0578 - mean_absolute_error: 0.1762 - val_loss: 0.0664 - val_mean_absolute_error: 0.2136\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0577 - mean_absolute_error: 0.1761 - val_loss: 0.0664 - val_mean_absolute_error: 0.2134\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0577 - mean_absolute_error: 0.1759 - val_loss: 0.0663 - val_mean_absolute_error: 0.2132\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0577 - mean_absolute_error: 0.1758 - val_loss: 0.0662 - val_mean_absolute_error: 0.2129\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0576 - mean_absolute_error: 0.1757 - val_loss: 0.0662 - val_mean_absolute_error: 0.2127\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0576 - mean_absolute_error: 0.1755 - val_loss: 0.0661 - val_mean_absolute_error: 0.2125\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0576 - mean_absolute_error: 0.1754 - val_loss: 0.0660 - val_mean_absolute_error: 0.2122\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0575 - mean_absolute_error: 0.1753 - val_loss: 0.0660 - val_mean_absolute_error: 0.2120\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0575 - mean_absolute_error: 0.1751 - val_loss: 0.0659 - val_mean_absolute_error: 0.2118\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0575 - mean_absolute_error: 0.1750 - val_loss: 0.0659 - val_mean_absolute_error: 0.2115\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0574 - mean_absolute_error: 0.1749 - val_loss: 0.0658 - val_mean_absolute_error: 0.2113\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0574 - mean_absolute_error: 0.1747 - val_loss: 0.0657 - val_mean_absolute_error: 0.2111\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0574 - mean_absolute_error: 0.1746 - val_loss: 0.0657 - val_mean_absolute_error: 0.2108\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0573 - mean_absolute_error: 0.1745 - val_loss: 0.0656 - val_mean_absolute_error: 0.2106\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0573 - mean_absolute_error: 0.1743 - val_loss: 0.0656 - val_mean_absolute_error: 0.2103\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0573 - mean_absolute_error: 0.1742 - val_loss: 0.0655 - val_mean_absolute_error: 0.2101\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0572 - mean_absolute_error: 0.1741 - val_loss: 0.0655 - val_mean_absolute_error: 0.2099\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0572 - mean_absolute_error: 0.1740 - val_loss: 0.0654 - val_mean_absolute_error: 0.2096\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0572 - mean_absolute_error: 0.1738 - val_loss: 0.0654 - val_mean_absolute_error: 0.2094\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0571 - mean_absolute_error: 0.1737 - val_loss: 0.0653 - val_mean_absolute_error: 0.2092\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0571 - mean_absolute_error: 0.1736 - val_loss: 0.0652 - val_mean_absolute_error: 0.2089\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 411us/step - loss: 0.0571 - mean_absolute_error: 0.1735 - val_loss: 0.0652 - val_mean_absolute_error: 0.2087\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0570 - mean_absolute_error: 0.1734 - val_loss: 0.0651 - val_mean_absolute_error: 0.2085\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0570 - mean_absolute_error: 0.1732 - val_loss: 0.0651 - val_mean_absolute_error: 0.2083\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 411us/step - loss: 0.0570 - mean_absolute_error: 0.1731 - val_loss: 0.0650 - val_mean_absolute_error: 0.2081\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0570 - mean_absolute_error: 0.1730 - val_loss: 0.0650 - val_mean_absolute_error: 0.2078\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 398us/step - loss: 0.0569 - mean_absolute_error: 0.1729 - val_loss: 0.0650 - val_mean_absolute_error: 0.2076\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0569 - mean_absolute_error: 0.1728 - val_loss: 0.0649 - val_mean_absolute_error: 0.2074\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0569 - mean_absolute_error: 0.1726 - val_loss: 0.0649 - val_mean_absolute_error: 0.2072\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0568 - mean_absolute_error: 0.1725 - val_loss: 0.0648 - val_mean_absolute_error: 0.2069\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0568 - mean_absolute_error: 0.1724 - val_loss: 0.0648 - val_mean_absolute_error: 0.2067\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0568 - mean_absolute_error: 0.1723 - val_loss: 0.0647 - val_mean_absolute_error: 0.2065\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAADFCAYAAAB5PKoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04d4f7b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.279 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 2.837 RMSE\n",
      "Real\t:\n",
      " [[ -1.           0.        ]\n",
      " [  0.71000004  19.39999962]\n",
      " [  0.34000003  19.60000038]\n",
      " [ -1.          19.79999924]],\n",
      "Predict\t:\n",
      " [[ -5.40345252e-01   8.34283257e+00]\n",
      " [ -1.88405752e-01   1.57305298e+01]\n",
      " [ -1.50645971e-02   1.99463196e+01]\n",
      " [  5.10774851e-02   2.18455505e+01]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in test_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[-1], test_Y_hat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder-Decoder 2 (Seq2Seq) - It can be used for Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)\n",
    "!['lstm_seq_to_seq_1'](lstm_encdec_2.png)\n",
    "!['lstm_seq_to_seq_1'](seq2seq_1.png)\n",
    "!['Use of a summary state in the encoder-decoder architecture'](lstm_attention_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 4, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 4, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decoder_input_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_decoder_target_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "num_encoder_tokens = ndimX\n",
    "latent_dim = 4\n",
    "num_decoder_tokens = ndimY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Encoder_LSTM/kernel:0' shape=(3, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/recurrent_kernel:0' shape=(4, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/bias:0' shape=(16,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/kernel:0' shape=(2, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/recurrent_kernel:0' shape=(4, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/bias:0' shape=(16,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/kernel:0' shape=(4, 2) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/bias:0' shape=(2,) dtype=float32_ref>]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Input (InputLayer)      (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Input (InputLayer)      (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (LSTM)             [(None, 4), (None, 4 128         Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)             [(None, None, 4), (N 112         Decoder_Input[0][0]              \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Output (Dense)          (None, None, 2)      10          Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, ndimX), name='Encoder_Input')\n",
    "encoder = LSTM(latent_dim, return_state=True, name='Encoder_LSTM')\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='Decoder_Input')\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='sigmoid', name='Decoder_Output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "pprint(model.weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/300\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.1310 - mean_absolute_error: 0.3185 - val_loss: 0.1156 - val_mean_absolute_error: 0.2993\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.1303 - mean_absolute_error: 0.3176 - val_loss: 0.1153 - val_mean_absolute_error: 0.2996\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.1296 - mean_absolute_error: 0.3167 - val_loss: 0.1151 - val_mean_absolute_error: 0.2999\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.1288 - mean_absolute_error: 0.3157 - val_loss: 0.1149 - val_mean_absolute_error: 0.3002\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.1281 - mean_absolute_error: 0.3147 - val_loss: 0.1146 - val_mean_absolute_error: 0.3005\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.1273 - mean_absolute_error: 0.3138 - val_loss: 0.1144 - val_mean_absolute_error: 0.3008\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.1266 - mean_absolute_error: 0.3128 - val_loss: 0.1142 - val_mean_absolute_error: 0.3010\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.1259 - mean_absolute_error: 0.3119 - val_loss: 0.1140 - val_mean_absolute_error: 0.3013\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.1251 - mean_absolute_error: 0.3109 - val_loss: 0.1138 - val_mean_absolute_error: 0.3016\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.1244 - mean_absolute_error: 0.3100 - val_loss: 0.1136 - val_mean_absolute_error: 0.3018\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 340us/step - loss: 0.1237 - mean_absolute_error: 0.3091 - val_loss: 0.1134 - val_mean_absolute_error: 0.3021\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.1230 - mean_absolute_error: 0.3082 - val_loss: 0.1132 - val_mean_absolute_error: 0.3023\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.1223 - mean_absolute_error: 0.3073 - val_loss: 0.1131 - val_mean_absolute_error: 0.3026\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.1216 - mean_absolute_error: 0.3064 - val_loss: 0.1129 - val_mean_absolute_error: 0.3028\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.1210 - mean_absolute_error: 0.3055 - val_loss: 0.1127 - val_mean_absolute_error: 0.3030\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.1203 - mean_absolute_error: 0.3046 - val_loss: 0.1126 - val_mean_absolute_error: 0.3032\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.1196 - mean_absolute_error: 0.3037 - val_loss: 0.1124 - val_mean_absolute_error: 0.3035\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.1190 - mean_absolute_error: 0.3028 - val_loss: 0.1123 - val_mean_absolute_error: 0.3037\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.1183 - mean_absolute_error: 0.3019 - val_loss: 0.1122 - val_mean_absolute_error: 0.3039\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.1176 - mean_absolute_error: 0.3010 - val_loss: 0.1120 - val_mean_absolute_error: 0.3042\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.1170 - mean_absolute_error: 0.3001 - val_loss: 0.1119 - val_mean_absolute_error: 0.3044\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1267 - mean_absolute_error: 0.327 - 0s 352us/step - loss: 0.1163 - mean_absolute_error: 0.2992 - val_loss: 0.1118 - val_mean_absolute_error: 0.3046\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.1157 - mean_absolute_error: 0.2983 - val_loss: 0.1116 - val_mean_absolute_error: 0.3049\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.1150 - mean_absolute_error: 0.2974 - val_loss: 0.1115 - val_mean_absolute_error: 0.3051\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.1143 - mean_absolute_error: 0.2965 - val_loss: 0.1114 - val_mean_absolute_error: 0.3053\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.1137 - mean_absolute_error: 0.2956 - val_loss: 0.1113 - val_mean_absolute_error: 0.3055\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.1130 - mean_absolute_error: 0.2947 - val_loss: 0.1112 - val_mean_absolute_error: 0.3057\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.1124 - mean_absolute_error: 0.2939 - val_loss: 0.1111 - val_mean_absolute_error: 0.3059\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.1117 - mean_absolute_error: 0.2930 - val_loss: 0.1110 - val_mean_absolute_error: 0.3061\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.1110 - mean_absolute_error: 0.2921 - val_loss: 0.1109 - val_mean_absolute_error: 0.3063\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.1104 - mean_absolute_error: 0.2912 - val_loss: 0.1108 - val_mean_absolute_error: 0.3065\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.1097 - mean_absolute_error: 0.2903 - val_loss: 0.1108 - val_mean_absolute_error: 0.3068\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.1090 - mean_absolute_error: 0.2894 - val_loss: 0.1107 - val_mean_absolute_error: 0.3070\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.1084 - mean_absolute_error: 0.2885 - val_loss: 0.1106 - val_mean_absolute_error: 0.3072\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.1077 - mean_absolute_error: 0.2876 - val_loss: 0.1105 - val_mean_absolute_error: 0.3074\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.1070 - mean_absolute_error: 0.2867 - val_loss: 0.1105 - val_mean_absolute_error: 0.3076\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.1063 - mean_absolute_error: 0.2858 - val_loss: 0.1104 - val_mean_absolute_error: 0.3078\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.1057 - mean_absolute_error: 0.2849 - val_loss: 0.1104 - val_mean_absolute_error: 0.3080\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.1050 - mean_absolute_error: 0.2839 - val_loss: 0.1103 - val_mean_absolute_error: 0.3082\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.1043 - mean_absolute_error: 0.2830 - val_loss: 0.1103 - val_mean_absolute_error: 0.3083\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.1036 - mean_absolute_error: 0.2821 - val_loss: 0.1103 - val_mean_absolute_error: 0.3085\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.1030 - mean_absolute_error: 0.2811 - val_loss: 0.1103 - val_mean_absolute_error: 0.3087\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.1023 - mean_absolute_error: 0.2802 - val_loss: 0.1103 - val_mean_absolute_error: 0.3089\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.1016 - mean_absolute_error: 0.2792 - val_loss: 0.1103 - val_mean_absolute_error: 0.3090\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.1010 - mean_absolute_error: 0.2782 - val_loss: 0.1103 - val_mean_absolute_error: 0.3092\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.1003 - mean_absolute_error: 0.2773 - val_loss: 0.1103 - val_mean_absolute_error: 0.3094\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0997 - mean_absolute_error: 0.2763 - val_loss: 0.1103 - val_mean_absolute_error: 0.3095\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0990 - mean_absolute_error: 0.2754 - val_loss: 0.1104 - val_mean_absolute_error: 0.3097\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0984 - mean_absolute_error: 0.2744 - val_loss: 0.1104 - val_mean_absolute_error: 0.3099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0977 - mean_absolute_error: 0.2735 - val_loss: 0.1105 - val_mean_absolute_error: 0.3101\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0971 - mean_absolute_error: 0.2726 - val_loss: 0.1105 - val_mean_absolute_error: 0.3103\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0965 - mean_absolute_error: 0.2717 - val_loss: 0.1106 - val_mean_absolute_error: 0.3105\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0959 - mean_absolute_error: 0.2708 - val_loss: 0.1107 - val_mean_absolute_error: 0.3107\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0953 - mean_absolute_error: 0.2699 - val_loss: 0.1108 - val_mean_absolute_error: 0.3109\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0947 - mean_absolute_error: 0.2690 - val_loss: 0.1109 - val_mean_absolute_error: 0.3111\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0941 - mean_absolute_error: 0.2681 - val_loss: 0.1111 - val_mean_absolute_error: 0.3113\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0935 - mean_absolute_error: 0.2673 - val_loss: 0.1112 - val_mean_absolute_error: 0.3115\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0930 - mean_absolute_error: 0.2665 - val_loss: 0.1113 - val_mean_absolute_error: 0.3117\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0924 - mean_absolute_error: 0.2657 - val_loss: 0.1115 - val_mean_absolute_error: 0.3119\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0919 - mean_absolute_error: 0.2649 - val_loss: 0.1116 - val_mean_absolute_error: 0.3121\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0914 - mean_absolute_error: 0.2641 - val_loss: 0.1118 - val_mean_absolute_error: 0.3122\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0909 - mean_absolute_error: 0.2633 - val_loss: 0.1120 - val_mean_absolute_error: 0.3124\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0904 - mean_absolute_error: 0.2626 - val_loss: 0.1121 - val_mean_absolute_error: 0.3126\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0899 - mean_absolute_error: 0.2618 - val_loss: 0.1123 - val_mean_absolute_error: 0.3127\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0894 - mean_absolute_error: 0.2611 - val_loss: 0.1125 - val_mean_absolute_error: 0.3129\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0889 - mean_absolute_error: 0.2603 - val_loss: 0.1127 - val_mean_absolute_error: 0.3130\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0885 - mean_absolute_error: 0.2596 - val_loss: 0.1128 - val_mean_absolute_error: 0.3131\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0881 - mean_absolute_error: 0.2589 - val_loss: 0.1130 - val_mean_absolute_error: 0.3133\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0876 - mean_absolute_error: 0.2582 - val_loss: 0.1132 - val_mean_absolute_error: 0.3134\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0872 - mean_absolute_error: 0.2575 - val_loss: 0.1134 - val_mean_absolute_error: 0.3135\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0868 - mean_absolute_error: 0.2568 - val_loss: 0.1135 - val_mean_absolute_error: 0.3136\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0864 - mean_absolute_error: 0.2561 - val_loss: 0.1137 - val_mean_absolute_error: 0.3137\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0861 - mean_absolute_error: 0.2555 - val_loss: 0.1138 - val_mean_absolute_error: 0.3138\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0857 - mean_absolute_error: 0.2548 - val_loss: 0.1140 - val_mean_absolute_error: 0.3139\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0853 - mean_absolute_error: 0.2542 - val_loss: 0.1141 - val_mean_absolute_error: 0.3140\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0850 - mean_absolute_error: 0.2536 - val_loss: 0.1143 - val_mean_absolute_error: 0.3141\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0847 - mean_absolute_error: 0.2530 - val_loss: 0.1144 - val_mean_absolute_error: 0.3141\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0843 - mean_absolute_error: 0.2525 - val_loss: 0.1145 - val_mean_absolute_error: 0.3142\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0840 - mean_absolute_error: 0.2519 - val_loss: 0.1146 - val_mean_absolute_error: 0.3142\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0837 - mean_absolute_error: 0.2514 - val_loss: 0.1147 - val_mean_absolute_error: 0.3142\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0834 - mean_absolute_error: 0.2508 - val_loss: 0.1148 - val_mean_absolute_error: 0.3143\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0831 - mean_absolute_error: 0.2503 - val_loss: 0.1148 - val_mean_absolute_error: 0.3143\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0828 - mean_absolute_error: 0.2498 - val_loss: 0.1149 - val_mean_absolute_error: 0.3143\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.0825 - mean_absolute_error: 0.2494 - val_loss: 0.1149 - val_mean_absolute_error: 0.3143\n",
      "Epoch 85/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0822 - mean_absolute_error: 0.2489 - val_loss: 0.1150 - val_mean_absolute_error: 0.3143\n",
      "Epoch 86/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0820 - mean_absolute_error: 0.2484 - val_loss: 0.1150 - val_mean_absolute_error: 0.3142\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0817 - mean_absolute_error: 0.2480 - val_loss: 0.1150 - val_mean_absolute_error: 0.3142\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0814 - mean_absolute_error: 0.2475 - val_loss: 0.1150 - val_mean_absolute_error: 0.3142\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0812 - mean_absolute_error: 0.2471 - val_loss: 0.1150 - val_mean_absolute_error: 0.3142\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0809 - mean_absolute_error: 0.2467 - val_loss: 0.1150 - val_mean_absolute_error: 0.3141\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0807 - mean_absolute_error: 0.2463 - val_loss: 0.1150 - val_mean_absolute_error: 0.3141\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0804 - mean_absolute_error: 0.2459 - val_loss: 0.1150 - val_mean_absolute_error: 0.3140\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0802 - mean_absolute_error: 0.2455 - val_loss: 0.1149 - val_mean_absolute_error: 0.3140\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0799 - mean_absolute_error: 0.2451 - val_loss: 0.1149 - val_mean_absolute_error: 0.3139\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0797 - mean_absolute_error: 0.2447 - val_loss: 0.1148 - val_mean_absolute_error: 0.3138\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0795 - mean_absolute_error: 0.2443 - val_loss: 0.1147 - val_mean_absolute_error: 0.3137\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0792 - mean_absolute_error: 0.2439 - val_loss: 0.1147 - val_mean_absolute_error: 0.3136\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0790 - mean_absolute_error: 0.2435 - val_loss: 0.1146 - val_mean_absolute_error: 0.3135\n",
      "Epoch 99/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0788 - mean_absolute_error: 0.2431 - val_loss: 0.1145 - val_mean_absolute_error: 0.3133\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0786 - mean_absolute_error: 0.2427 - val_loss: 0.1144 - val_mean_absolute_error: 0.3132\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0784 - mean_absolute_error: 0.2424 - val_loss: 0.1143 - val_mean_absolute_error: 0.3130\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0782 - mean_absolute_error: 0.2420 - val_loss: 0.1142 - val_mean_absolute_error: 0.3129\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0779 - mean_absolute_error: 0.2416 - val_loss: 0.1141 - val_mean_absolute_error: 0.3127\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0777 - mean_absolute_error: 0.2413 - val_loss: 0.1140 - val_mean_absolute_error: 0.3125\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0775 - mean_absolute_error: 0.2409 - val_loss: 0.1138 - val_mean_absolute_error: 0.3123\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0773 - mean_absolute_error: 0.2405 - val_loss: 0.1137 - val_mean_absolute_error: 0.3121\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0771 - mean_absolute_error: 0.2401 - val_loss: 0.1136 - val_mean_absolute_error: 0.3119\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0769 - mean_absolute_error: 0.2398 - val_loss: 0.1134 - val_mean_absolute_error: 0.3117\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0767 - mean_absolute_error: 0.2394 - val_loss: 0.1133 - val_mean_absolute_error: 0.3115\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0765 - mean_absolute_error: 0.2390 - val_loss: 0.1131 - val_mean_absolute_error: 0.3112\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0764 - mean_absolute_error: 0.2387 - val_loss: 0.1130 - val_mean_absolute_error: 0.3110\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0762 - mean_absolute_error: 0.2383 - val_loss: 0.1128 - val_mean_absolute_error: 0.3107\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0760 - mean_absolute_error: 0.2379 - val_loss: 0.1126 - val_mean_absolute_error: 0.3105\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0758 - mean_absolute_error: 0.2376 - val_loss: 0.1124 - val_mean_absolute_error: 0.3102\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0756 - mean_absolute_error: 0.2372 - val_loss: 0.1123 - val_mean_absolute_error: 0.3099\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0754 - mean_absolute_error: 0.2368 - val_loss: 0.1121 - val_mean_absolute_error: 0.3097\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0752 - mean_absolute_error: 0.2365 - val_loss: 0.1119 - val_mean_absolute_error: 0.3094\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0750 - mean_absolute_error: 0.2361 - val_loss: 0.1117 - val_mean_absolute_error: 0.3091\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0749 - mean_absolute_error: 0.2358 - val_loss: 0.1115 - val_mean_absolute_error: 0.3088\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0747 - mean_absolute_error: 0.2354 - val_loss: 0.1113 - val_mean_absolute_error: 0.3085\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0745 - mean_absolute_error: 0.2350 - val_loss: 0.1111 - val_mean_absolute_error: 0.3082\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0743 - mean_absolute_error: 0.2347 - val_loss: 0.1109 - val_mean_absolute_error: 0.3079\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0741 - mean_absolute_error: 0.2343 - val_loss: 0.1107 - val_mean_absolute_error: 0.3075\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0740 - mean_absolute_error: 0.2340 - val_loss: 0.1104 - val_mean_absolute_error: 0.3072\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0738 - mean_absolute_error: 0.2336 - val_loss: 0.1102 - val_mean_absolute_error: 0.3069\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0736 - mean_absolute_error: 0.2332 - val_loss: 0.1100 - val_mean_absolute_error: 0.3065\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0734 - mean_absolute_error: 0.2329 - val_loss: 0.1098 - val_mean_absolute_error: 0.3062\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0733 - mean_absolute_error: 0.2325 - val_loss: 0.1095 - val_mean_absolute_error: 0.3059\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0731 - mean_absolute_error: 0.2322 - val_loss: 0.1093 - val_mean_absolute_error: 0.3056\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0729 - mean_absolute_error: 0.2318 - val_loss: 0.1090 - val_mean_absolute_error: 0.3052\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0727 - mean_absolute_error: 0.2315 - val_loss: 0.1088 - val_mean_absolute_error: 0.3049\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0726 - mean_absolute_error: 0.2311 - val_loss: 0.1085 - val_mean_absolute_error: 0.3046\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0724 - mean_absolute_error: 0.2308 - val_loss: 0.1083 - val_mean_absolute_error: 0.3043\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0722 - mean_absolute_error: 0.2304 - val_loss: 0.1080 - val_mean_absolute_error: 0.3039\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0721 - mean_absolute_error: 0.2300 - val_loss: 0.1078 - val_mean_absolute_error: 0.3036\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0719 - mean_absolute_error: 0.2297 - val_loss: 0.1075 - val_mean_absolute_error: 0.3032\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0717 - mean_absolute_error: 0.2293 - val_loss: 0.1073 - val_mean_absolute_error: 0.3029\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0716 - mean_absolute_error: 0.2290 - val_loss: 0.1070 - val_mean_absolute_error: 0.3025\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0714 - mean_absolute_error: 0.2286 - val_loss: 0.1067 - val_mean_absolute_error: 0.3022\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0712 - mean_absolute_error: 0.2283 - val_loss: 0.1065 - val_mean_absolute_error: 0.3018\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0711 - mean_absolute_error: 0.2279 - val_loss: 0.1062 - val_mean_absolute_error: 0.3015\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0709 - mean_absolute_error: 0.2276 - val_loss: 0.1059 - val_mean_absolute_error: 0.3011\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0707 - mean_absolute_error: 0.2272 - val_loss: 0.1056 - val_mean_absolute_error: 0.3007\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0706 - mean_absolute_error: 0.2269 - val_loss: 0.1054 - val_mean_absolute_error: 0.3003\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0704 - mean_absolute_error: 0.2265 - val_loss: 0.1051 - val_mean_absolute_error: 0.3000\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0702 - mean_absolute_error: 0.2262 - val_loss: 0.1048 - val_mean_absolute_error: 0.2996\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0701 - mean_absolute_error: 0.2258 - val_loss: 0.1045 - val_mean_absolute_error: 0.2992\n",
      "Epoch 148/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 348us/step - loss: 0.0699 - mean_absolute_error: 0.2254 - val_loss: 0.1042 - val_mean_absolute_error: 0.2988\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0697 - mean_absolute_error: 0.2251 - val_loss: 0.1039 - val_mean_absolute_error: 0.2985\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0696 - mean_absolute_error: 0.2247 - val_loss: 0.1037 - val_mean_absolute_error: 0.2981\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0694 - mean_absolute_error: 0.2244 - val_loss: 0.1034 - val_mean_absolute_error: 0.2977\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0692 - mean_absolute_error: 0.2240 - val_loss: 0.1031 - val_mean_absolute_error: 0.2973\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0691 - mean_absolute_error: 0.2236 - val_loss: 0.1028 - val_mean_absolute_error: 0.2969\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0689 - mean_absolute_error: 0.2233 - val_loss: 0.1025 - val_mean_absolute_error: 0.2965\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0687 - mean_absolute_error: 0.2229 - val_loss: 0.1022 - val_mean_absolute_error: 0.2961\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0686 - mean_absolute_error: 0.2225 - val_loss: 0.1019 - val_mean_absolute_error: 0.2957\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0684 - mean_absolute_error: 0.2222 - val_loss: 0.1016 - val_mean_absolute_error: 0.2953\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0682 - mean_absolute_error: 0.2218 - val_loss: 0.1013 - val_mean_absolute_error: 0.2949\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0681 - mean_absolute_error: 0.2214 - val_loss: 0.1010 - val_mean_absolute_error: 0.2945\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0679 - mean_absolute_error: 0.2211 - val_loss: 0.1007 - val_mean_absolute_error: 0.2940\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0677 - mean_absolute_error: 0.2207 - val_loss: 0.1004 - val_mean_absolute_error: 0.2936\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0676 - mean_absolute_error: 0.2203 - val_loss: 0.1001 - val_mean_absolute_error: 0.2932\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0674 - mean_absolute_error: 0.2199 - val_loss: 0.0998 - val_mean_absolute_error: 0.2928\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0673 - mean_absolute_error: 0.2196 - val_loss: 0.0995 - val_mean_absolute_error: 0.2924\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0671 - mean_absolute_error: 0.2192 - val_loss: 0.0992 - val_mean_absolute_error: 0.2919\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0669 - mean_absolute_error: 0.2188 - val_loss: 0.0989 - val_mean_absolute_error: 0.2915\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0668 - mean_absolute_error: 0.2184 - val_loss: 0.0986 - val_mean_absolute_error: 0.2911\n",
      "Epoch 168/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0666 - mean_absolute_error: 0.2180 - val_loss: 0.0982 - val_mean_absolute_error: 0.2906\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0664 - mean_absolute_error: 0.2176 - val_loss: 0.0979 - val_mean_absolute_error: 0.2902\n",
      "Epoch 170/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0663 - mean_absolute_error: 0.2173 - val_loss: 0.0976 - val_mean_absolute_error: 0.2898\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0661 - mean_absolute_error: 0.2169 - val_loss: 0.0973 - val_mean_absolute_error: 0.2893\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0660 - mean_absolute_error: 0.2165 - val_loss: 0.0970 - val_mean_absolute_error: 0.2889\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0658 - mean_absolute_error: 0.2161 - val_loss: 0.0967 - val_mean_absolute_error: 0.2884\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0656 - mean_absolute_error: 0.2157 - val_loss: 0.0964 - val_mean_absolute_error: 0.2880\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0655 - mean_absolute_error: 0.2153 - val_loss: 0.0961 - val_mean_absolute_error: 0.2876\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0653 - mean_absolute_error: 0.2149 - val_loss: 0.0958 - val_mean_absolute_error: 0.2871\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0652 - mean_absolute_error: 0.2145 - val_loss: 0.0955 - val_mean_absolute_error: 0.2867\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0650 - mean_absolute_error: 0.2141 - val_loss: 0.0951 - val_mean_absolute_error: 0.2862\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0648 - mean_absolute_error: 0.2137 - val_loss: 0.0948 - val_mean_absolute_error: 0.2858\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0647 - mean_absolute_error: 0.2133 - val_loss: 0.0945 - val_mean_absolute_error: 0.2853\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0645 - mean_absolute_error: 0.2129 - val_loss: 0.0942 - val_mean_absolute_error: 0.2849\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0644 - mean_absolute_error: 0.2125 - val_loss: 0.0939 - val_mean_absolute_error: 0.2844\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0642 - mean_absolute_error: 0.2121 - val_loss: 0.0936 - val_mean_absolute_error: 0.2839\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0640 - mean_absolute_error: 0.2117 - val_loss: 0.0933 - val_mean_absolute_error: 0.2835\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0639 - mean_absolute_error: 0.2113 - val_loss: 0.0930 - val_mean_absolute_error: 0.2830\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0637 - mean_absolute_error: 0.2109 - val_loss: 0.0926 - val_mean_absolute_error: 0.2825\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0636 - mean_absolute_error: 0.2104 - val_loss: 0.0923 - val_mean_absolute_error: 0.2820\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0634 - mean_absolute_error: 0.2100 - val_loss: 0.0920 - val_mean_absolute_error: 0.2816\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0633 - mean_absolute_error: 0.2096 - val_loss: 0.0917 - val_mean_absolute_error: 0.2811\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0631 - mean_absolute_error: 0.2092 - val_loss: 0.0914 - val_mean_absolute_error: 0.2806\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0629 - mean_absolute_error: 0.2088 - val_loss: 0.0911 - val_mean_absolute_error: 0.2801\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0628 - mean_absolute_error: 0.2084 - val_loss: 0.0908 - val_mean_absolute_error: 0.2796\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.0626 - mean_absolute_error: 0.2080 - val_loss: 0.0905 - val_mean_absolute_error: 0.2791\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0625 - mean_absolute_error: 0.2076 - val_loss: 0.0902 - val_mean_absolute_error: 0.2786\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0623 - mean_absolute_error: 0.2072 - val_loss: 0.0898 - val_mean_absolute_error: 0.2780\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0622 - mean_absolute_error: 0.2068 - val_loss: 0.0895 - val_mean_absolute_error: 0.2775\n",
      "Epoch 197/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0620 - mean_absolute_error: 0.2064 - val_loss: 0.0892 - val_mean_absolute_error: 0.2770\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0619 - mean_absolute_error: 0.2059 - val_loss: 0.0889 - val_mean_absolute_error: 0.2765\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0617 - mean_absolute_error: 0.2055 - val_loss: 0.0886 - val_mean_absolute_error: 0.2759\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0616 - mean_absolute_error: 0.2051 - val_loss: 0.0883 - val_mean_absolute_error: 0.2754\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0614 - mean_absolute_error: 0.2047 - val_loss: 0.0880 - val_mean_absolute_error: 0.2749\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0613 - mean_absolute_error: 0.2043 - val_loss: 0.0877 - val_mean_absolute_error: 0.2743\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0611 - mean_absolute_error: 0.2039 - val_loss: 0.0874 - val_mean_absolute_error: 0.2738\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0610 - mean_absolute_error: 0.2035 - val_loss: 0.0871 - val_mean_absolute_error: 0.2732\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0608 - mean_absolute_error: 0.2031 - val_loss: 0.0868 - val_mean_absolute_error: 0.2727\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0607 - mean_absolute_error: 0.2027 - val_loss: 0.0864 - val_mean_absolute_error: 0.2721\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0605 - mean_absolute_error: 0.2023 - val_loss: 0.0861 - val_mean_absolute_error: 0.2715\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0604 - mean_absolute_error: 0.2019 - val_loss: 0.0858 - val_mean_absolute_error: 0.2710\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0602 - mean_absolute_error: 0.2015 - val_loss: 0.0855 - val_mean_absolute_error: 0.2704\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0601 - mean_absolute_error: 0.2011 - val_loss: 0.0852 - val_mean_absolute_error: 0.2698\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0599 - mean_absolute_error: 0.2007 - val_loss: 0.0849 - val_mean_absolute_error: 0.2693\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0598 - mean_absolute_error: 0.2003 - val_loss: 0.0846 - val_mean_absolute_error: 0.2687\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0596 - mean_absolute_error: 0.1999 - val_loss: 0.0843 - val_mean_absolute_error: 0.2681\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0595 - mean_absolute_error: 0.1995 - val_loss: 0.0840 - val_mean_absolute_error: 0.2676\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0593 - mean_absolute_error: 0.1991 - val_loss: 0.0837 - val_mean_absolute_error: 0.2670\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0592 - mean_absolute_error: 0.1987 - val_loss: 0.0834 - val_mean_absolute_error: 0.2664\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0591 - mean_absolute_error: 0.1983 - val_loss: 0.0831 - val_mean_absolute_error: 0.2658\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0589 - mean_absolute_error: 0.1979 - val_loss: 0.0828 - val_mean_absolute_error: 0.2652\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0588 - mean_absolute_error: 0.1975 - val_loss: 0.0825 - val_mean_absolute_error: 0.2646\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0586 - mean_absolute_error: 0.1971 - val_loss: 0.0822 - val_mean_absolute_error: 0.2641\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0585 - mean_absolute_error: 0.1968 - val_loss: 0.0819 - val_mean_absolute_error: 0.2635\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0584 - mean_absolute_error: 0.1964 - val_loss: 0.0817 - val_mean_absolute_error: 0.2629\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0582 - mean_absolute_error: 0.1960 - val_loss: 0.0814 - val_mean_absolute_error: 0.2623\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0581 - mean_absolute_error: 0.1956 - val_loss: 0.0811 - val_mean_absolute_error: 0.2617\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0579 - mean_absolute_error: 0.1952 - val_loss: 0.0808 - val_mean_absolute_error: 0.2611\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0578 - mean_absolute_error: 0.1948 - val_loss: 0.0805 - val_mean_absolute_error: 0.2606\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0577 - mean_absolute_error: 0.1945 - val_loss: 0.0802 - val_mean_absolute_error: 0.2600\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.0575 - mean_absolute_error: 0.1941 - val_loss: 0.0800 - val_mean_absolute_error: 0.2594\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0574 - mean_absolute_error: 0.1937 - val_loss: 0.0797 - val_mean_absolute_error: 0.2588\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0573 - mean_absolute_error: 0.1934 - val_loss: 0.0794 - val_mean_absolute_error: 0.2583\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0571 - mean_absolute_error: 0.1930 - val_loss: 0.0792 - val_mean_absolute_error: 0.2577\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0570 - mean_absolute_error: 0.1926 - val_loss: 0.0789 - val_mean_absolute_error: 0.2572\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0569 - mean_absolute_error: 0.1923 - val_loss: 0.0786 - val_mean_absolute_error: 0.2566\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0568 - mean_absolute_error: 0.1919 - val_loss: 0.0784 - val_mean_absolute_error: 0.2561\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0566 - mean_absolute_error: 0.1915 - val_loss: 0.0781 - val_mean_absolute_error: 0.2555\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0565 - mean_absolute_error: 0.1912 - val_loss: 0.0778 - val_mean_absolute_error: 0.2550\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0564 - mean_absolute_error: 0.1908 - val_loss: 0.0776 - val_mean_absolute_error: 0.2544\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0563 - mean_absolute_error: 0.1904 - val_loss: 0.0773 - val_mean_absolute_error: 0.2539\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0561 - mean_absolute_error: 0.1901 - val_loss: 0.0771 - val_mean_absolute_error: 0.2533\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0560 - mean_absolute_error: 0.1897 - val_loss: 0.0769 - val_mean_absolute_error: 0.2528\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0559 - mean_absolute_error: 0.1894 - val_loss: 0.0766 - val_mean_absolute_error: 0.2523\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0558 - mean_absolute_error: 0.1890 - val_loss: 0.0764 - val_mean_absolute_error: 0.2518\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0556 - mean_absolute_error: 0.1887 - val_loss: 0.0761 - val_mean_absolute_error: 0.2512\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0555 - mean_absolute_error: 0.1883 - val_loss: 0.0759 - val_mean_absolute_error: 0.2507\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0554 - mean_absolute_error: 0.1880 - val_loss: 0.0757 - val_mean_absolute_error: 0.2502\n",
      "Epoch 246/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 398us/step - loss: 0.0553 - mean_absolute_error: 0.1876 - val_loss: 0.0754 - val_mean_absolute_error: 0.2497\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0552 - mean_absolute_error: 0.1873 - val_loss: 0.0752 - val_mean_absolute_error: 0.2492\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0551 - mean_absolute_error: 0.1869 - val_loss: 0.0750 - val_mean_absolute_error: 0.2487\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0549 - mean_absolute_error: 0.1866 - val_loss: 0.0748 - val_mean_absolute_error: 0.2482\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0548 - mean_absolute_error: 0.1863 - val_loss: 0.0746 - val_mean_absolute_error: 0.2477\n",
      "Epoch 251/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0547 - mean_absolute_error: 0.1859 - val_loss: 0.0743 - val_mean_absolute_error: 0.2472\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0546 - mean_absolute_error: 0.1856 - val_loss: 0.0741 - val_mean_absolute_error: 0.2467\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0545 - mean_absolute_error: 0.1853 - val_loss: 0.0739 - val_mean_absolute_error: 0.2463\n",
      "Epoch 254/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0544 - mean_absolute_error: 0.1850 - val_loss: 0.0737 - val_mean_absolute_error: 0.2458\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0543 - mean_absolute_error: 0.1847 - val_loss: 0.0735 - val_mean_absolute_error: 0.2453\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0542 - mean_absolute_error: 0.1844 - val_loss: 0.0733 - val_mean_absolute_error: 0.2448\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0541 - mean_absolute_error: 0.1841 - val_loss: 0.0731 - val_mean_absolute_error: 0.2444\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0539 - mean_absolute_error: 0.1838 - val_loss: 0.0729 - val_mean_absolute_error: 0.2439\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0538 - mean_absolute_error: 0.1835 - val_loss: 0.0727 - val_mean_absolute_error: 0.2434\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0537 - mean_absolute_error: 0.1832 - val_loss: 0.0725 - val_mean_absolute_error: 0.2430\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0536 - mean_absolute_error: 0.1829 - val_loss: 0.0723 - val_mean_absolute_error: 0.2425\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0535 - mean_absolute_error: 0.1826 - val_loss: 0.0721 - val_mean_absolute_error: 0.2421\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0534 - mean_absolute_error: 0.1823 - val_loss: 0.0719 - val_mean_absolute_error: 0.2416\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0533 - mean_absolute_error: 0.1820 - val_loss: 0.0717 - val_mean_absolute_error: 0.2412\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0532 - mean_absolute_error: 0.1817 - val_loss: 0.0716 - val_mean_absolute_error: 0.2407\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0531 - mean_absolute_error: 0.1815 - val_loss: 0.0714 - val_mean_absolute_error: 0.2403\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0530 - mean_absolute_error: 0.1812 - val_loss: 0.0712 - val_mean_absolute_error: 0.2398\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0529 - mean_absolute_error: 0.1809 - val_loss: 0.0710 - val_mean_absolute_error: 0.2394\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0528 - mean_absolute_error: 0.1806 - val_loss: 0.0708 - val_mean_absolute_error: 0.2390\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0527 - mean_absolute_error: 0.1804 - val_loss: 0.0706 - val_mean_absolute_error: 0.2385\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0526 - mean_absolute_error: 0.1801 - val_loss: 0.0705 - val_mean_absolute_error: 0.2381\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0525 - mean_absolute_error: 0.1798 - val_loss: 0.0703 - val_mean_absolute_error: 0.2377\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0524 - mean_absolute_error: 0.1796 - val_loss: 0.0701 - val_mean_absolute_error: 0.2372\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0523 - mean_absolute_error: 0.1793 - val_loss: 0.0699 - val_mean_absolute_error: 0.2368\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0522 - mean_absolute_error: 0.1790 - val_loss: 0.0698 - val_mean_absolute_error: 0.2364\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0521 - mean_absolute_error: 0.1788 - val_loss: 0.0696 - val_mean_absolute_error: 0.2360\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0520 - mean_absolute_error: 0.1785 - val_loss: 0.0694 - val_mean_absolute_error: 0.2356\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0519 - mean_absolute_error: 0.1783 - val_loss: 0.0693 - val_mean_absolute_error: 0.2351\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0518 - mean_absolute_error: 0.1780 - val_loss: 0.0691 - val_mean_absolute_error: 0.2347\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0517 - mean_absolute_error: 0.1778 - val_loss: 0.0689 - val_mean_absolute_error: 0.2343\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0517 - mean_absolute_error: 0.1776 - val_loss: 0.0688 - val_mean_absolute_error: 0.2339\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0516 - mean_absolute_error: 0.1773 - val_loss: 0.0686 - val_mean_absolute_error: 0.2335\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0515 - mean_absolute_error: 0.1771 - val_loss: 0.0685 - val_mean_absolute_error: 0.2331\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0514 - mean_absolute_error: 0.1769 - val_loss: 0.0683 - val_mean_absolute_error: 0.2327\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0513 - mean_absolute_error: 0.1766 - val_loss: 0.0681 - val_mean_absolute_error: 0.2324\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0512 - mean_absolute_error: 0.1764 - val_loss: 0.0680 - val_mean_absolute_error: 0.2320\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0511 - mean_absolute_error: 0.1762 - val_loss: 0.0678 - val_mean_absolute_error: 0.2316\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0511 - mean_absolute_error: 0.1759 - val_loss: 0.0677 - val_mean_absolute_error: 0.2312\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0510 - mean_absolute_error: 0.1757 - val_loss: 0.0675 - val_mean_absolute_error: 0.2308\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0509 - mean_absolute_error: 0.1755 - val_loss: 0.0674 - val_mean_absolute_error: 0.2304\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0508 - mean_absolute_error: 0.1753 - val_loss: 0.0672 - val_mean_absolute_error: 0.2301\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0507 - mean_absolute_error: 0.1751 - val_loss: 0.0671 - val_mean_absolute_error: 0.2297\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0507 - mean_absolute_error: 0.1749 - val_loss: 0.0669 - val_mean_absolute_error: 0.2293\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0506 - mean_absolute_error: 0.1747 - val_loss: 0.0668 - val_mean_absolute_error: 0.2290\n",
      "Epoch 295/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0505 - mean_absolute_error: 0.1745 - val_loss: 0.0667 - val_mean_absolute_error: 0.2286\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0504 - mean_absolute_error: 0.1743 - val_loss: 0.0665 - val_mean_absolute_error: 0.2282\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0504 - mean_absolute_error: 0.1741 - val_loss: 0.0664 - val_mean_absolute_error: 0.2279\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0503 - mean_absolute_error: 0.1739 - val_loss: 0.0663 - val_mean_absolute_error: 0.2275\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0502 - mean_absolute_error: 0.1737 - val_loss: 0.0661 - val_mean_absolute_error: 0.2272\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0501 - mean_absolute_error: 0.1735 - val_loss: 0.0660 - val_mean_absolute_error: 0.2268\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "EPOCH_NUM = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit([train_X, train_decoder_input_Y], train_decoder_target_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAADFCAYAAAB5PKoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04b0701240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 3.449 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict([train_X, train_decoder_input_Y])\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "train_mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(train_mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_X = np.stack([np.arange(xlen) + 100,\n",
    "                  np.arange(xlen) + 101,\n",
    "                  np.arange(xlen) + 102]).T.astype('float32')\n",
    "print(new_X)\n",
    "\n",
    "test_X = scalerX.transform(new_X)\n",
    "test_X = test_X.reshape(-1, xlen, xfeature)\n",
    "print('X Shape:', test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.34155399,  0.12794316,  0.63422275,  0.41103649],\n",
       "        [ 0.34805363,  0.13129497,  0.665245  ,  0.40535268],\n",
       "        [ 0.35728961,  0.13137034,  0.64570004,  0.40362003],\n",
       "        [ 0.36422932,  0.13112892,  0.59659773,  0.4061698 ],\n",
       "        [ 0.35794801,  0.1323998 ,  0.56656885,  0.40852755],\n",
       "        [ 0.3500548 ,  0.13670979,  0.5889768 ,  0.41284588],\n",
       "        [ 0.35354623,  0.14291742,  0.64875805,  0.41512296],\n",
       "        [ 0.35937878,  0.14728042,  0.69551158,  0.40921324],\n",
       "        [ 0.36279699,  0.14792942,  0.69146323,  0.40357471],\n",
       "        [ 0.36750945,  0.14715962,  0.64391941,  0.40439186],\n",
       "        [ 0.36647722,  0.14728887,  0.59854269,  0.40767112],\n",
       "        [ 0.36221474,  0.15034114,  0.60176283,  0.41311565],\n",
       "        [ 0.36761406,  0.15624236,  0.65524793,  0.41809171],\n",
       "        [ 0.37561244,  0.16142178,  0.7133342 ,  0.41387612],\n",
       "        [ 0.37626621,  0.16282205,  0.72652006,  0.40523377],\n",
       "        [ 0.37611356,  0.16164017,  0.6861335 ,  0.40305066],\n",
       "        [ 0.3767235 ,  0.16077438,  0.63122696,  0.40633047],\n",
       "        [ 0.37586772,  0.16266198,  0.61583394,  0.41254395],\n",
       "        [ 0.38279164,  0.16803655,  0.65736955,  0.41964972],\n",
       "        [ 0.39432678,  0.17385761,  0.72094309,  0.41848028],\n",
       "        [ 0.39616999,  0.17623208,  0.75142169,  0.40861684],\n",
       "        [ 0.39129993,  0.17492121,  0.72328061,  0.40276116],\n",
       "        [ 0.38964102,  0.17305318,  0.66430753,  0.40479666],\n",
       "        [ 0.39069074,  0.17374007,  0.63175446,  0.41127127],\n",
       "        [ 0.39866826,  0.17837831,  0.6573419 ,  0.41986108],\n",
       "        [ 0.41431394,  0.18478978,  0.72172636,  0.42233765],\n",
       "        [ 0.42022848,  0.18824378,  0.76727003,  0.41313711],\n",
       "        [ 0.41306943,  0.18720391,  0.75489789,  0.40375832],\n",
       "        [ 0.40714747,  0.18445334,  0.69780767,  0.40343946]], dtype=float32),\n",
       " array([[ 0.43250483,  0.22681622,  1.12678289,  0.98889339],\n",
       "        [ 0.42263559,  0.22824973,  1.12289965,  0.93236947],\n",
       "        [ 0.44195151,  0.23140991,  1.11195827,  0.93836582],\n",
       "        [ 0.47900891,  0.23952976,  1.11096251,  0.99858761],\n",
       "        [ 0.49161348,  0.24929595,  1.1398176 ,  1.05702972],\n",
       "        [ 0.47157097,  0.25634789,  1.19585395,  1.07223833],\n",
       "        [ 0.44662809,  0.2591913 ,  1.2385807 ,  1.02691031],\n",
       "        [ 0.42989513,  0.26008451,  1.24915934,  0.95524406],\n",
       "        [ 0.43322328,  0.26286891,  1.24522197,  0.93235308],\n",
       "        [ 0.4615832 ,  0.2707338 ,  1.23981929,  0.97768277],\n",
       "        [ 0.48641443,  0.28068149,  1.24974799,  1.04712987],\n",
       "        [ 0.48225316,  0.28762472,  1.29082298,  1.08699322],\n",
       "        [ 0.46421459,  0.28983831,  1.33812261,  1.06264496],\n",
       "        [ 0.44592941,  0.28973961,  1.36209631,  0.98614621],\n",
       "        [ 0.4377661 ,  0.29151434,  1.36704373,  0.93599218],\n",
       "        [ 0.45369416,  0.29857945,  1.36248422,  0.9595741 ],\n",
       "        [ 0.48223323,  0.30885205,  1.35957599,  1.03060269],\n",
       "        [ 0.4923296 ,  0.31640664,  1.38349748,  1.09115362],\n",
       "        [ 0.48317122,  0.31868887,  1.4286201 ,  1.09225368],\n",
       "        [ 0.46747541,  0.3176966 ,  1.46331465,  1.02253127],\n",
       "        [ 0.45313686,  0.31802517,  1.4784323 ,  0.95049095],\n",
       "        [ 0.45631045,  0.32365587,  1.47872853,  0.94749224],\n",
       "        [ 0.48102441,  0.33387136,  1.46916163,  1.01071084],\n",
       "        [ 0.50158131,  0.34248587,  1.47623539,  1.08599329],\n",
       "        [ 0.50254345,  0.34558523,  1.51287508,  1.11408281],\n",
       "        [ 0.49229994,  0.34409231,  1.55422759,  1.06029046],\n",
       "        [ 0.4761416 ,  0.34284562,  1.57983208,  0.97415268],\n",
       "        [ 0.46896002,  0.34654528,  1.58772647,  0.94290257],\n",
       "        [ 0.48547336,  0.35598192,  1.57713628,  0.99005449]], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_value = encoder_model.predict(test_X)\n",
    "states_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq = np.zeros((1, timestepY, ndimY))\n",
    "target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.6011681 ,  0.54658979],\n",
       "         [ 0.62254471,  0.54875469],\n",
       "         [ 0.63908684,  0.54325449],\n",
       "         [ 0.65576249,  0.54028392]]], dtype=float32),\n",
       " array([[ 0.44945848,  0.12053088,  0.18813108,  0.07915933]], dtype=float32),\n",
       " array([[ 0.90206313,  0.23443614,  0.40799987,  0.15756212]], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "states_value = [h, c]\n",
    "output_tokens, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model within a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(latent_dim, test_X, timestepY, ndimY, encoder_inputs, encoder_states):\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    states_value = encoder_model.predict(test_X)\n",
    "    \n",
    "    samples, _, _ = test_X.shape\n",
    "    target_seq = np.zeros((samples, timestepY, ndimY))\n",
    "    \n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    states_value = [h, c]\n",
    "    output_tokens, h, c\n",
    "    \n",
    "    y_hat_array = output_tokens\n",
    "    \n",
    "    return y_hat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.6011681 ,  0.54658979],\n",
       "        [ 0.62254471,  0.54875469],\n",
       "        [ 0.63908684,  0.54325449],\n",
       "        [ 0.65576249,  0.54028392]],\n",
       "\n",
       "       [[ 0.59953177,  0.55048043],\n",
       "        [ 0.62205076,  0.55413681],\n",
       "        [ 0.63853568,  0.54821783],\n",
       "        [ 0.65526384,  0.54489046]],\n",
       "\n",
       "       [[ 0.60420161,  0.54936898],\n",
       "        [ 0.62634736,  0.55300939],\n",
       "        [ 0.642524  ,  0.54788858],\n",
       "        [ 0.65862077,  0.54528415]],\n",
       "\n",
       "       [[ 0.612535  ,  0.54543215],\n",
       "        [ 0.633183  ,  0.54818529],\n",
       "        [ 0.64879102,  0.54458112],\n",
       "        [ 0.66383207,  0.54331779]],\n",
       "\n",
       "       [[ 0.6159308 ,  0.5440948 ],\n",
       "        [ 0.63537955,  0.54604107],\n",
       "        [ 0.65070897,  0.54308927],\n",
       "        [ 0.66535908,  0.54236466]],\n",
       "\n",
       "       [[ 0.61261517,  0.54761875],\n",
       "        [ 0.63230157,  0.55029231],\n",
       "        [ 0.64761782,  0.54689324],\n",
       "        [ 0.66262192,  0.54565728]],\n",
       "\n",
       "       [[ 0.60750222,  0.55362439],\n",
       "        [ 0.62883985,  0.55890274],\n",
       "        [ 0.64403665,  0.55466032],\n",
       "        [ 0.65942633,  0.55260098]],\n",
       "\n",
       "       [[ 0.60403466,  0.55887449],\n",
       "        [ 0.62701601,  0.56649667],\n",
       "        [ 0.64208895,  0.56145865],\n",
       "        [ 0.65768212,  0.558725  ]],\n",
       "\n",
       "       [[ 0.60546088,  0.56020498],\n",
       "        [ 0.62846196,  0.56809789],\n",
       "        [ 0.64338499,  0.56319124],\n",
       "        [ 0.65874016,  0.56055743]],\n",
       "\n",
       "       [[ 0.6125654 ,  0.55733675],\n",
       "        [ 0.63400644,  0.56392992],\n",
       "        [ 0.64855009,  0.56032103],\n",
       "        [ 0.66305131,  0.55874908]],\n",
       "\n",
       "       [[ 0.61864513,  0.55413043],\n",
       "        [ 0.63841605,  0.55928946],\n",
       "        [ 0.6526159 ,  0.55682474],\n",
       "        [ 0.66641831,  0.55621207]],\n",
       "\n",
       "       [[ 0.61831474,  0.55469042],\n",
       "        [ 0.63789356,  0.56001449],\n",
       "        [ 0.65197617,  0.55761725],\n",
       "        [ 0.66579509,  0.55702984]],\n",
       "\n",
       "       [[ 0.61424923,  0.55891144],\n",
       "        [ 0.63542992,  0.56685424],\n",
       "        [ 0.64929932,  0.56381494],\n",
       "        [ 0.66337639,  0.56265557]],\n",
       "\n",
       "       [[ 0.60995644,  0.56442988],\n",
       "        [ 0.63324255,  0.57555532],\n",
       "        [ 0.64686292,  0.5715487 ],\n",
       "        [ 0.66117662,  0.56961834]],\n",
       "\n",
       "       [[ 0.60877275,  0.5676927 ],\n",
       "        [ 0.63265556,  0.57979655],\n",
       "        [ 0.64617413,  0.57535601],\n",
       "        [ 0.66052765,  0.57305658]],\n",
       "\n",
       "       [[ 0.61362499,  0.56654364],\n",
       "        [ 0.63605416,  0.57719386],\n",
       "        [ 0.64940256,  0.57364267],\n",
       "        [ 0.66321033,  0.57196611]],\n",
       "\n",
       "       [[ 0.62086409,  0.5628528 ],\n",
       "        [ 0.64120591,  0.57142788],\n",
       "        [ 0.65425789,  0.56926745],\n",
       "        [ 0.66726065,  0.56868172]],\n",
       "\n",
       "       [[ 0.62339604,  0.56110746],\n",
       "        [ 0.64298689,  0.56912822],\n",
       "        [ 0.65583867,  0.56753772],\n",
       "        [ 0.66854113,  0.5674445 ]],\n",
       "\n",
       "       [[ 0.6209327 ,  0.56322128],\n",
       "        [ 0.64186054,  0.57349986],\n",
       "        [ 0.65442961,  0.57155633],\n",
       "        [ 0.66722405,  0.57121927]],\n",
       "\n",
       "       [[ 0.61667693,  0.56807691],\n",
       "        [ 0.64001924,  0.58214146],\n",
       "        [ 0.65219194,  0.5792371 ],\n",
       "        [ 0.66517067,  0.57820868]],\n",
       "\n",
       "       [[ 0.61375284,  0.57253414],\n",
       "        [ 0.63839447,  0.5885976 ],\n",
       "        [ 0.65039182,  0.58483917],\n",
       "        [ 0.66355503,  0.58316088]],\n",
       "\n",
       "       [[ 0.6160152 ,  0.57331121],\n",
       "        [ 0.63955581,  0.5881778 ],\n",
       "        [ 0.6515488 ,  0.5848043 ],\n",
       "        [ 0.66448474,  0.58327985]],\n",
       "\n",
       "       [[ 0.62293768,  0.57021493],\n",
       "        [ 0.6440863 ,  0.5824396 ],\n",
       "        [ 0.655936  ,  0.58046407],\n",
       "        [ 0.6681487 ,  0.57990807]],\n",
       "\n",
       "       [[ 0.62778664,  0.56697404],\n",
       "        [ 0.64753282,  0.57779706],\n",
       "        [ 0.65915865,  0.57682574],\n",
       "        [ 0.67083037,  0.57709455]],\n",
       "\n",
       "       [[ 0.6273151 ,  0.56696266],\n",
       "        [ 0.64792025,  0.57936043],\n",
       "        [ 0.65921885,  0.57839143],\n",
       "        [ 0.67078912,  0.57877749]],\n",
       "\n",
       "       [[ 0.62370592,  0.57054126],\n",
       "        [ 0.64687932,  0.58705384],\n",
       "        [ 0.65763235,  0.5852499 ],\n",
       "        [ 0.66927993,  0.58513916]],\n",
       "\n",
       "       [[ 0.61981595,  0.57542402],\n",
       "        [ 0.64498872,  0.5950377 ],\n",
       "        [ 0.65540671,  0.59210706],\n",
       "        [ 0.66728085,  0.59122944]],\n",
       "\n",
       "       [[ 0.61972952,  0.57792771],\n",
       "        [ 0.64440376,  0.597045  ],\n",
       "        [ 0.65488178,  0.59397972],\n",
       "        [ 0.66678458,  0.59285426]],\n",
       "\n",
       "       [[ 0.6253373 ,  0.5761373 ],\n",
       "        [ 0.64752412,  0.59230882],\n",
       "        [ 0.65805268,  0.59046268],\n",
       "        [ 0.66942185,  0.59002632]]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y_hat_array = inference_model(latent_dim, test_X, timestepY, ndimY, encoder_inputs, encoder_states)\n",
    "test_Y_hat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 5.311 RMSE\n",
      "Real\t:\n",
      " [[  0.97000003  13.80000019]\n",
      " [ -0.19999999  14.        ]\n",
      " [ -0.81        14.19999981]],\n",
      "Predict\t:\n",
      " [[  0.20233619  10.82247734]\n",
      " [  0.24508941  10.86534214]\n",
      " [  0.27817369  10.75643826]]\n"
     ]
    }
   ],
   "source": [
    "#test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y in test_decoder_target_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[0][:ylen], test_Y_hat[0][:ylen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __I’m given a big sequence (e.g. Time Series) and I split it into smaller sequences to construct my input matrix `X`. Is it possible that the LSTM may find dependencies between the sequences?__\n",
    "\n",
    "    No it’s not possible unless you go for the stateful LSTM.  \n",
    "    Most of the problems can be solved with stateless LSTM so if you go for the stateful mode, make sure you really need it.  \n",
    "    In stateless mode, long term memory does not mean that the LSTM will remember the content of the previous batches.\n",
    "    \n",
    "\n",
    "* __Why do we make the difference between stateless and stateful LSTM in Keras?__\n",
    "\n",
    "    A LSTM has cells and is therefore stateful by definition (not the same stateful meaning as used in Keras). Fabien Chollet gives this definition of statefulness:\n",
    "    > stateful: Boolean (default False).\n",
    "    > If `True`, the last state for each sample at index `i` in a batch will be used as initial state for the sample of index `i` in the following batch.\n",
    "\n",
    "    Said differently, whenever you train or test your LSTM, you first have to build your input matrix X\n",
    "    of shape `(nb_samples, timesteps, input_dim)` where your batch size divides `nb_samples`.  \n",
    "    For instance, if `nb_samples=1024` and `batch_size=64`, it means that your model will receive blocks of 64 samples,  \n",
    "    compute each output (whatever the number of `timesteps` is for every sample), average the gradients and propagate it to update the parameters vector.\n",
    "\n",
    "    By default, Keras shuffles (permutes) the samples in `X` and the dependencies between `Xi` and `Xi+1` are lost. Let’s assume there’s no shuffling in our explanation.\n",
    "\n",
    "    If the model is `stateless`, the cell states are reset at each sequence.  \n",
    "    _With the `stateful` model, **all the states are propagated to the next batch.**_  \n",
    "    It means that the state of the sample located at index `i`, `Xi` will be used in the computation of the sample `Xi+bs` in the next batch, where `bs` is the batch size (no `shuffling`).\n",
    "\n",
    "\n",
    "* __Why do Keras require the batch size in stateful mode?__\n",
    "\n",
    "    When the model is `stateless`, Keras allocates an array for the states of size `output_dim` (understand number of cells in your LSTM).  \n",
    "    At each sequence processing, this state array is reset.\n",
    "\n",
    "    __In Stateful model, Keras must propagate the previous states for each sample across the batches.__  \n",
    "    Referring to the explanation above, a sample at index `i` in batch `#1 (Xi+bs)` will know the states of the sample `i` in batch `#0 (Xi)`.  \n",
    "    In this case, the structure to store the states is of the shape `(batch_size, output_dim)`.  \n",
    "    This is the reason why you have to specify the batch size at the creation of the LSTM.  \n",
    "    If you don’t do so, Keras may raise an error to remind you: If a RNN is stateful, a complete `input_shape` must be provided (including batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RNN Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  \n",
    "[Attention Structure 3](https://distill.pub/2016/augmented-rnns/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyRNNAttention (Feed-Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyRNNAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyRNNAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, h, c\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.Wh_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wh_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # Output(ht -> yt), hidden state\n",
    "        num = len(['h_tilda'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.Wh_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"f\"; forget gate\n",
    "        # f = sigmoid(xt * Ur + ht-1 * Wr + br) \n",
    "        # New f = sigmoid(xt * Ur + ht-1 * Wr + br + context * Vr)\n",
    "        # calculate the proposal \"h\"; hidden state for now(tilda)\n",
    "        # h_tilda = tanh(xt * Uh + ht-1 * Wh + bh)\n",
    "        # New h_tilda = tanh(xt * Uh + ht-1 * Wh + bh + context * Vh)\n",
    "        h_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ht_before, self.U[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        ht = h_tilda\n",
    "       \n",
    "        # Output Activation\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht]\n",
    "        else:\n",
    "            return yt, [yt, ht]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 64)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 4, 64)             2304      \n",
      "_________________________________________________________________\n",
      "MyRNNAttentionDecoder (MyRNN (None, 4, 2)              8550      \n",
      "=================================================================\n",
      "Total params: 10,854\n",
      "Trainable params: 10,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = seq_X.shape\n",
    "_, timestepY, ndimY = padded_seq_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "i = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = Bidirectional(SimpleRNN(latent_dim, return_sequences=True), merge_mode='concat')(i)\n",
    "dec = MyRNNAttentionDecoder(latent_dim, ndimY)(enc)\n",
    "model = Model(inputs=i, outputs=dec)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/1000\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.7992 - mean_absolute_error: 0.7657 - val_loss: 1.0495 - val_mean_absolute_error: 0.8492\n",
      "Epoch 2/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.5245 - mean_absolute_error: 0.6035 - val_loss: 0.3138 - val_mean_absolute_error: 0.4400\n",
      "Epoch 3/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.2331 - mean_absolute_error: 0.4080 - val_loss: 0.3529 - val_mean_absolute_error: 0.4802\n",
      "Epoch 4/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.3170 - mean_absolute_error: 0.5087 - val_loss: 0.3315 - val_mean_absolute_error: 0.4743\n",
      "Epoch 5/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.3253 - mean_absolute_error: 0.5187 - val_loss: 0.2857 - val_mean_absolute_error: 0.4416\n",
      "Epoch 6/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.3061 - mean_absolute_error: 0.5040 - val_loss: 0.2355 - val_mean_absolute_error: 0.4065\n",
      "Epoch 7/1000\n",
      "52/52 [==============================] - 0s 304us/step - loss: 0.2676 - mean_absolute_error: 0.4679 - val_loss: 0.1800 - val_mean_absolute_error: 0.3560\n",
      "Epoch 8/1000\n",
      "52/52 [==============================] - 0s 307us/step - loss: 0.2075 - mean_absolute_error: 0.3955 - val_loss: 0.1293 - val_mean_absolute_error: 0.2984\n",
      "Epoch 9/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.1848 - mean_absolute_error: 0.3688 - val_loss: 0.1495 - val_mean_absolute_error: 0.3113\n",
      "Epoch 10/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.1911 - mean_absolute_error: 0.3540 - val_loss: 0.1248 - val_mean_absolute_error: 0.2851\n",
      "Epoch 11/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.1430 - mean_absolute_error: 0.3085 - val_loss: 0.1125 - val_mean_absolute_error: 0.2582\n",
      "Epoch 12/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.1102 - mean_absolute_error: 0.2806 - val_loss: 0.1231 - val_mean_absolute_error: 0.2714\n",
      "Epoch 13/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.1069 - mean_absolute_error: 0.2801 - val_loss: 0.0998 - val_mean_absolute_error: 0.2291\n",
      "Epoch 14/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0798 - mean_absolute_error: 0.2235 - val_loss: 0.0975 - val_mean_absolute_error: 0.2655\n",
      "Epoch 15/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0789 - mean_absolute_error: 0.2257 - val_loss: 0.1099 - val_mean_absolute_error: 0.2867\n",
      "Epoch 16/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0713 - mean_absolute_error: 0.2088 - val_loss: 0.0671 - val_mean_absolute_error: 0.2016\n",
      "Epoch 17/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0697 - mean_absolute_error: 0.2149 - val_loss: 0.0657 - val_mean_absolute_error: 0.1973\n",
      "Epoch 18/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0751 - mean_absolute_error: 0.2281 - val_loss: 0.0628 - val_mean_absolute_error: 0.1908\n",
      "Epoch 19/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0633 - mean_absolute_error: 0.1996 - val_loss: 0.0703 - val_mean_absolute_error: 0.2178\n",
      "Epoch 20/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0638 - mean_absolute_error: 0.1964 - val_loss: 0.0711 - val_mean_absolute_error: 0.2181\n",
      "Epoch 21/1000\n",
      "52/52 [==============================] - 0s 305us/step - loss: 0.0619 - mean_absolute_error: 0.1924 - val_loss: 0.0611 - val_mean_absolute_error: 0.1822\n",
      "Epoch 22/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 0.0570 - mean_absolute_error: 0.1810 - val_loss: 0.0606 - val_mean_absolute_error: 0.1799\n",
      "Epoch 23/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 0.0581 - mean_absolute_error: 0.1849 - val_loss: 0.0613 - val_mean_absolute_error: 0.1846\n",
      "Epoch 24/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0537 - mean_absolute_error: 0.1711 - val_loss: 0.0718 - val_mean_absolute_error: 0.2147\n",
      "Epoch 25/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0549 - mean_absolute_error: 0.1753 - val_loss: 0.0685 - val_mean_absolute_error: 0.2079\n",
      "Epoch 26/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0515 - mean_absolute_error: 0.1648 - val_loss: 0.0593 - val_mean_absolute_error: 0.1838\n",
      "Epoch 27/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0522 - mean_absolute_error: 0.1691 - val_loss: 0.0581 - val_mean_absolute_error: 0.1809\n",
      "Epoch 28/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0513 - mean_absolute_error: 0.1666 - val_loss: 0.0630 - val_mean_absolute_error: 0.1975\n",
      "Epoch 29/1000\n",
      "52/52 [==============================] - 0s 307us/step - loss: 0.0495 - mean_absolute_error: 0.1588 - val_loss: 0.0642 - val_mean_absolute_error: 0.2034\n",
      "Epoch 30/1000\n",
      "52/52 [==============================] - 0s 306us/step - loss: 0.0486 - mean_absolute_error: 0.1556 - val_loss: 0.0572 - val_mean_absolute_error: 0.1836\n",
      "Epoch 31/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0479 - mean_absolute_error: 0.1551 - val_loss: 0.0550 - val_mean_absolute_error: 0.1749\n",
      "Epoch 32/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0479 - mean_absolute_error: 0.1565 - val_loss: 0.0555 - val_mean_absolute_error: 0.1800\n",
      "Epoch 33/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0469 - mean_absolute_error: 0.1503 - val_loss: 0.0564 - val_mean_absolute_error: 0.1856\n",
      "Epoch 34/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0470 - mean_absolute_error: 0.1506 - val_loss: 0.0537 - val_mean_absolute_error: 0.1756\n",
      "Epoch 35/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0461 - mean_absolute_error: 0.1473 - val_loss: 0.0520 - val_mean_absolute_error: 0.1685\n",
      "Epoch 36/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 0.0460 - mean_absolute_error: 0.1478 - val_loss: 0.0529 - val_mean_absolute_error: 0.1735\n",
      "Epoch 37/1000\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0454 - mean_absolute_error: 0.1444 - val_loss: 0.0546 - val_mean_absolute_error: 0.1806\n",
      "Epoch 38/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0451 - mean_absolute_error: 0.1429 - val_loss: 0.0531 - val_mean_absolute_error: 0.1744\n",
      "Epoch 39/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0446 - mean_absolute_error: 0.1416 - val_loss: 0.0515 - val_mean_absolute_error: 0.1672\n",
      "Epoch 40/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.0444 - mean_absolute_error: 0.1424 - val_loss: 0.0519 - val_mean_absolute_error: 0.1688\n",
      "Epoch 41/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0440 - mean_absolute_error: 0.1400 - val_loss: 0.0525 - val_mean_absolute_error: 0.1714\n",
      "Epoch 42/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.0437 - mean_absolute_error: 0.1383 - val_loss: 0.0512 - val_mean_absolute_error: 0.1655\n",
      "Epoch 43/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0433 - mean_absolute_error: 0.1378 - val_loss: 0.0503 - val_mean_absolute_error: 0.1608\n",
      "Epoch 44/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 0.0431 - mean_absolute_error: 0.1378 - val_loss: 0.0507 - val_mean_absolute_error: 0.1636\n",
      "Epoch 45/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0427 - mean_absolute_error: 0.1362 - val_loss: 0.0511 - val_mean_absolute_error: 0.1658\n",
      "Epoch 46/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 0.0425 - mean_absolute_error: 0.1351 - val_loss: 0.0502 - val_mean_absolute_error: 0.1619\n",
      "Epoch 47/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0421 - mean_absolute_error: 0.1349 - val_loss: 0.0497 - val_mean_absolute_error: 0.1599\n",
      "Epoch 48/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0419 - mean_absolute_error: 0.1345 - val_loss: 0.0500 - val_mean_absolute_error: 0.1622\n",
      "Epoch 49/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0416 - mean_absolute_error: 0.1333 - val_loss: 0.0497 - val_mean_absolute_error: 0.1619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0412 - mean_absolute_error: 0.1325 - val_loss: 0.0489 - val_mean_absolute_error: 0.1582\n",
      "Epoch 51/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0410 - mean_absolute_error: 0.1324 - val_loss: 0.0487 - val_mean_absolute_error: 0.1578\n",
      "Epoch 52/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.0407 - mean_absolute_error: 0.1316 - val_loss: 0.0488 - val_mean_absolute_error: 0.1597\n",
      "Epoch 53/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0403 - mean_absolute_error: 0.1305 - val_loss: 0.0484 - val_mean_absolute_error: 0.1585\n",
      "Epoch 54/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0400 - mean_absolute_error: 0.1299 - val_loss: 0.0479 - val_mean_absolute_error: 0.1566\n",
      "Epoch 55/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0397 - mean_absolute_error: 0.1296 - val_loss: 0.0479 - val_mean_absolute_error: 0.1575\n",
      "Epoch 56/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0394 - mean_absolute_error: 0.1286 - val_loss: 0.0478 - val_mean_absolute_error: 0.1578\n",
      "Epoch 57/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0391 - mean_absolute_error: 0.1278 - val_loss: 0.0472 - val_mean_absolute_error: 0.1558\n",
      "Epoch 58/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0388 - mean_absolute_error: 0.1275 - val_loss: 0.0468 - val_mean_absolute_error: 0.1549\n",
      "Epoch 59/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0385 - mean_absolute_error: 0.1270 - val_loss: 0.0466 - val_mean_absolute_error: 0.1555\n",
      "Epoch 60/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0381 - mean_absolute_error: 0.1262 - val_loss: 0.0462 - val_mean_absolute_error: 0.1545\n",
      "Epoch 61/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0378 - mean_absolute_error: 0.1258 - val_loss: 0.0458 - val_mean_absolute_error: 0.1532\n",
      "Epoch 62/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0375 - mean_absolute_error: 0.1254 - val_loss: 0.0456 - val_mean_absolute_error: 0.1535\n",
      "Epoch 63/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 0.0371 - mean_absolute_error: 0.1248 - val_loss: 0.0453 - val_mean_absolute_error: 0.1534\n",
      "Epoch 64/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0368 - mean_absolute_error: 0.1243 - val_loss: 0.0448 - val_mean_absolute_error: 0.1523\n",
      "Epoch 65/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0364 - mean_absolute_error: 0.1240 - val_loss: 0.0445 - val_mean_absolute_error: 0.1519\n",
      "Epoch 66/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0360 - mean_absolute_error: 0.1235 - val_loss: 0.0443 - val_mean_absolute_error: 0.1518\n",
      "Epoch 67/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0356 - mean_absolute_error: 0.1229 - val_loss: 0.0438 - val_mean_absolute_error: 0.1507\n",
      "Epoch 68/1000\n",
      "52/52 [==============================] - 0s 305us/step - loss: 0.0353 - mean_absolute_error: 0.1225 - val_loss: 0.0434 - val_mean_absolute_error: 0.1499\n",
      "Epoch 69/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 0.0349 - mean_absolute_error: 0.1220 - val_loss: 0.0431 - val_mean_absolute_error: 0.1497\n",
      "Epoch 70/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0344 - mean_absolute_error: 0.1215 - val_loss: 0.0426 - val_mean_absolute_error: 0.1489\n",
      "Epoch 71/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0340 - mean_absolute_error: 0.1210 - val_loss: 0.0422 - val_mean_absolute_error: 0.1481\n",
      "Epoch 72/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0336 - mean_absolute_error: 0.1206 - val_loss: 0.0418 - val_mean_absolute_error: 0.1478\n",
      "Epoch 73/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0332 - mean_absolute_error: 0.1200 - val_loss: 0.0414 - val_mean_absolute_error: 0.1473\n",
      "Epoch 74/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0327 - mean_absolute_error: 0.1195 - val_loss: 0.0410 - val_mean_absolute_error: 0.1464\n",
      "Epoch 75/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 0.0322 - mean_absolute_error: 0.1189 - val_loss: 0.0405 - val_mean_absolute_error: 0.1459\n",
      "Epoch 76/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0318 - mean_absolute_error: 0.1184 - val_loss: 0.0401 - val_mean_absolute_error: 0.1453\n",
      "Epoch 77/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0313 - mean_absolute_error: 0.1178 - val_loss: 0.0396 - val_mean_absolute_error: 0.1445\n",
      "Epoch 78/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0308 - mean_absolute_error: 0.1172 - val_loss: 0.0392 - val_mean_absolute_error: 0.1439\n",
      "Epoch 79/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0302 - mean_absolute_error: 0.1165 - val_loss: 0.0387 - val_mean_absolute_error: 0.1433\n",
      "Epoch 80/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0297 - mean_absolute_error: 0.1159 - val_loss: 0.0382 - val_mean_absolute_error: 0.1425\n",
      "Epoch 81/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0292 - mean_absolute_error: 0.1152 - val_loss: 0.0377 - val_mean_absolute_error: 0.1417\n",
      "Epoch 82/1000\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0286 - mean_absolute_error: 0.1145 - val_loss: 0.0372 - val_mean_absolute_error: 0.1411\n",
      "Epoch 83/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0281 - mean_absolute_error: 0.1138 - val_loss: 0.0366 - val_mean_absolute_error: 0.1403\n",
      "Epoch 84/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0275 - mean_absolute_error: 0.1130 - val_loss: 0.0361 - val_mean_absolute_error: 0.1394\n",
      "Epoch 85/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0269 - mean_absolute_error: 0.1122 - val_loss: 0.0356 - val_mean_absolute_error: 0.1387\n",
      "Epoch 86/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0264 - mean_absolute_error: 0.1114 - val_loss: 0.0350 - val_mean_absolute_error: 0.1378\n",
      "Epoch 87/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0258 - mean_absolute_error: 0.1106 - val_loss: 0.0344 - val_mean_absolute_error: 0.1369\n",
      "Epoch 88/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 0.0252 - mean_absolute_error: 0.1098 - val_loss: 0.0339 - val_mean_absolute_error: 0.1361\n",
      "Epoch 89/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0246 - mean_absolute_error: 0.1089 - val_loss: 0.0333 - val_mean_absolute_error: 0.1351\n",
      "Epoch 90/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0240 - mean_absolute_error: 0.1080 - val_loss: 0.0327 - val_mean_absolute_error: 0.1341\n",
      "Epoch 91/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0234 - mean_absolute_error: 0.1071 - val_loss: 0.0321 - val_mean_absolute_error: 0.1331\n",
      "Epoch 92/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0228 - mean_absolute_error: 0.1062 - val_loss: 0.0315 - val_mean_absolute_error: 0.1321\n",
      "Epoch 93/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0223 - mean_absolute_error: 0.1052 - val_loss: 0.0309 - val_mean_absolute_error: 0.1310\n",
      "Epoch 94/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0217 - mean_absolute_error: 0.1043 - val_loss: 0.0304 - val_mean_absolute_error: 0.1300\n",
      "Epoch 95/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0211 - mean_absolute_error: 0.1033 - val_loss: 0.0298 - val_mean_absolute_error: 0.1290\n",
      "Epoch 96/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.0206 - mean_absolute_error: 0.1023 - val_loss: 0.0292 - val_mean_absolute_error: 0.1279\n",
      "Epoch 97/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0201 - mean_absolute_error: 0.1013 - val_loss: 0.0287 - val_mean_absolute_error: 0.1269\n",
      "Epoch 98/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0196 - mean_absolute_error: 0.1003 - val_loss: 0.0281 - val_mean_absolute_error: 0.1259\n",
      "Epoch 99/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 326us/step - loss: 0.0191 - mean_absolute_error: 0.0993 - val_loss: 0.0276 - val_mean_absolute_error: 0.1248\n",
      "Epoch 100/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0186 - mean_absolute_error: 0.0983 - val_loss: 0.0270 - val_mean_absolute_error: 0.1238\n",
      "Epoch 101/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.0181 - mean_absolute_error: 0.0973 - val_loss: 0.0265 - val_mean_absolute_error: 0.1227\n",
      "Epoch 102/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0176 - mean_absolute_error: 0.0963 - val_loss: 0.0259 - val_mean_absolute_error: 0.1216\n",
      "Epoch 103/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0172 - mean_absolute_error: 0.0953 - val_loss: 0.0254 - val_mean_absolute_error: 0.1205\n",
      "Epoch 104/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0167 - mean_absolute_error: 0.0944 - val_loss: 0.0249 - val_mean_absolute_error: 0.1194\n",
      "Epoch 105/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0163 - mean_absolute_error: 0.0934 - val_loss: 0.0244 - val_mean_absolute_error: 0.1184\n",
      "Epoch 106/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0159 - mean_absolute_error: 0.0924 - val_loss: 0.0239 - val_mean_absolute_error: 0.1173\n",
      "Epoch 107/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0155 - mean_absolute_error: 0.0915 - val_loss: 0.0234 - val_mean_absolute_error: 0.1161\n",
      "Epoch 108/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 0.0151 - mean_absolute_error: 0.0906 - val_loss: 0.0229 - val_mean_absolute_error: 0.1150\n",
      "Epoch 109/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0147 - mean_absolute_error: 0.0897 - val_loss: 0.0225 - val_mean_absolute_error: 0.1140\n",
      "Epoch 110/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0144 - mean_absolute_error: 0.0888 - val_loss: 0.0220 - val_mean_absolute_error: 0.1130\n",
      "Epoch 111/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0140 - mean_absolute_error: 0.0879 - val_loss: 0.0215 - val_mean_absolute_error: 0.1120\n",
      "Epoch 112/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0137 - mean_absolute_error: 0.0870 - val_loss: 0.0211 - val_mean_absolute_error: 0.1110\n",
      "Epoch 113/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0133 - mean_absolute_error: 0.0861 - val_loss: 0.0207 - val_mean_absolute_error: 0.1100\n",
      "Epoch 114/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0130 - mean_absolute_error: 0.0852 - val_loss: 0.0202 - val_mean_absolute_error: 0.1090\n",
      "Epoch 115/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0127 - mean_absolute_error: 0.0843 - val_loss: 0.0198 - val_mean_absolute_error: 0.1080\n",
      "Epoch 116/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0124 - mean_absolute_error: 0.0834 - val_loss: 0.0194 - val_mean_absolute_error: 0.1070\n",
      "Epoch 117/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0121 - mean_absolute_error: 0.0825 - val_loss: 0.0191 - val_mean_absolute_error: 0.1060\n",
      "Epoch 118/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0118 - mean_absolute_error: 0.0817 - val_loss: 0.0187 - val_mean_absolute_error: 0.1050\n",
      "Epoch 119/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0116 - mean_absolute_error: 0.0809 - val_loss: 0.0183 - val_mean_absolute_error: 0.1041\n",
      "Epoch 120/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0113 - mean_absolute_error: 0.0801 - val_loss: 0.0180 - val_mean_absolute_error: 0.1031\n",
      "Epoch 121/1000\n",
      "52/52 [==============================] - 0s 306us/step - loss: 0.0111 - mean_absolute_error: 0.0793 - val_loss: 0.0176 - val_mean_absolute_error: 0.1022\n",
      "Epoch 122/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0108 - mean_absolute_error: 0.0785 - val_loss: 0.0173 - val_mean_absolute_error: 0.1013\n",
      "Epoch 123/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0106 - mean_absolute_error: 0.0778 - val_loss: 0.0170 - val_mean_absolute_error: 0.1006\n",
      "Epoch 124/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0104 - mean_absolute_error: 0.0770 - val_loss: 0.0167 - val_mean_absolute_error: 0.0998\n",
      "Epoch 125/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0102 - mean_absolute_error: 0.0763 - val_loss: 0.0164 - val_mean_absolute_error: 0.0990\n",
      "Epoch 126/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0101 - mean_absolute_error: 0.077 - 0s 312us/step - loss: 0.0100 - mean_absolute_error: 0.0755 - val_loss: 0.0161 - val_mean_absolute_error: 0.0983\n",
      "Epoch 127/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0098 - mean_absolute_error: 0.0748 - val_loss: 0.0158 - val_mean_absolute_error: 0.0976\n",
      "Epoch 128/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0096 - mean_absolute_error: 0.0741 - val_loss: 0.0156 - val_mean_absolute_error: 0.0969\n",
      "Epoch 129/1000\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0094 - mean_absolute_error: 0.0734 - val_loss: 0.0153 - val_mean_absolute_error: 0.0962\n",
      "Epoch 130/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.0092 - mean_absolute_error: 0.0728 - val_loss: 0.0151 - val_mean_absolute_error: 0.0955\n",
      "Epoch 131/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0090 - mean_absolute_error: 0.0721 - val_loss: 0.0148 - val_mean_absolute_error: 0.0949\n",
      "Epoch 132/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0089 - mean_absolute_error: 0.0715 - val_loss: 0.0146 - val_mean_absolute_error: 0.0942\n",
      "Epoch 133/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0087 - mean_absolute_error: 0.0708 - val_loss: 0.0144 - val_mean_absolute_error: 0.0936\n",
      "Epoch 134/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0085 - mean_absolute_error: 0.0702 - val_loss: 0.0142 - val_mean_absolute_error: 0.0930\n",
      "Epoch 135/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0084 - mean_absolute_error: 0.0695 - val_loss: 0.0140 - val_mean_absolute_error: 0.0924\n",
      "Epoch 136/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0082 - mean_absolute_error: 0.0689 - val_loss: 0.0138 - val_mean_absolute_error: 0.0918\n",
      "Epoch 137/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 0.0081 - mean_absolute_error: 0.0683 - val_loss: 0.0136 - val_mean_absolute_error: 0.0913\n",
      "Epoch 138/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0080 - mean_absolute_error: 0.0676 - val_loss: 0.0134 - val_mean_absolute_error: 0.0907\n",
      "Epoch 139/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0078 - mean_absolute_error: 0.0670 - val_loss: 0.0132 - val_mean_absolute_error: 0.0902\n",
      "Epoch 140/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0077 - mean_absolute_error: 0.0664 - val_loss: 0.0130 - val_mean_absolute_error: 0.0897\n",
      "Epoch 141/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0075 - mean_absolute_error: 0.0658 - val_loss: 0.0128 - val_mean_absolute_error: 0.0892\n",
      "Epoch 142/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0074 - mean_absolute_error: 0.0652 - val_loss: 0.0127 - val_mean_absolute_error: 0.0887\n",
      "Epoch 143/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0073 - mean_absolute_error: 0.0646 - val_loss: 0.0125 - val_mean_absolute_error: 0.0882\n",
      "Epoch 144/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0072 - mean_absolute_error: 0.0640 - val_loss: 0.0123 - val_mean_absolute_error: 0.0877\n",
      "Epoch 145/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 0.0070 - mean_absolute_error: 0.0634 - val_loss: 0.0122 - val_mean_absolute_error: 0.0873\n",
      "Epoch 146/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0069 - mean_absolute_error: 0.0629 - val_loss: 0.0120 - val_mean_absolute_error: 0.0868\n",
      "Epoch 147/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0068 - mean_absolute_error: 0.0623 - val_loss: 0.0119 - val_mean_absolute_error: 0.0864\n",
      "Epoch 148/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0067 - mean_absolute_error: 0.0618 - val_loss: 0.0117 - val_mean_absolute_error: 0.0859\n",
      "Epoch 149/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0066 - mean_absolute_error: 0.0612 - val_loss: 0.0116 - val_mean_absolute_error: 0.0855\n",
      "Epoch 150/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.0065 - mean_absolute_error: 0.0607 - val_loss: 0.0114 - val_mean_absolute_error: 0.0850\n",
      "Epoch 151/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0064 - mean_absolute_error: 0.0601 - val_loss: 0.0113 - val_mean_absolute_error: 0.0846\n",
      "Epoch 152/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0063 - mean_absolute_error: 0.0596 - val_loss: 0.0112 - val_mean_absolute_error: 0.0841\n",
      "Epoch 153/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0062 - mean_absolute_error: 0.0591 - val_loss: 0.0110 - val_mean_absolute_error: 0.0837\n",
      "Epoch 154/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0061 - mean_absolute_error: 0.0586 - val_loss: 0.0109 - val_mean_absolute_error: 0.0833\n",
      "Epoch 155/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0060 - mean_absolute_error: 0.0581 - val_loss: 0.0108 - val_mean_absolute_error: 0.0828\n",
      "Epoch 156/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0059 - mean_absolute_error: 0.0576 - val_loss: 0.0107 - val_mean_absolute_error: 0.0824\n",
      "Epoch 157/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0058 - mean_absolute_error: 0.0571 - val_loss: 0.0105 - val_mean_absolute_error: 0.0820\n",
      "Epoch 158/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0057 - mean_absolute_error: 0.0566 - val_loss: 0.0104 - val_mean_absolute_error: 0.0815\n",
      "Epoch 159/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 0.0056 - mean_absolute_error: 0.0561 - val_loss: 0.0103 - val_mean_absolute_error: 0.0811\n",
      "Epoch 160/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 0.0055 - mean_absolute_error: 0.0557 - val_loss: 0.0102 - val_mean_absolute_error: 0.0807\n",
      "Epoch 161/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 0.0054 - mean_absolute_error: 0.0552 - val_loss: 0.0101 - val_mean_absolute_error: 0.0803\n",
      "Epoch 162/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0054 - mean_absolute_error: 0.0547 - val_loss: 0.0100 - val_mean_absolute_error: 0.0799\n",
      "Epoch 163/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0053 - mean_absolute_error: 0.0543 - val_loss: 0.0098 - val_mean_absolute_error: 0.0795\n",
      "Epoch 164/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 0.0052 - mean_absolute_error: 0.0538 - val_loss: 0.0097 - val_mean_absolute_error: 0.0790\n",
      "Epoch 165/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0051 - mean_absolute_error: 0.0534 - val_loss: 0.0096 - val_mean_absolute_error: 0.0786\n",
      "Epoch 166/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0050 - mean_absolute_error: 0.0529 - val_loss: 0.0095 - val_mean_absolute_error: 0.0782\n",
      "Epoch 167/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0050 - mean_absolute_error: 0.0525 - val_loss: 0.0094 - val_mean_absolute_error: 0.0778\n",
      "Epoch 168/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0049 - mean_absolute_error: 0.0521 - val_loss: 0.0093 - val_mean_absolute_error: 0.0774\n",
      "Epoch 169/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0048 - mean_absolute_error: 0.0516 - val_loss: 0.0092 - val_mean_absolute_error: 0.0770\n",
      "Epoch 170/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0047 - mean_absolute_error: 0.0512 - val_loss: 0.0091 - val_mean_absolute_error: 0.0766\n",
      "Epoch 171/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0047 - mean_absolute_error: 0.0508 - val_loss: 0.0090 - val_mean_absolute_error: 0.0762\n",
      "Epoch 172/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0046 - mean_absolute_error: 0.0504 - val_loss: 0.0089 - val_mean_absolute_error: 0.0759\n",
      "Epoch 173/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0045 - mean_absolute_error: 0.0500 - val_loss: 0.0088 - val_mean_absolute_error: 0.0755\n",
      "Epoch 174/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0045 - mean_absolute_error: 0.0496 - val_loss: 0.0088 - val_mean_absolute_error: 0.0751\n",
      "Epoch 175/1000\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0044 - mean_absolute_error: 0.0492 - val_loss: 0.0087 - val_mean_absolute_error: 0.0748\n",
      "Epoch 176/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0043 - mean_absolute_error: 0.0488 - val_loss: 0.0086 - val_mean_absolute_error: 0.0744\n",
      "Epoch 177/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0043 - mean_absolute_error: 0.0484 - val_loss: 0.0085 - val_mean_absolute_error: 0.0741\n",
      "Epoch 178/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0042 - mean_absolute_error: 0.0481 - val_loss: 0.0084 - val_mean_absolute_error: 0.0737\n",
      "Epoch 179/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0042 - mean_absolute_error: 0.0477 - val_loss: 0.0083 - val_mean_absolute_error: 0.0734\n",
      "Epoch 180/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0041 - mean_absolute_error: 0.0473 - val_loss: 0.0082 - val_mean_absolute_error: 0.0731\n",
      "Epoch 181/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0040 - mean_absolute_error: 0.0470 - val_loss: 0.0082 - val_mean_absolute_error: 0.0727\n",
      "Epoch 182/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0040 - mean_absolute_error: 0.0466 - val_loss: 0.0081 - val_mean_absolute_error: 0.0724\n",
      "Epoch 183/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0039 - mean_absolute_error: 0.0462 - val_loss: 0.0080 - val_mean_absolute_error: 0.0721\n",
      "Epoch 184/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0039 - mean_absolute_error: 0.0459 - val_loss: 0.0079 - val_mean_absolute_error: 0.0718\n",
      "Epoch 185/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0038 - mean_absolute_error: 0.0456 - val_loss: 0.0078 - val_mean_absolute_error: 0.0715\n",
      "Epoch 186/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0038 - mean_absolute_error: 0.0452 - val_loss: 0.0078 - val_mean_absolute_error: 0.0711\n",
      "Epoch 187/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0037 - mean_absolute_error: 0.0449 - val_loss: 0.0077 - val_mean_absolute_error: 0.0708\n",
      "Epoch 188/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0037 - mean_absolute_error: 0.0446 - val_loss: 0.0076 - val_mean_absolute_error: 0.0705\n",
      "Epoch 189/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0036 - mean_absolute_error: 0.0443 - val_loss: 0.0076 - val_mean_absolute_error: 0.0702\n",
      "Epoch 190/1000\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0036 - mean_absolute_error: 0.0439 - val_loss: 0.0075 - val_mean_absolute_error: 0.0699\n",
      "Epoch 191/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0035 - mean_absolute_error: 0.0436 - val_loss: 0.0074 - val_mean_absolute_error: 0.0696\n",
      "Epoch 192/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0035 - mean_absolute_error: 0.0433 - val_loss: 0.0073 - val_mean_absolute_error: 0.0693\n",
      "Epoch 193/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0034 - mean_absolute_error: 0.0430 - val_loss: 0.0073 - val_mean_absolute_error: 0.0690\n",
      "Epoch 194/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0034 - mean_absolute_error: 0.0427 - val_loss: 0.0072 - val_mean_absolute_error: 0.0687\n",
      "Epoch 195/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0033 - mean_absolute_error: 0.0424 - val_loss: 0.0071 - val_mean_absolute_error: 0.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0033 - mean_absolute_error: 0.0422 - val_loss: 0.0071 - val_mean_absolute_error: 0.0681\n",
      "Epoch 197/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0033 - mean_absolute_error: 0.0419 - val_loss: 0.0070 - val_mean_absolute_error: 0.0678\n",
      "Epoch 198/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0032 - mean_absolute_error: 0.0416 - val_loss: 0.0070 - val_mean_absolute_error: 0.0675\n",
      "Epoch 199/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 0.0032 - mean_absolute_error: 0.0413 - val_loss: 0.0069 - val_mean_absolute_error: 0.0672\n",
      "Epoch 200/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0031 - mean_absolute_error: 0.0410 - val_loss: 0.0068 - val_mean_absolute_error: 0.0670\n",
      "Epoch 201/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0031 - mean_absolute_error: 0.0408 - val_loss: 0.0068 - val_mean_absolute_error: 0.0667\n",
      "Epoch 202/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0031 - mean_absolute_error: 0.0405 - val_loss: 0.0067 - val_mean_absolute_error: 0.0664\n",
      "Epoch 203/1000\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0030 - mean_absolute_error: 0.0402 - val_loss: 0.0067 - val_mean_absolute_error: 0.0661\n",
      "Epoch 204/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0030 - mean_absolute_error: 0.0400 - val_loss: 0.0066 - val_mean_absolute_error: 0.0658\n",
      "Epoch 205/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 0.0030 - mean_absolute_error: 0.0397 - val_loss: 0.0066 - val_mean_absolute_error: 0.0655\n",
      "Epoch 206/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0029 - mean_absolute_error: 0.0395 - val_loss: 0.0065 - val_mean_absolute_error: 0.0653\n",
      "Epoch 207/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0029 - mean_absolute_error: 0.0392 - val_loss: 0.0064 - val_mean_absolute_error: 0.0650\n",
      "Epoch 208/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0029 - mean_absolute_error: 0.0389 - val_loss: 0.0064 - val_mean_absolute_error: 0.0647\n",
      "Epoch 209/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0028 - mean_absolute_error: 0.0387 - val_loss: 0.0063 - val_mean_absolute_error: 0.0644\n",
      "Epoch 210/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0028 - mean_absolute_error: 0.0385 - val_loss: 0.0063 - val_mean_absolute_error: 0.0642\n",
      "Epoch 211/1000\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0028 - mean_absolute_error: 0.0382 - val_loss: 0.0062 - val_mean_absolute_error: 0.0639\n",
      "Epoch 212/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0027 - mean_absolute_error: 0.0380 - val_loss: 0.0062 - val_mean_absolute_error: 0.0637\n",
      "Epoch 213/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0027 - mean_absolute_error: 0.0377 - val_loss: 0.0061 - val_mean_absolute_error: 0.0634\n",
      "Epoch 214/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.0027 - mean_absolute_error: 0.0375 - val_loss: 0.0061 - val_mean_absolute_error: 0.0632\n",
      "Epoch 215/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0026 - mean_absolute_error: 0.0373 - val_loss: 0.0060 - val_mean_absolute_error: 0.0629\n",
      "Epoch 216/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0026 - mean_absolute_error: 0.0370 - val_loss: 0.0060 - val_mean_absolute_error: 0.0627\n",
      "Epoch 217/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0026 - mean_absolute_error: 0.0368 - val_loss: 0.0059 - val_mean_absolute_error: 0.0624\n",
      "Epoch 218/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0026 - mean_absolute_error: 0.0366 - val_loss: 0.0059 - val_mean_absolute_error: 0.0622\n",
      "Epoch 219/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0025 - mean_absolute_error: 0.0363 - val_loss: 0.0059 - val_mean_absolute_error: 0.0620\n",
      "Epoch 220/1000\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0025 - mean_absolute_error: 0.0361 - val_loss: 0.0058 - val_mean_absolute_error: 0.0617\n",
      "Epoch 221/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0058 - val_mean_absolute_error: 0.0615\n",
      "Epoch 222/1000\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0025 - mean_absolute_error: 0.0357 - val_loss: 0.0057 - val_mean_absolute_error: 0.0613\n",
      "Epoch 223/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 0.0024 - mean_absolute_error: 0.0355 - val_loss: 0.0057 - val_mean_absolute_error: 0.0610\n",
      "Epoch 224/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0024 - mean_absolute_error: 0.0353 - val_loss: 0.0056 - val_mean_absolute_error: 0.0608\n",
      "Epoch 225/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0024 - mean_absolute_error: 0.0350 - val_loss: 0.0056 - val_mean_absolute_error: 0.0606\n",
      "Epoch 226/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0024 - mean_absolute_error: 0.0349 - val_loss: 0.0056 - val_mean_absolute_error: 0.0604\n",
      "Epoch 227/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0023 - mean_absolute_error: 0.0347 - val_loss: 0.0055 - val_mean_absolute_error: 0.0601\n",
      "Epoch 228/1000\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0345 - val_loss: 0.0055 - val_mean_absolute_error: 0.0599\n",
      "Epoch 229/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0343 - val_loss: 0.0054 - val_mean_absolute_error: 0.0597\n",
      "Epoch 230/1000\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0341 - val_loss: 0.0054 - val_mean_absolute_error: 0.0595\n",
      "Epoch 231/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0022 - mean_absolute_error: 0.0339 - val_loss: 0.0054 - val_mean_absolute_error: 0.0593\n",
      "Epoch 232/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0022 - mean_absolute_error: 0.0337 - val_loss: 0.0053 - val_mean_absolute_error: 0.0591\n",
      "Epoch 233/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0022 - mean_absolute_error: 0.0336 - val_loss: 0.0053 - val_mean_absolute_error: 0.0588\n",
      "Epoch 234/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0022 - mean_absolute_error: 0.0334 - val_loss: 0.0052 - val_mean_absolute_error: 0.0587\n",
      "Epoch 235/1000\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0022 - mean_absolute_error: 0.0332 - val_loss: 0.0052 - val_mean_absolute_error: 0.0584\n",
      "Epoch 236/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0021 - mean_absolute_error: 0.0330 - val_loss: 0.0052 - val_mean_absolute_error: 0.0583\n",
      "Epoch 237/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0021 - mean_absolute_error: 0.0329 - val_loss: 0.0051 - val_mean_absolute_error: 0.0580\n",
      "Epoch 238/1000\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0021 - mean_absolute_error: 0.0327 - val_loss: 0.0051 - val_mean_absolute_error: 0.0579\n",
      "Epoch 239/1000\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0021 - mean_absolute_error: 0.0325 - val_loss: 0.0051 - val_mean_absolute_error: 0.0576\n",
      "Epoch 240/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0021 - mean_absolute_error: 0.0324 - val_loss: 0.0050 - val_mean_absolute_error: 0.0575\n",
      "Epoch 241/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0020 - mean_absolute_error: 0.0322 - val_loss: 0.0050 - val_mean_absolute_error: 0.0572\n",
      "Epoch 242/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 0.0020 - mean_absolute_error: 0.0320 - val_loss: 0.0050 - val_mean_absolute_error: 0.0571\n",
      "Epoch 243/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0020 - mean_absolute_error: 0.0319 - val_loss: 0.0049 - val_mean_absolute_error: 0.0568\n",
      "Epoch 244/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0020 - mean_absolute_error: 0.0317 - val_loss: 0.0049 - val_mean_absolute_error: 0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0020 - mean_absolute_error: 0.0315 - val_loss: 0.0049 - val_mean_absolute_error: 0.0564\n",
      "Epoch 246/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0019 - mean_absolute_error: 0.0314 - val_loss: 0.0049 - val_mean_absolute_error: 0.0564\n",
      "Epoch 247/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0019 - mean_absolute_error: 0.0312 - val_loss: 0.0048 - val_mean_absolute_error: 0.0560\n",
      "Epoch 248/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0019 - mean_absolute_error: 0.0311 - val_loss: 0.0048 - val_mean_absolute_error: 0.0561\n",
      "Epoch 249/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0019 - mean_absolute_error: 0.0309 - val_loss: 0.0047 - val_mean_absolute_error: 0.0556\n",
      "Epoch 250/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.0019 - mean_absolute_error: 0.0308 - val_loss: 0.0047 - val_mean_absolute_error: 0.0557\n",
      "Epoch 251/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0019 - mean_absolute_error: 0.0306 - val_loss: 0.0047 - val_mean_absolute_error: 0.0553\n",
      "Epoch 252/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0018 - mean_absolute_error: 0.0305 - val_loss: 0.0047 - val_mean_absolute_error: 0.0554\n",
      "Epoch 253/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.0018 - mean_absolute_error: 0.0303 - val_loss: 0.0046 - val_mean_absolute_error: 0.0549\n",
      "Epoch 254/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0018 - mean_absolute_error: 0.0303 - val_loss: 0.0046 - val_mean_absolute_error: 0.0551\n",
      "Epoch 255/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0018 - mean_absolute_error: 0.0300 - val_loss: 0.0046 - val_mean_absolute_error: 0.0545\n",
      "Epoch 256/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0018 - mean_absolute_error: 0.0300 - val_loss: 0.0046 - val_mean_absolute_error: 0.0549\n",
      "Epoch 257/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0018 - mean_absolute_error: 0.0297 - val_loss: 0.0045 - val_mean_absolute_error: 0.0542\n",
      "Epoch 258/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0018 - mean_absolute_error: 0.0298 - val_loss: 0.0045 - val_mean_absolute_error: 0.0546\n",
      "Epoch 259/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0044 - val_mean_absolute_error: 0.0538\n",
      "Epoch 260/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0045 - val_mean_absolute_error: 0.0543\n",
      "Epoch 261/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0017 - mean_absolute_error: 0.0292 - val_loss: 0.0044 - val_mean_absolute_error: 0.0535\n",
      "Epoch 262/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.0017 - mean_absolute_error: 0.0293 - val_loss: 0.0045 - val_mean_absolute_error: 0.0540\n",
      "Epoch 263/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0017 - mean_absolute_error: 0.0290 - val_loss: 0.0043 - val_mean_absolute_error: 0.0531\n",
      "Epoch 264/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 0.0017 - mean_absolute_error: 0.0291 - val_loss: 0.0044 - val_mean_absolute_error: 0.0537\n",
      "Epoch 265/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0017 - mean_absolute_error: 0.0287 - val_loss: 0.0043 - val_mean_absolute_error: 0.0528\n",
      "Epoch 266/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0017 - mean_absolute_error: 0.0289 - val_loss: 0.0043 - val_mean_absolute_error: 0.0534\n",
      "Epoch 267/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0016 - mean_absolute_error: 0.0285 - val_loss: 0.0042 - val_mean_absolute_error: 0.0526\n",
      "Epoch 268/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0016 - mean_absolute_error: 0.0287 - val_loss: 0.0043 - val_mean_absolute_error: 0.0530\n",
      "Epoch 269/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0016 - mean_absolute_error: 0.0283 - val_loss: 0.0042 - val_mean_absolute_error: 0.0523\n",
      "Epoch 270/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.029 - 0s 320us/step - loss: 0.0016 - mean_absolute_error: 0.0284 - val_loss: 0.0042 - val_mean_absolute_error: 0.0527\n",
      "Epoch 271/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0016 - mean_absolute_error: 0.0281 - val_loss: 0.0042 - val_mean_absolute_error: 0.0522\n",
      "Epoch 272/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0016 - mean_absolute_error: 0.0281 - val_loss: 0.0042 - val_mean_absolute_error: 0.0523\n",
      "Epoch 273/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0016 - mean_absolute_error: 0.0278 - val_loss: 0.0041 - val_mean_absolute_error: 0.0520\n",
      "Epoch 274/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 0.0015 - mean_absolute_error: 0.0278 - val_loss: 0.0041 - val_mean_absolute_error: 0.0520\n",
      "Epoch 275/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0015 - mean_absolute_error: 0.0276 - val_loss: 0.0041 - val_mean_absolute_error: 0.0519\n",
      "Epoch 276/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0015 - mean_absolute_error: 0.0274 - val_loss: 0.0041 - val_mean_absolute_error: 0.0517\n",
      "Epoch 277/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.0015 - mean_absolute_error: 0.0273 - val_loss: 0.0041 - val_mean_absolute_error: 0.0518\n",
      "Epoch 278/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0015 - mean_absolute_error: 0.0271 - val_loss: 0.0040 - val_mean_absolute_error: 0.0514\n",
      "Epoch 279/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0015 - mean_absolute_error: 0.0271 - val_loss: 0.0040 - val_mean_absolute_error: 0.0516\n",
      "Epoch 280/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0015 - mean_absolute_error: 0.0269 - val_loss: 0.0040 - val_mean_absolute_error: 0.0511\n",
      "Epoch 281/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0015 - mean_absolute_error: 0.0268 - val_loss: 0.0040 - val_mean_absolute_error: 0.0513\n",
      "Epoch 282/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 0.0014 - mean_absolute_error: 0.0266 - val_loss: 0.0039 - val_mean_absolute_error: 0.0509\n",
      "Epoch 283/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 0.0014 - mean_absolute_error: 0.0266 - val_loss: 0.0040 - val_mean_absolute_error: 0.0511\n",
      "Epoch 284/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0014 - mean_absolute_error: 0.0264 - val_loss: 0.0039 - val_mean_absolute_error: 0.0507\n",
      "Epoch 285/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0014 - mean_absolute_error: 0.0264 - val_loss: 0.0039 - val_mean_absolute_error: 0.0508\n",
      "Epoch 286/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0014 - mean_absolute_error: 0.0262 - val_loss: 0.0039 - val_mean_absolute_error: 0.0506\n",
      "Epoch 287/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0014 - mean_absolute_error: 0.0261 - val_loss: 0.0039 - val_mean_absolute_error: 0.0504\n",
      "Epoch 288/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0014 - mean_absolute_error: 0.0260 - val_loss: 0.0039 - val_mean_absolute_error: 0.0504\n",
      "Epoch 289/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0014 - mean_absolute_error: 0.0259 - val_loss: 0.0038 - val_mean_absolute_error: 0.0502\n",
      "Epoch 290/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 0.0014 - mean_absolute_error: 0.0258 - val_loss: 0.0038 - val_mean_absolute_error: 0.0502\n",
      "Epoch 291/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0013 - mean_absolute_error: 0.0257 - val_loss: 0.0038 - val_mean_absolute_error: 0.0499\n",
      "Epoch 292/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0013 - mean_absolute_error: 0.0256 - val_loss: 0.0038 - val_mean_absolute_error: 0.0500\n",
      "Epoch 293/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0013 - mean_absolute_error: 0.0255 - val_loss: 0.0038 - val_mean_absolute_error: 0.0498\n",
      "Epoch 294/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0013 - mean_absolute_error: 0.0254 - val_loss: 0.0037 - val_mean_absolute_error: 0.0497\n",
      "Epoch 295/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0013 - mean_absolute_error: 0.0253 - val_loss: 0.0037 - val_mean_absolute_error: 0.0496\n",
      "Epoch 296/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0013 - mean_absolute_error: 0.0252 - val_loss: 0.0037 - val_mean_absolute_error: 0.0495\n",
      "Epoch 297/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0013 - mean_absolute_error: 0.0251 - val_loss: 0.0037 - val_mean_absolute_error: 0.0494\n",
      "Epoch 298/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0013 - mean_absolute_error: 0.0250 - val_loss: 0.0037 - val_mean_absolute_error: 0.0493\n",
      "Epoch 299/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0013 - mean_absolute_error: 0.0249 - val_loss: 0.0037 - val_mean_absolute_error: 0.0492\n",
      "Epoch 300/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0013 - mean_absolute_error: 0.0248 - val_loss: 0.0036 - val_mean_absolute_error: 0.0491\n",
      "Epoch 301/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0013 - mean_absolute_error: 0.0248 - val_loss: 0.0036 - val_mean_absolute_error: 0.0490\n",
      "Epoch 302/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 0.0012 - mean_absolute_error: 0.0247 - val_loss: 0.0036 - val_mean_absolute_error: 0.0489\n",
      "Epoch 303/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 0.0012 - mean_absolute_error: 0.0246 - val_loss: 0.0036 - val_mean_absolute_error: 0.0488\n",
      "Epoch 304/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0012 - mean_absolute_error: 0.0245 - val_loss: 0.0036 - val_mean_absolute_error: 0.0488\n",
      "Epoch 305/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.0012 - mean_absolute_error: 0.0244 - val_loss: 0.0036 - val_mean_absolute_error: 0.0486\n",
      "Epoch 306/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 0.0012 - mean_absolute_error: 0.0243 - val_loss: 0.0035 - val_mean_absolute_error: 0.0486\n",
      "Epoch 307/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0012 - mean_absolute_error: 0.0242 - val_loss: 0.0035 - val_mean_absolute_error: 0.0485\n",
      "Epoch 308/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.0012 - mean_absolute_error: 0.0241 - val_loss: 0.0035 - val_mean_absolute_error: 0.0484\n",
      "Epoch 309/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0012 - mean_absolute_error: 0.0241 - val_loss: 0.0035 - val_mean_absolute_error: 0.0483\n",
      "Epoch 310/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0012 - mean_absolute_error: 0.0240 - val_loss: 0.0035 - val_mean_absolute_error: 0.0482\n",
      "Epoch 311/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0012 - mean_absolute_error: 0.0239 - val_loss: 0.0035 - val_mean_absolute_error: 0.0481\n",
      "Epoch 312/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0012 - mean_absolute_error: 0.0238 - val_loss: 0.0035 - val_mean_absolute_error: 0.0481\n",
      "Epoch 313/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0012 - mean_absolute_error: 0.0237 - val_loss: 0.0034 - val_mean_absolute_error: 0.0480\n",
      "Epoch 314/1000\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0011 - mean_absolute_error: 0.0237 - val_loss: 0.0034 - val_mean_absolute_error: 0.0479\n",
      "Epoch 315/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0011 - mean_absolute_error: 0.0236 - val_loss: 0.0034 - val_mean_absolute_error: 0.0478\n",
      "Epoch 316/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 0.0011 - mean_absolute_error: 0.0235 - val_loss: 0.0034 - val_mean_absolute_error: 0.0477\n",
      "Epoch 317/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0011 - mean_absolute_error: 0.0234 - val_loss: 0.0034 - val_mean_absolute_error: 0.0477\n",
      "Epoch 318/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0011 - mean_absolute_error: 0.0233 - val_loss: 0.0034 - val_mean_absolute_error: 0.0476\n",
      "Epoch 319/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0011 - mean_absolute_error: 0.0233 - val_loss: 0.0034 - val_mean_absolute_error: 0.0475\n",
      "Epoch 320/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0011 - mean_absolute_error: 0.0232 - val_loss: 0.0033 - val_mean_absolute_error: 0.0474\n",
      "Epoch 321/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0011 - mean_absolute_error: 0.0231 - val_loss: 0.0033 - val_mean_absolute_error: 0.0473\n",
      "Epoch 322/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 0.0011 - mean_absolute_error: 0.0230 - val_loss: 0.0033 - val_mean_absolute_error: 0.0473\n",
      "Epoch 323/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 0.0011 - mean_absolute_error: 0.0229 - val_loss: 0.0033 - val_mean_absolute_error: 0.0472\n",
      "Epoch 324/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 0.0011 - mean_absolute_error: 0.0229 - val_loss: 0.0033 - val_mean_absolute_error: 0.0471\n",
      "Epoch 325/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 0.0011 - mean_absolute_error: 0.0228 - val_loss: 0.0033 - val_mean_absolute_error: 0.0470\n",
      "Epoch 326/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0011 - mean_absolute_error: 0.0227 - val_loss: 0.0033 - val_mean_absolute_error: 0.0469\n",
      "Epoch 327/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0010 - mean_absolute_error: 0.0226 - val_loss: 0.0032 - val_mean_absolute_error: 0.0469\n",
      "Epoch 328/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0010 - mean_absolute_error: 0.0226 - val_loss: 0.0032 - val_mean_absolute_error: 0.0468\n",
      "Epoch 329/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 0.0010 - mean_absolute_error: 0.0225 - val_loss: 0.0032 - val_mean_absolute_error: 0.0467\n",
      "Epoch 330/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0010 - mean_absolute_error: 0.0224 - val_loss: 0.0032 - val_mean_absolute_error: 0.0466\n",
      "Epoch 331/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0010 - mean_absolute_error: 0.0223 - val_loss: 0.0032 - val_mean_absolute_error: 0.0466\n",
      "Epoch 332/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0010 - mean_absolute_error: 0.0222 - val_loss: 0.0032 - val_mean_absolute_error: 0.0465\n",
      "Epoch 333/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 0.0010 - mean_absolute_error: 0.0222 - val_loss: 0.0032 - val_mean_absolute_error: 0.0464\n",
      "Epoch 334/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 9.9762e-04 - mean_absolute_error: 0.0221 - val_loss: 0.0032 - val_mean_absolute_error: 0.0463\n",
      "Epoch 335/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 9.9104e-04 - mean_absolute_error: 0.0220 - val_loss: 0.0031 - val_mean_absolute_error: 0.0463\n",
      "Epoch 336/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 9.8455e-04 - mean_absolute_error: 0.0220 - val_loss: 0.0031 - val_mean_absolute_error: 0.0462\n",
      "Epoch 337/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 9.7813e-04 - mean_absolute_error: 0.0219 - val_loss: 0.0031 - val_mean_absolute_error: 0.0461\n",
      "Epoch 338/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 9.7172e-04 - mean_absolute_error: 0.0218 - val_loss: 0.0031 - val_mean_absolute_error: 0.0461\n",
      "Epoch 339/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 9.6542e-04 - mean_absolute_error: 0.0217 - val_loss: 0.0031 - val_mean_absolute_error: 0.0460\n",
      "Epoch 340/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 9.5920e-04 - mean_absolute_error: 0.0217 - val_loss: 0.0031 - val_mean_absolute_error: 0.0459\n",
      "Epoch 341/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 9.5295e-04 - mean_absolute_error: 0.0216 - val_loss: 0.0031 - val_mean_absolute_error: 0.0458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 9.4683e-04 - mean_absolute_error: 0.0215 - val_loss: 0.0031 - val_mean_absolute_error: 0.0458\n",
      "Epoch 343/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 9.4083e-04 - mean_absolute_error: 0.0215 - val_loss: 0.0031 - val_mean_absolute_error: 0.0457\n",
      "Epoch 344/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 9.3471e-04 - mean_absolute_error: 0.0214 - val_loss: 0.0030 - val_mean_absolute_error: 0.0456\n",
      "Epoch 345/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 9.2877e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0030 - val_mean_absolute_error: 0.0456\n",
      "Epoch 346/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 9.2306e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0030 - val_mean_absolute_error: 0.0455\n",
      "Epoch 347/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 9.1699e-04 - mean_absolute_error: 0.0212 - val_loss: 0.0030 - val_mean_absolute_error: 0.0454\n",
      "Epoch 348/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 9.1123e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0030 - val_mean_absolute_error: 0.0454\n",
      "Epoch 349/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 9.0616e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0030 - val_mean_absolute_error: 0.0454\n",
      "Epoch 350/1000\n",
      "52/52 [==============================] - 0s 390us/step - loss: 8.9990e-04 - mean_absolute_error: 0.0210 - val_loss: 0.0030 - val_mean_absolute_error: 0.0451\n",
      "Epoch 351/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 8.9440e-04 - mean_absolute_error: 0.0209 - val_loss: 0.0030 - val_mean_absolute_error: 0.0452\n",
      "Epoch 352/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 8.9176e-04 - mean_absolute_error: 0.0209 - val_loss: 0.0030 - val_mean_absolute_error: 0.0453\n",
      "Epoch 353/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 8.8455e-04 - mean_absolute_error: 0.0208 - val_loss: 0.0029 - val_mean_absolute_error: 0.0446\n",
      "Epoch 354/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 8.8056e-04 - mean_absolute_error: 0.0208 - val_loss: 0.0029 - val_mean_absolute_error: 0.0450\n",
      "Epoch 355/1000\n",
      "52/52 [==============================] - 0s 392us/step - loss: 8.9222e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0030 - val_mean_absolute_error: 0.0455\n",
      "Epoch 356/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 8.8024e-04 - mean_absolute_error: 0.0210 - val_loss: 0.0028 - val_mean_absolute_error: 0.0436\n",
      "Epoch 357/1000\n",
      "52/52 [==============================] - 0s 352us/step - loss: 8.9121e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0029 - val_mean_absolute_error: 0.0449\n",
      "Epoch 358/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 0.0010 - mean_absolute_error: 0.0235 - val_loss: 0.0030 - val_mean_absolute_error: 0.0461\n",
      "Epoch 359/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 9.8829e-04 - mean_absolute_error: 0.0234 - val_loss: 0.0025 - val_mean_absolute_error: 0.0402\n",
      "Epoch 360/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0011 - mean_absolute_error: 0.0247 - val_loss: 0.0031 - val_mean_absolute_error: 0.0464\n",
      "Epoch 361/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0393 - val_loss: 0.0027 - val_mean_absolute_error: 0.0423\n",
      "Epoch 362/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 0.0037 - mean_absolute_error: 0.0462 - val_loss: 0.0032 - val_mean_absolute_error: 0.0452\n",
      "Epoch 363/1000\n",
      "52/52 [==============================] - 0s 447us/step - loss: 0.0051 - mean_absolute_error: 0.0582 - val_loss: 0.0043 - val_mean_absolute_error: 0.0506\n",
      "Epoch 364/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0177 - mean_absolute_error: 0.1007 - val_loss: 0.0067 - val_mean_absolute_error: 0.0687\n",
      "Epoch 365/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0186 - mean_absolute_error: 0.1017 - val_loss: 0.0129 - val_mean_absolute_error: 0.0973\n",
      "Epoch 366/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 0.0061 - mean_absolute_error: 0.0637 - val_loss: 0.0042 - val_mean_absolute_error: 0.0559\n",
      "Epoch 367/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 0.0100 - mean_absolute_error: 0.0701 - val_loss: 0.0129 - val_mean_absolute_error: 0.0910\n",
      "Epoch 368/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0168 - mean_absolute_error: 0.0989 - val_loss: 0.0081 - val_mean_absolute_error: 0.0791\n",
      "Epoch 369/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 0.0196 - mean_absolute_error: 0.0940 - val_loss: 0.0299 - val_mean_absolute_error: 0.1446\n",
      "Epoch 370/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0122 - mean_absolute_error: 0.0884 - val_loss: 0.0042 - val_mean_absolute_error: 0.0563\n",
      "Epoch 371/1000\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0198 - mean_absolute_error: 0.1061 - val_loss: 0.0026 - val_mean_absolute_error: 0.0388\n",
      "Epoch 372/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0069 - mean_absolute_error: 0.0596 - val_loss: 0.0034 - val_mean_absolute_error: 0.0448\n",
      "Epoch 373/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0108 - mean_absolute_error: 0.0803 - val_loss: 0.0158 - val_mean_absolute_error: 0.1098\n",
      "Epoch 374/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0153 - mean_absolute_error: 0.0945 - val_loss: 0.0075 - val_mean_absolute_error: 0.0720\n",
      "Epoch 375/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0156 - mean_absolute_error: 0.0891 - val_loss: 0.0026 - val_mean_absolute_error: 0.0414\n",
      "Epoch 376/1000\n",
      "52/52 [==============================] - 0s 402us/step - loss: 0.0048 - mean_absolute_error: 0.0538 - val_loss: 0.0028 - val_mean_absolute_error: 0.0409\n",
      "Epoch 377/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0057 - mean_absolute_error: 0.0578 - val_loss: 0.0100 - val_mean_absolute_error: 0.0824\n",
      "Epoch 378/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 0.0034 - mean_absolute_error: 0.0452 - val_loss: 0.0026 - val_mean_absolute_error: 0.0389\n",
      "Epoch 379/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 0.0040 - mean_absolute_error: 0.0498 - val_loss: 0.0034 - val_mean_absolute_error: 0.0438\n",
      "Epoch 380/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 0.0021 - mean_absolute_error: 0.0372 - val_loss: 0.0035 - val_mean_absolute_error: 0.0466\n",
      "Epoch 381/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0020 - mean_absolute_error: 0.0367 - val_loss: 0.0024 - val_mean_absolute_error: 0.0379\n",
      "Epoch 382/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 0.0023 - mean_absolute_error: 0.0373 - val_loss: 0.0057 - val_mean_absolute_error: 0.0619\n",
      "Epoch 383/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0021 - mean_absolute_error: 0.0376 - val_loss: 0.0030 - val_mean_absolute_error: 0.0467\n",
      "Epoch 384/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0015 - mean_absolute_error: 0.0303 - val_loss: 0.0026 - val_mean_absolute_error: 0.0424\n",
      "Epoch 385/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0013 - mean_absolute_error: 0.0289 - val_loss: 0.0043 - val_mean_absolute_error: 0.0552\n",
      "Epoch 386/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0018 - mean_absolute_error: 0.0332 - val_loss: 0.0023 - val_mean_absolute_error: 0.0368\n",
      "Epoch 387/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0013 - mean_absolute_error: 0.0291 - val_loss: 0.0027 - val_mean_absolute_error: 0.0425\n",
      "Epoch 388/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 9.7466e-04 - mean_absolute_error: 0.0233 - val_loss: 0.0035 - val_mean_absolute_error: 0.0489\n",
      "Epoch 389/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 9.7870e-04 - mean_absolute_error: 0.0237 - val_loss: 0.0024 - val_mean_absolute_error: 0.0390\n",
      "Epoch 390/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0012 - mean_absolute_error: 0.0262 - val_loss: 0.0034 - val_mean_absolute_error: 0.0485\n",
      "Epoch 391/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 9.5450e-04 - mean_absolute_error: 0.0238 - val_loss: 0.0027 - val_mean_absolute_error: 0.0431\n",
      "Epoch 392/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 8.6602e-04 - mean_absolute_error: 0.0219 - val_loss: 0.0022 - val_mean_absolute_error: 0.0378\n",
      "Epoch 393/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 8.6896e-04 - mean_absolute_error: 0.0220 - val_loss: 0.0030 - val_mean_absolute_error: 0.0459\n",
      "Epoch 394/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 9.6201e-04 - mean_absolute_error: 0.0234 - val_loss: 0.0024 - val_mean_absolute_error: 0.0408\n",
      "Epoch 395/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 8.1741e-04 - mean_absolute_error: 0.0212 - val_loss: 0.0026 - val_mean_absolute_error: 0.0428\n",
      "Epoch 396/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 8.1591e-04 - mean_absolute_error: 0.0210 - val_loss: 0.0031 - val_mean_absolute_error: 0.0466\n",
      "Epoch 397/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 7.8935e-04 - mean_absolute_error: 0.0205 - val_loss: 0.0024 - val_mean_absolute_error: 0.0404\n",
      "Epoch 398/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 8.0016e-04 - mean_absolute_error: 0.0209 - val_loss: 0.0027 - val_mean_absolute_error: 0.0432\n",
      "Epoch 399/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 7.6735e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0027 - val_mean_absolute_error: 0.0428\n",
      "Epoch 400/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 7.5276e-04 - mean_absolute_error: 0.0196 - val_loss: 0.0024 - val_mean_absolute_error: 0.0406\n",
      "Epoch 401/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 7.5353e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0028 - val_mean_absolute_error: 0.0443\n",
      "Epoch 402/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 7.4399e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0025 - val_mean_absolute_error: 0.0418\n",
      "Epoch 403/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 7.3147e-04 - mean_absolute_error: 0.0195 - val_loss: 0.0025 - val_mean_absolute_error: 0.0412\n",
      "Epoch 404/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 7.1916e-04 - mean_absolute_error: 0.0192 - val_loss: 0.0026 - val_mean_absolute_error: 0.0427\n",
      "Epoch 405/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 7.2612e-04 - mean_absolute_error: 0.0192 - val_loss: 0.0024 - val_mean_absolute_error: 0.0405\n",
      "Epoch 406/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 7.1536e-04 - mean_absolute_error: 0.0191 - val_loss: 0.0026 - val_mean_absolute_error: 0.0420\n",
      "Epoch 407/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 7.0725e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0026 - val_mean_absolute_error: 0.0424\n",
      "Epoch 408/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 6.9975e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0025 - val_mean_absolute_error: 0.0411\n",
      "Epoch 409/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 7.0089e-04 - mean_absolute_error: 0.0190 - val_loss: 0.0026 - val_mean_absolute_error: 0.0424\n",
      "Epoch 410/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 6.9352e-04 - mean_absolute_error: 0.0188 - val_loss: 0.0025 - val_mean_absolute_error: 0.0412\n",
      "Epoch 411/1000\n",
      "52/52 [==============================] - 0s 306us/step - loss: 6.8770e-04 - mean_absolute_error: 0.0187 - val_loss: 0.0024 - val_mean_absolute_error: 0.0410\n",
      "Epoch 412/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 6.8323e-04 - mean_absolute_error: 0.0186 - val_loss: 0.0025 - val_mean_absolute_error: 0.0418\n",
      "Epoch 413/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 6.8177e-04 - mean_absolute_error: 0.0186 - val_loss: 0.0024 - val_mean_absolute_error: 0.0409\n",
      "Epoch 414/1000\n",
      "52/52 [==============================] - 0s 350us/step - loss: 6.7737e-04 - mean_absolute_error: 0.0186 - val_loss: 0.0025 - val_mean_absolute_error: 0.0415\n",
      "Epoch 415/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 6.7302e-04 - mean_absolute_error: 0.0185 - val_loss: 0.0025 - val_mean_absolute_error: 0.0414\n",
      "Epoch 416/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 6.6901e-04 - mean_absolute_error: 0.0184 - val_loss: 0.0024 - val_mean_absolute_error: 0.0407\n",
      "Epoch 417/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 6.6620e-04 - mean_absolute_error: 0.0184 - val_loss: 0.0025 - val_mean_absolute_error: 0.0413\n",
      "Epoch 418/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 6.6352e-04 - mean_absolute_error: 0.0183 - val_loss: 0.0024 - val_mean_absolute_error: 0.0408\n",
      "Epoch 419/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 6.5943e-04 - mean_absolute_error: 0.0183 - val_loss: 0.0024 - val_mean_absolute_error: 0.0408\n",
      "Epoch 420/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 6.5585e-04 - mean_absolute_error: 0.0182 - val_loss: 0.0025 - val_mean_absolute_error: 0.0412\n",
      "Epoch 421/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 6.5304e-04 - mean_absolute_error: 0.0182 - val_loss: 0.0024 - val_mean_absolute_error: 0.0407\n",
      "Epoch 422/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 6.5017e-04 - mean_absolute_error: 0.0182 - val_loss: 0.0024 - val_mean_absolute_error: 0.0410\n",
      "Epoch 423/1000\n",
      "52/52 [==============================] - 0s 398us/step - loss: 6.4684e-04 - mean_absolute_error: 0.0181 - val_loss: 0.0024 - val_mean_absolute_error: 0.0408\n",
      "Epoch 424/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 6.5052e-04 - mean_absolute_error: 0.018 - 0s 313us/step - loss: 6.4386e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0024 - val_mean_absolute_error: 0.0405\n",
      "Epoch 425/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 6.4055e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0024 - val_mean_absolute_error: 0.0408\n",
      "Epoch 426/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 6.3796e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0024 - val_mean_absolute_error: 0.0406\n",
      "Epoch 427/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 6.3496e-04 - mean_absolute_error: 0.0179 - val_loss: 0.0024 - val_mean_absolute_error: 0.0407\n",
      "Epoch 428/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 6.3214e-04 - mean_absolute_error: 0.0179 - val_loss: 0.0024 - val_mean_absolute_error: 0.0406\n",
      "Epoch 429/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 6.2940e-04 - mean_absolute_error: 0.0178 - val_loss: 0.0024 - val_mean_absolute_error: 0.0404\n",
      "Epoch 430/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 6.2654e-04 - mean_absolute_error: 0.0178 - val_loss: 0.0024 - val_mean_absolute_error: 0.0405\n",
      "Epoch 431/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 6.2396e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0024 - val_mean_absolute_error: 0.0403\n",
      "Epoch 432/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 6.2123e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0024 - val_mean_absolute_error: 0.0403\n",
      "Epoch 433/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 6.1853e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0024 - val_mean_absolute_error: 0.0404\n",
      "Epoch 434/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 6.1598e-04 - mean_absolute_error: 0.0176 - val_loss: 0.0023 - val_mean_absolute_error: 0.0402\n",
      "Epoch 435/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 6.1329e-04 - mean_absolute_error: 0.0176 - val_loss: 0.0023 - val_mean_absolute_error: 0.0402\n",
      "Epoch 436/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 6.1073e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0023 - val_mean_absolute_error: 0.0402\n",
      "Epoch 437/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 330us/step - loss: 6.0821e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0023 - val_mean_absolute_error: 0.0401\n",
      "Epoch 438/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 6.0564e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0023 - val_mean_absolute_error: 0.0401\n",
      "Epoch 439/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 6.0321e-04 - mean_absolute_error: 0.0174 - val_loss: 0.0023 - val_mean_absolute_error: 0.0400\n",
      "Epoch 440/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 6.0069e-04 - mean_absolute_error: 0.0174 - val_loss: 0.0023 - val_mean_absolute_error: 0.0400\n",
      "Epoch 441/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 5.9826e-04 - mean_absolute_error: 0.0174 - val_loss: 0.0023 - val_mean_absolute_error: 0.0399\n",
      "Epoch 442/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 6.0207e-04 - mean_absolute_error: 0.017 - 0s 318us/step - loss: 5.9589e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0023 - val_mean_absolute_error: 0.0399\n",
      "Epoch 443/1000\n",
      "52/52 [==============================] - 0s 347us/step - loss: 5.9346e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0023 - val_mean_absolute_error: 0.0399\n",
      "Epoch 444/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 5.9113e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0023 - val_mean_absolute_error: 0.0398\n",
      "Epoch 445/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 5.8874e-04 - mean_absolute_error: 0.0172 - val_loss: 0.0023 - val_mean_absolute_error: 0.0397\n",
      "Epoch 446/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 5.8641e-04 - mean_absolute_error: 0.0172 - val_loss: 0.0023 - val_mean_absolute_error: 0.0397\n",
      "Epoch 447/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 5.8414e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0023 - val_mean_absolute_error: 0.0396\n",
      "Epoch 448/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 5.8184e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0023 - val_mean_absolute_error: 0.0396\n",
      "Epoch 449/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 5.7960e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0023 - val_mean_absolute_error: 0.0395\n",
      "Epoch 450/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 5.7735e-04 - mean_absolute_error: 0.0170 - val_loss: 0.0023 - val_mean_absolute_error: 0.0395\n",
      "Epoch 451/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 5.7512e-04 - mean_absolute_error: 0.0170 - val_loss: 0.0023 - val_mean_absolute_error: 0.0395\n",
      "Epoch 452/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 5.7294e-04 - mean_absolute_error: 0.0170 - val_loss: 0.0022 - val_mean_absolute_error: 0.0394\n",
      "Epoch 453/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 5.7074e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0022 - val_mean_absolute_error: 0.0394\n",
      "Epoch 454/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 5.6858e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0022 - val_mean_absolute_error: 0.0393\n",
      "Epoch 455/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 5.6643e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0022 - val_mean_absolute_error: 0.0393\n",
      "Epoch 456/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 5.6430e-04 - mean_absolute_error: 0.0168 - val_loss: 0.0022 - val_mean_absolute_error: 0.0393\n",
      "Epoch 457/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 5.6220e-04 - mean_absolute_error: 0.0168 - val_loss: 0.0022 - val_mean_absolute_error: 0.0392\n",
      "Epoch 458/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 5.6010e-04 - mean_absolute_error: 0.0168 - val_loss: 0.0022 - val_mean_absolute_error: 0.0392\n",
      "Epoch 459/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 5.5804e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0022 - val_mean_absolute_error: 0.0391\n",
      "Epoch 460/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 5.5598e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0022 - val_mean_absolute_error: 0.0391\n",
      "Epoch 461/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 5.5394e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0022 - val_mean_absolute_error: 0.0391\n",
      "Epoch 462/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 5.5193e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0022 - val_mean_absolute_error: 0.0390\n",
      "Epoch 463/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 5.4991e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0022 - val_mean_absolute_error: 0.0390\n",
      "Epoch 464/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 5.4793e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0022 - val_mean_absolute_error: 0.0389\n",
      "Epoch 465/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 5.4595e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0022 - val_mean_absolute_error: 0.0389\n",
      "Epoch 466/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 5.4400e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0022 - val_mean_absolute_error: 0.0389\n",
      "Epoch 467/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 5.4206e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0022 - val_mean_absolute_error: 0.0388\n",
      "Epoch 468/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 5.4013e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0022 - val_mean_absolute_error: 0.0388\n",
      "Epoch 469/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 5.3823e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0022 - val_mean_absolute_error: 0.0387\n",
      "Epoch 470/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 5.3633e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0022 - val_mean_absolute_error: 0.0387\n",
      "Epoch 471/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 5.3445e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0022 - val_mean_absolute_error: 0.0387\n",
      "Epoch 472/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 5.3259e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0022 - val_mean_absolute_error: 0.0386\n",
      "Epoch 473/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 5.3074e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0021 - val_mean_absolute_error: 0.0386\n",
      "Epoch 474/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 5.2890e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0021 - val_mean_absolute_error: 0.0386\n",
      "Epoch 475/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 5.2708e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0021 - val_mean_absolute_error: 0.0385\n",
      "Epoch 476/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 5.2527e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0021 - val_mean_absolute_error: 0.0385\n",
      "Epoch 477/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 5.2348e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0021 - val_mean_absolute_error: 0.0384\n",
      "Epoch 478/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 5.2170e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0021 - val_mean_absolute_error: 0.0384\n",
      "Epoch 479/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 5.1994e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0021 - val_mean_absolute_error: 0.0384\n",
      "Epoch 480/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 5.1818e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0021 - val_mean_absolute_error: 0.0383\n",
      "Epoch 481/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 5.1645e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0021 - val_mean_absolute_error: 0.0383\n",
      "Epoch 482/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 5.1472e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0021 - val_mean_absolute_error: 0.0383\n",
      "Epoch 483/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 5.1301e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0021 - val_mean_absolute_error: 0.0382\n",
      "Epoch 484/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 5.1131e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0021 - val_mean_absolute_error: 0.0382\n",
      "Epoch 485/1000\n",
      "52/52 [==============================] - 0s 350us/step - loss: 5.0962e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0021 - val_mean_absolute_error: 0.0382\n",
      "Epoch 486/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 5.0795e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0021 - val_mean_absolute_error: 0.0381\n",
      "Epoch 487/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 5.0629e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0021 - val_mean_absolute_error: 0.0381\n",
      "Epoch 488/1000\n",
      "52/52 [==============================] - 0s 347us/step - loss: 5.0464e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0021 - val_mean_absolute_error: 0.0380\n",
      "Epoch 489/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 5.0300e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0380\n",
      "Epoch 490/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 5.0137e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0380\n",
      "Epoch 491/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 4.9976e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0379\n",
      "Epoch 492/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 4.9816e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0379\n",
      "Epoch 493/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.9657e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0379\n",
      "Epoch 494/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 4.9499e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0378\n",
      "Epoch 495/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 4.9343e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0378\n",
      "Epoch 496/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 4.9187e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0378\n",
      "Epoch 497/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.9033e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0021 - val_mean_absolute_error: 0.0377\n",
      "Epoch 498/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 4.8879e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0020 - val_mean_absolute_error: 0.0377\n",
      "Epoch 499/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 4.8727e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0020 - val_mean_absolute_error: 0.0377\n",
      "Epoch 500/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 4.8576e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0020 - val_mean_absolute_error: 0.0376\n",
      "Epoch 501/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 4.8426e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0020 - val_mean_absolute_error: 0.0376\n",
      "Epoch 502/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 4.8277e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0020 - val_mean_absolute_error: 0.0376\n",
      "Epoch 503/1000\n",
      "52/52 [==============================] - 0s 307us/step - loss: 4.8129e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0020 - val_mean_absolute_error: 0.0375\n",
      "Epoch 504/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 4.7982e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0020 - val_mean_absolute_error: 0.0375\n",
      "Epoch 505/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 4.7837e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0020 - val_mean_absolute_error: 0.0375\n",
      "Epoch 506/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 4.7692e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0020 - val_mean_absolute_error: 0.0374\n",
      "Epoch 507/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 4.7548e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0020 - val_mean_absolute_error: 0.0374\n",
      "Epoch 508/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 4.7405e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0020 - val_mean_absolute_error: 0.0373\n",
      "Epoch 509/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 4.7264e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0020 - val_mean_absolute_error: 0.0373\n",
      "Epoch 510/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 4.7123e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0020 - val_mean_absolute_error: 0.0373\n",
      "Epoch 511/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 4.6983e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0020 - val_mean_absolute_error: 0.0372\n",
      "Epoch 512/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.6844e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0020 - val_mean_absolute_error: 0.0372\n",
      "Epoch 513/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.6706e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0020 - val_mean_absolute_error: 0.0372\n",
      "Epoch 514/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 4.6569e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0020 - val_mean_absolute_error: 0.0372\n",
      "Epoch 515/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 4.6433e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0020 - val_mean_absolute_error: 0.0371\n",
      "Epoch 516/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 4.6298e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0020 - val_mean_absolute_error: 0.0371\n",
      "Epoch 517/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.6164e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0020 - val_mean_absolute_error: 0.0371\n",
      "Epoch 518/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 4.6031e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0020 - val_mean_absolute_error: 0.0370\n",
      "Epoch 519/1000\n",
      "52/52 [==============================] - 0s 351us/step - loss: 4.5898e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0020 - val_mean_absolute_error: 0.0370\n",
      "Epoch 520/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 4.5767e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0020 - val_mean_absolute_error: 0.0370\n",
      "Epoch 521/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 4.5636e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0020 - val_mean_absolute_error: 0.0369\n",
      "Epoch 522/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.5507e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0020 - val_mean_absolute_error: 0.0369\n",
      "Epoch 523/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 4.5378e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0020 - val_mean_absolute_error: 0.0369\n",
      "Epoch 524/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 4.5250e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0020 - val_mean_absolute_error: 0.0368\n",
      "Epoch 525/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 4.5122e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0020 - val_mean_absolute_error: 0.0368\n",
      "Epoch 526/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 4.4996e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0019 - val_mean_absolute_error: 0.0368\n",
      "Epoch 527/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 4.4871e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0019 - val_mean_absolute_error: 0.0367\n",
      "Epoch 528/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.4746e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0019 - val_mean_absolute_error: 0.0367\n",
      "Epoch 529/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 4.4622e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0019 - val_mean_absolute_error: 0.0367\n",
      "Epoch 530/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 4.4499e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0019 - val_mean_absolute_error: 0.0367\n",
      "Epoch 531/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 4.4377e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0019 - val_mean_absolute_error: 0.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 4.4256e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0019 - val_mean_absolute_error: 0.0366\n",
      "Epoch 533/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 4.4135e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0019 - val_mean_absolute_error: 0.0366\n",
      "Epoch 534/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 4.4015e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0019 - val_mean_absolute_error: 0.0365\n",
      "Epoch 535/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 4.3896e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0019 - val_mean_absolute_error: 0.0365\n",
      "Epoch 536/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 4.3778e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0019 - val_mean_absolute_error: 0.0365\n",
      "Epoch 537/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 4.3660e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0019 - val_mean_absolute_error: 0.0365\n",
      "Epoch 538/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 4.3543e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0019 - val_mean_absolute_error: 0.0364\n",
      "Epoch 539/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 4.3427e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0019 - val_mean_absolute_error: 0.0364\n",
      "Epoch 540/1000\n",
      "52/52 [==============================] - 0s 353us/step - loss: 4.3312e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0019 - val_mean_absolute_error: 0.0364\n",
      "Epoch 541/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 4.3822e-04 - mean_absolute_error: 0.015 - 0s 311us/step - loss: 4.3197e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0019 - val_mean_absolute_error: 0.0364\n",
      "Epoch 542/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 4.3084e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0019 - val_mean_absolute_error: 0.0363\n",
      "Epoch 543/1000\n",
      "52/52 [==============================] - 0s 364us/step - loss: 4.2971e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0019 - val_mean_absolute_error: 0.0363\n",
      "Epoch 544/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 4.2858e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0019 - val_mean_absolute_error: 0.0363\n",
      "Epoch 545/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 4.2746e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0019 - val_mean_absolute_error: 0.0362\n",
      "Epoch 546/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 4.2635e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0019 - val_mean_absolute_error: 0.0362\n",
      "Epoch 547/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.2525e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0019 - val_mean_absolute_error: 0.0362\n",
      "Epoch 548/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 4.2415e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0019 - val_mean_absolute_error: 0.0362\n",
      "Epoch 549/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 4.2306e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0019 - val_mean_absolute_error: 0.0361\n",
      "Epoch 550/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 4.2198e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0019 - val_mean_absolute_error: 0.0361\n",
      "Epoch 551/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 4.2090e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0019 - val_mean_absolute_error: 0.0361\n",
      "Epoch 552/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 4.1983e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0019 - val_mean_absolute_error: 0.0361\n",
      "Epoch 553/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 4.1877e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0019 - val_mean_absolute_error: 0.0360\n",
      "Epoch 554/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 4.1771e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0019 - val_mean_absolute_error: 0.0360\n",
      "Epoch 555/1000\n",
      "52/52 [==============================] - 0s 358us/step - loss: 4.1666e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0019 - val_mean_absolute_error: 0.0360\n",
      "Epoch 556/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 4.1562e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0019 - val_mean_absolute_error: 0.0360\n",
      "Epoch 557/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 4.1458e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0019 - val_mean_absolute_error: 0.0359\n",
      "Epoch 558/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.1355e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0019 - val_mean_absolute_error: 0.0359\n",
      "Epoch 559/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 4.1253e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0019 - val_mean_absolute_error: 0.0359\n",
      "Epoch 560/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 4.1151e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0018 - val_mean_absolute_error: 0.0359\n",
      "Epoch 561/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 4.1049e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0018 - val_mean_absolute_error: 0.0358\n",
      "Epoch 562/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 4.0949e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0018 - val_mean_absolute_error: 0.0358\n",
      "Epoch 563/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 4.0849e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0018 - val_mean_absolute_error: 0.0358\n",
      "Epoch 564/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 4.1405e-04 - mean_absolute_error: 0.014 - 0s 371us/step - loss: 4.0749e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0018 - val_mean_absolute_error: 0.0358\n",
      "Epoch 565/1000\n",
      "52/52 [==============================] - 0s 385us/step - loss: 4.0650e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0018 - val_mean_absolute_error: 0.0357\n",
      "Epoch 566/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 4.0552e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0018 - val_mean_absolute_error: 0.0357\n",
      "Epoch 567/1000\n",
      "52/52 [==============================] - 0s 356us/step - loss: 4.0454e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0357\n",
      "Epoch 568/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 4.0357e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0357\n",
      "Epoch 569/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 4.0260e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0356\n",
      "Epoch 570/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 4.0164e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0356\n",
      "Epoch 571/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 4.0069e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0356\n",
      "Epoch 572/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.9974e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0018 - val_mean_absolute_error: 0.0356\n",
      "Epoch 573/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.9880e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0355\n",
      "Epoch 574/1000\n",
      "52/52 [==============================] - 0s 358us/step - loss: 3.9786e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0355\n",
      "Epoch 575/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 3.9692e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0355\n",
      "Epoch 576/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.9600e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0355\n",
      "Epoch 577/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.9507e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0354\n",
      "Epoch 578/1000\n",
      "52/52 [==============================] - 0s 307us/step - loss: 3.9416e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0018 - val_mean_absolute_error: 0.0354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 579/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.9325e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0354\n",
      "Epoch 580/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 3.9234e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0354\n",
      "Epoch 581/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.9144e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0353\n",
      "Epoch 582/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.9054e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0353\n",
      "Epoch 583/1000\n",
      "52/52 [==============================] - 0s 340us/step - loss: 3.8965e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0353\n",
      "Epoch 584/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 3.8876e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0018 - val_mean_absolute_error: 0.0353\n",
      "Epoch 585/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.8788e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0352\n",
      "Epoch 586/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.8701e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0352\n",
      "Epoch 587/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.8613e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0352\n",
      "Epoch 588/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 3.8527e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0352\n",
      "Epoch 589/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 3.8441e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0351\n",
      "Epoch 590/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 3.8355e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0351\n",
      "Epoch 591/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 3.8270e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0351\n",
      "Epoch 592/1000\n",
      "52/52 [==============================] - 0s 344us/step - loss: 3.8185e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0351\n",
      "Epoch 593/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 3.8100e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0350\n",
      "Epoch 594/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 3.8017e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0350\n",
      "Epoch 595/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 3.7933e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0350\n",
      "Epoch 596/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 3.7850e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0018 - val_mean_absolute_error: 0.0350\n",
      "Epoch 597/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 3.7768e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0018 - val_mean_absolute_error: 0.0349\n",
      "Epoch 598/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.7686e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0018 - val_mean_absolute_error: 0.0349\n",
      "Epoch 599/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 3.7604e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0018 - val_mean_absolute_error: 0.0349\n",
      "Epoch 600/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 3.7523e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0018 - val_mean_absolute_error: 0.0349\n",
      "Epoch 601/1000\n",
      "52/52 [==============================] - 0s 380us/step - loss: 3.7442e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0017 - val_mean_absolute_error: 0.0348\n",
      "Epoch 602/1000\n",
      "52/52 [==============================] - 0s 350us/step - loss: 3.7362e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0017 - val_mean_absolute_error: 0.0348\n",
      "Epoch 603/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 3.7282e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0348\n",
      "Epoch 604/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.7202e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0348\n",
      "Epoch 605/1000\n",
      "52/52 [==============================] - 0s 354us/step - loss: 3.7123e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0348\n",
      "Epoch 606/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 3.7045e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0347\n",
      "Epoch 607/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 3.6966e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0347\n",
      "Epoch 608/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.6889e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0347\n",
      "Epoch 609/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.6811e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0017 - val_mean_absolute_error: 0.0347\n",
      "Epoch 610/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.6734e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0346\n",
      "Epoch 611/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.6658e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0346\n",
      "Epoch 612/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 3.6581e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0346\n",
      "Epoch 613/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.6506e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0346\n",
      "Epoch 614/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 3.6430e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0345\n",
      "Epoch 615/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.6355e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0017 - val_mean_absolute_error: 0.0345\n",
      "Epoch 616/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.6281e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0345\n",
      "Epoch 617/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 3.6206e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0345\n",
      "Epoch 618/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 3.6132e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0345\n",
      "Epoch 619/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 3.6059e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0344\n",
      "Epoch 620/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.5986e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0344\n",
      "Epoch 621/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.5913e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0344\n",
      "Epoch 622/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 3.5840e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0017 - val_mean_absolute_error: 0.0344\n",
      "Epoch 623/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.5768e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0343\n",
      "Epoch 624/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 3.5697e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0343\n",
      "Epoch 625/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 3.5625e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0343\n",
      "Epoch 626/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 3.5554e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0343\n",
      "Epoch 627/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 3.5484e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0342\n",
      "Epoch 628/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.5413e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0342\n",
      "Epoch 629/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 3.5344e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0342\n",
      "Epoch 630/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 3.5274e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0017 - val_mean_absolute_error: 0.0342\n",
      "Epoch 631/1000\n",
      "52/52 [==============================] - 0s 349us/step - loss: 3.5205e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0342\n",
      "Epoch 632/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 3.5136e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0341\n",
      "Epoch 633/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 3.5067e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0341\n",
      "Epoch 634/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.4999e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0341\n",
      "Epoch 635/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.4931e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0341\n",
      "Epoch 636/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.4864e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0340\n",
      "Epoch 637/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 3.4796e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0017 - val_mean_absolute_error: 0.0340\n",
      "Epoch 638/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 3.4729e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0340\n",
      "Epoch 639/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 3.4663e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0340\n",
      "Epoch 640/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 3.4597e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0340\n",
      "Epoch 641/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 3.4531e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0339\n",
      "Epoch 642/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.4465e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0339\n",
      "Epoch 643/1000\n",
      "52/52 [==============================] - 0s 347us/step - loss: 3.4400e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0339\n",
      "Epoch 644/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 3.4335e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0339\n",
      "Epoch 645/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.4270e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0339\n",
      "Epoch 646/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 3.4206e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0017 - val_mean_absolute_error: 0.0338\n",
      "Epoch 647/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.4141e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0017 - val_mean_absolute_error: 0.0338\n",
      "Epoch 648/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.4078e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0017 - val_mean_absolute_error: 0.0338\n",
      "Epoch 649/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 3.4014e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0017 - val_mean_absolute_error: 0.0338\n",
      "Epoch 650/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 3.3951e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0017 - val_mean_absolute_error: 0.0337\n",
      "Epoch 651/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 3.3888e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0016 - val_mean_absolute_error: 0.0337\n",
      "Epoch 652/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.3825e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0016 - val_mean_absolute_error: 0.0337\n",
      "Epoch 653/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.3763e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0016 - val_mean_absolute_error: 0.0337\n",
      "Epoch 654/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 3.3701e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0016 - val_mean_absolute_error: 0.0337\n",
      "Epoch 655/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.3639e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0016 - val_mean_absolute_error: 0.0336\n",
      "Epoch 656/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 3.3578e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0336\n",
      "Epoch 657/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 3.3517e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0336\n",
      "Epoch 658/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.3456e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0336\n",
      "Epoch 659/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 3.3395e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0336\n",
      "Epoch 660/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 3.4123e-04 - mean_absolute_error: 0.013 - 0s 315us/step - loss: 3.3335e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0335\n",
      "Epoch 661/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 3.3275e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0335\n",
      "Epoch 662/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 3.3215e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0335\n",
      "Epoch 663/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.3155e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0335\n",
      "Epoch 664/1000\n",
      "52/52 [==============================] - 0s 351us/step - loss: 3.3096e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0016 - val_mean_absolute_error: 0.0334\n",
      "Epoch 665/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 3.3037e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0334\n",
      "Epoch 666/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.2978e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0334\n",
      "Epoch 667/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.2920e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0334\n",
      "Epoch 668/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.2862e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0334\n",
      "Epoch 669/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.2803e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0333\n",
      "Epoch 670/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 3.2746e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0333\n",
      "Epoch 671/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 3.2688e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0333\n",
      "Epoch 672/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 3.2631e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0333\n",
      "Epoch 673/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 321us/step - loss: 3.2574e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0016 - val_mean_absolute_error: 0.0333\n",
      "Epoch 674/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.2518e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0332\n",
      "Epoch 675/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.2461e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0332\n",
      "Epoch 676/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.2405e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0332\n",
      "Epoch 677/1000\n",
      "52/52 [==============================] - 0s 368us/step - loss: 3.2349e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0332\n",
      "Epoch 678/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 3.2293e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0332\n",
      "Epoch 679/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 3.2238e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331\n",
      "Epoch 680/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.2183e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331\n",
      "Epoch 681/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.2127e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331\n",
      "Epoch 682/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 3.2073e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331\n",
      "Epoch 683/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 3.2018e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0016 - val_mean_absolute_error: 0.0331\n",
      "Epoch 684/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 3.1964e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0330\n",
      "Epoch 685/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 3.1910e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0330\n",
      "Epoch 686/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 3.1856e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0330\n",
      "Epoch 687/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 3.1802e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0330\n",
      "Epoch 688/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 3.1749e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0330\n",
      "Epoch 689/1000\n",
      "52/52 [==============================] - 0s 401us/step - loss: 3.1696e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0329\n",
      "Epoch 690/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 3.1643e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0329\n",
      "Epoch 691/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 3.1590e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0329\n",
      "Epoch 692/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 3.1538e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0016 - val_mean_absolute_error: 0.0329\n",
      "Epoch 693/1000\n",
      "52/52 [==============================] - 0s 312us/step - loss: 3.1485e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0329\n",
      "Epoch 694/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 3.1434e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0328\n",
      "Epoch 695/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 3.1381e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0328\n",
      "Epoch 696/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.1331e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0328\n",
      "Epoch 697/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 3.1278e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0328\n",
      "Epoch 698/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 3.1228e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0328\n",
      "Epoch 699/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.1176e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0327\n",
      "Epoch 700/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.1127e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0327\n",
      "Epoch 701/1000\n",
      "52/52 [==============================] - 0s 337us/step - loss: 3.1075e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0327\n",
      "Epoch 702/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 3.1026e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0016 - val_mean_absolute_error: 0.0327\n",
      "Epoch 703/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 3.0974e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0327\n",
      "Epoch 704/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 3.0926e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 705/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 3.0874e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 706/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 3.0827e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 707/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 3.0775e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 708/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.0729e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 709/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 3.0677e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0326\n",
      "Epoch 710/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.0632e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0325\n",
      "Epoch 711/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.0579e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0325\n",
      "Epoch 712/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 3.0535e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0016 - val_mean_absolute_error: 0.0325\n",
      "Epoch 713/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.0482e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0325\n",
      "Epoch 714/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.0439e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0325\n",
      "Epoch 715/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 3.0386e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0325\n",
      "Epoch 716/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 3.0345e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0324\n",
      "Epoch 717/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 3.0291e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0324\n",
      "Epoch 718/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 3.0251e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0324\n",
      "Epoch 719/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 3.0196e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0324\n",
      "Epoch 720/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 3.0157e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 3.0102e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n",
      "Epoch 722/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 3.0065e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n",
      "Epoch 723/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 3.0009e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n",
      "Epoch 724/1000\n",
      "52/52 [==============================] - 0s 346us/step - loss: 2.9974e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n",
      "Epoch 725/1000\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 2.9916e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0323\n",
      "Epoch 726/1000\n",
      "52/52 [==============================] - 0s 791us/step - loss: 2.9883e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 727/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.9824e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 728/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.9793e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 729/1000\n",
      "52/52 [==============================] - 0s 376us/step - loss: 2.9732e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 730/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.9704e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 731/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.9641e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0322\n",
      "Epoch 732/1000\n",
      "52/52 [==============================] - 0s 386us/step - loss: 2.9616e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0321\n",
      "Epoch 733/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.9551e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0321\n",
      "Epoch 734/1000\n",
      "52/52 [==============================] - 0s 405us/step - loss: 2.9529e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0015 - val_mean_absolute_error: 0.0321\n",
      "Epoch 735/1000\n",
      "52/52 [==============================] - 0s 354us/step - loss: 2.9462e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0321\n",
      "Epoch 736/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 2.9442e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 737/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 2.9373e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0321\n",
      "Epoch 738/1000\n",
      "52/52 [==============================] - 0s 370us/step - loss: 2.9356e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 739/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.9285e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 740/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.9271e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 741/1000\n",
      "52/52 [==============================] - 0s 386us/step - loss: 2.9197e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 742/1000\n",
      "52/52 [==============================] - 0s 364us/step - loss: 2.9186e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 743/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.9111e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0320\n",
      "Epoch 744/1000\n",
      "52/52 [==============================] - 0s 412us/step - loss: 2.9102e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 745/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.9025e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 746/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 2.9018e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 747/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 2.8940e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 748/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 2.8935e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 749/1000\n",
      "52/52 [==============================] - 0s 406us/step - loss: 2.8856e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0319\n",
      "Epoch 750/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 2.8852e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 751/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.8773e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 752/1000\n",
      "52/52 [==============================] - 0s 378us/step - loss: 2.8768e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 753/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.8690e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 754/1000\n",
      "52/52 [==============================] - 0s 405us/step - loss: 2.8685e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 755/1000\n",
      "52/52 [==============================] - 0s 359us/step - loss: 2.8609e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0318\n",
      "Epoch 756/1000\n",
      "52/52 [==============================] - 0s 380us/step - loss: 2.8603e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 757/1000\n",
      "52/52 [==============================] - 0s 396us/step - loss: 2.8528e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 758/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.8520e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 759/1000\n",
      "52/52 [==============================] - 0s 385us/step - loss: 2.8448e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 760/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.8437e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 761/1000\n",
      "52/52 [==============================] - 0s 410us/step - loss: 2.8369e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0317\n",
      "Epoch 762/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.8354e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 763/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.8291e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 764/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.8272e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 765/1000\n",
      "52/52 [==============================] - 0s 386us/step - loss: 2.8213e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 766/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 2.8190e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 767/1000\n",
      "52/52 [==============================] - 0s 384us/step - loss: 2.8136e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 768/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.8109e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 769/1000\n",
      "52/52 [==============================] - 0s 388us/step - loss: 2.8060e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 770/1000\n",
      "52/52 [==============================] - 0s 378us/step - loss: 2.8029e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 771/1000\n",
      "52/52 [==============================] - 0s 389us/step - loss: 2.7984e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 772/1000\n",
      "52/52 [==============================] - 0s 383us/step - loss: 2.7950e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 773/1000\n",
      "52/52 [==============================] - 0s 368us/step - loss: 2.7908e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0315\n",
      "Epoch 774/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.7871e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 775/1000\n",
      "52/52 [==============================] - 0s 384us/step - loss: 2.7833e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 776/1000\n",
      "52/52 [==============================] - 0s 409us/step - loss: 2.7794e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 777/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.7758e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 778/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.7718e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 779/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.7683e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 780/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.7642e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0314\n",
      "Epoch 781/1000\n",
      "52/52 [==============================] - 0s 390us/step - loss: 2.7608e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 782/1000\n",
      "52/52 [==============================] - 0s 383us/step - loss: 2.7568e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 783/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.7533e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 784/1000\n",
      "52/52 [==============================] - 0s 399us/step - loss: 2.7494e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 785/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.7459e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 786/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.7421e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 787/1000\n",
      "52/52 [==============================] - 0s 364us/step - loss: 2.7385e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0015 - val_mean_absolute_error: 0.0313\n",
      "Epoch 788/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 2.7348e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 789/1000\n",
      "52/52 [==============================] - 0s 394us/step - loss: 2.7312e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 790/1000\n",
      "52/52 [==============================] - 0s 376us/step - loss: 2.7276e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 791/1000\n",
      "52/52 [==============================] - 0s 390us/step - loss: 2.7240e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 792/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.7204e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 793/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.7167e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 794/1000\n",
      "52/52 [==============================] - 0s 370us/step - loss: 2.7132e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 795/1000\n",
      "52/52 [==============================] - 0s 358us/step - loss: 2.7096e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0312\n",
      "Epoch 796/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.7061e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 797/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.7025e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 798/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.6990e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 799/1000\n",
      "52/52 [==============================] - 0s 370us/step - loss: 2.6955e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 800/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.6920e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 801/1000\n",
      "52/52 [==============================] - 0s 364us/step - loss: 2.6885e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 802/1000\n",
      "52/52 [==============================] - 0s 445us/step - loss: 2.6850e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0311\n",
      "Epoch 803/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.6816e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 804/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.6781e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 805/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.6747e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 806/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.6712e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 807/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.6678e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 808/1000\n",
      "52/52 [==============================] - 0s 384us/step - loss: 2.6644e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 809/1000\n",
      "52/52 [==============================] - 0s 361us/step - loss: 2.6610e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 810/1000\n",
      "52/52 [==============================] - 0s 373us/step - loss: 2.6576e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 811/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.6542e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 812/1000\n",
      "52/52 [==============================] - 0s 394us/step - loss: 2.6508e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 813/1000\n",
      "52/52 [==============================] - 0s 376us/step - loss: 2.6475e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 814/1000\n",
      "52/52 [==============================] - 0s 368us/step - loss: 2.6441e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 815/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 2.6408e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 816/1000\n",
      "52/52 [==============================] - 0s 427us/step - loss: 2.6375e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 817/1000\n",
      "52/52 [==============================] - 0s 373us/step - loss: 2.6341e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 818/1000\n",
      "52/52 [==============================] - 0s 387us/step - loss: 2.6308e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0309\n",
      "Epoch 819/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 2.6276e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 820/1000\n",
      "52/52 [==============================] - 0s 378us/step - loss: 2.6243e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 821/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.6210e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 822/1000\n",
      "52/52 [==============================] - 0s 376us/step - loss: 2.6177e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 823/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 2.6145e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 824/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 2.6112e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 825/1000\n",
      "52/52 [==============================] - 0s 373us/step - loss: 2.6080e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 826/1000\n",
      "52/52 [==============================] - 0s 389us/step - loss: 2.6048e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 827/1000\n",
      "52/52 [==============================] - 0s 368us/step - loss: 2.6016e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 828/1000\n",
      "52/52 [==============================] - 0s 402us/step - loss: 2.5984e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 829/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.5952e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 830/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.5920e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 831/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.5888e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 832/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 2.5857e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 833/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 2.5825e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 834/1000\n",
      "52/52 [==============================] - 0s 379us/step - loss: 2.5794e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0307\n",
      "Epoch 835/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.5762e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 836/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.5731e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 837/1000\n",
      "52/52 [==============================] - 0s 387us/step - loss: 2.5700e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 838/1000\n",
      "52/52 [==============================] - 0s 389us/step - loss: 2.5669e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 839/1000\n",
      "52/52 [==============================] - 0s 397us/step - loss: 2.5638e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 840/1000\n",
      "52/52 [==============================] - 0s 384us/step - loss: 2.5607e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 841/1000\n",
      "52/52 [==============================] - 0s 370us/step - loss: 2.5576e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 842/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.5546e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 843/1000\n",
      "52/52 [==============================] - 0s 392us/step - loss: 2.5515e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 844/1000\n",
      "52/52 [==============================] - 0s 380us/step - loss: 2.5485e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 845/1000\n",
      "52/52 [==============================] - 0s 362us/step - loss: 2.5454e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 846/1000\n",
      "52/52 [==============================] - 0s 378us/step - loss: 2.5424e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 847/1000\n",
      "52/52 [==============================] - 0s 382us/step - loss: 2.5394e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 848/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.5364e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 849/1000\n",
      "52/52 [==============================] - 0s 370us/step - loss: 2.5334e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 850/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 2.5304e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0014 - val_mean_absolute_error: 0.0305\n",
      "Epoch 851/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.5274e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 852/1000\n",
      "52/52 [==============================] - 0s 383us/step - loss: 2.5244e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 853/1000\n",
      "52/52 [==============================] - 0s 380us/step - loss: 2.5215e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 854/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.5185e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 855/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.5156e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 856/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.5126e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 857/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.5097e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 858/1000\n",
      "52/52 [==============================] - 0s 390us/step - loss: 2.5068e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 859/1000\n",
      "52/52 [==============================] - 0s 390us/step - loss: 2.5039e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0304\n",
      "Epoch 860/1000\n",
      "52/52 [==============================] - 0s 385us/step - loss: 2.5009e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 861/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.4980e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 862/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.4952e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 863/1000\n",
      "52/52 [==============================] - 0s 380us/step - loss: 2.4923e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 864/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.4894e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 865/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.4865e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 866/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.4837e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 867/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.4808e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 868/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.4780e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 869/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.4752e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 870/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.4724e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 871/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.4695e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 872/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 2.4667e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 873/1000\n",
      "52/52 [==============================] - 0s 402us/step - loss: 2.4639e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 874/1000\n",
      "52/52 [==============================] - 0s 414us/step - loss: 2.4611e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 875/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.4584e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 876/1000\n",
      "52/52 [==============================] - 0s 424us/step - loss: 2.4556e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0014 - val_mean_absolute_error: 0.0301\n",
      "Epoch 877/1000\n",
      "52/52 [==============================] - 0s 395us/step - loss: 2.4528e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0014 - val_mean_absolute_error: 0.0301\n",
      "Epoch 878/1000\n",
      "52/52 [==============================] - 0s 359us/step - loss: 2.4501e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 879/1000\n",
      "52/52 [==============================] - 0s 374us/step - loss: 2.4473e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 880/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.4446e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 881/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.4418e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 882/1000\n",
      "52/52 [==============================] - 0s 357us/step - loss: 2.4391e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 883/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.4364e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 884/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.4337e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0301\n",
      "Epoch 885/1000\n",
      "52/52 [==============================] - 0s 356us/step - loss: 2.4310e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 886/1000\n",
      "52/52 [==============================] - 0s 369us/step - loss: 2.4283e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 887/1000\n",
      "52/52 [==============================] - 0s 399us/step - loss: 2.4256e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 888/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.4229e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 889/1000\n",
      "52/52 [==============================] - 0s 358us/step - loss: 2.4202e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 890/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.4175e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 891/1000\n",
      "52/52 [==============================] - 0s 396us/step - loss: 2.4149e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 892/1000\n",
      "52/52 [==============================] - 0s 433us/step - loss: 2.4122e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 893/1000\n",
      "52/52 [==============================] - 0s 372us/step - loss: 2.4096e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0300\n",
      "Epoch 894/1000\n",
      "52/52 [==============================] - 0s 373us/step - loss: 2.4070e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 895/1000\n",
      "52/52 [==============================] - 0s 363us/step - loss: 2.4043e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 896/1000\n",
      "52/52 [==============================] - 0s 381us/step - loss: 2.4017e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 897/1000\n",
      "52/52 [==============================] - 0s 368us/step - loss: 2.3991e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 898/1000\n",
      "52/52 [==============================] - 0s 360us/step - loss: 2.3965e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 899/1000\n",
      "52/52 [==============================] - 0s 396us/step - loss: 2.3938e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 900/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.3913e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 901/1000\n",
      "52/52 [==============================] - 0s 386us/step - loss: 2.3887e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 902/1000\n",
      "52/52 [==============================] - 0s 377us/step - loss: 2.3861e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 903/1000\n",
      "52/52 [==============================] - 0s 366us/step - loss: 2.3835e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 904/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.3809e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 905/1000\n",
      "52/52 [==============================] - 0s 405us/step - loss: 2.3784e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 906/1000\n",
      "52/52 [==============================] - 0s 364us/step - loss: 2.3758e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 907/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 2.3733e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 908/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 2.3707e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 909/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 2.3682e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 910/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 2.3656e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 911/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 2.3631e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 912/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 2.3607e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 913/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 2.3581e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 914/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 2.3556e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 915/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 2.3532e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 916/1000\n",
      "52/52 [==============================] - 0s 310us/step - loss: 2.3506e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 917/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 2.3481e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 918/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 2.3458e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 919/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 2.3431e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 920/1000\n",
      "52/52 [==============================] - 0s 339us/step - loss: 2.3407e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 921/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 2.3385e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 922/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 2.3356e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 923/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 2.3334e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 924/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 2.3313e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 925/1000\n",
      "52/52 [==============================] - 0s 313us/step - loss: 2.3281e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 926/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 2.3261e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 927/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 2.3244e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 928/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 2.3205e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 929/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 2.3190e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 930/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 2.3182e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 931/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 2.3128e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 932/1000\n",
      "52/52 [==============================] - 0s 341us/step - loss: 2.3123e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 933/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 2.3141e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 934/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 2.3064e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0294\n",
      "Epoch 935/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 2.3076e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 936/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 2.3180e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 937/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 2.3118e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0292\n",
      "Epoch 938/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 2.3133e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 939/1000\n",
      "52/52 [==============================] - 0s 342us/step - loss: 2.3645e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0294\n",
      "Epoch 940/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 2.4062e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0012 - val_mean_absolute_error: 0.0288\n",
      "Epoch 941/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 2.3853e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0014 - val_mean_absolute_error: 0.0302\n",
      "Epoch 942/1000\n",
      "52/52 [==============================] - 0s 327us/step - loss: 2.7058e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0013 - val_mean_absolute_error: 0.0294\n",
      "Epoch 943/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 3.2121e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0011 - val_mean_absolute_error: 0.0273\n",
      "Epoch 944/1000\n",
      "52/52 [==============================] - 0s 338us/step - loss: 2.9917e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 945/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 6.1131e-04 - mean_absolute_error: 0.0197 - val_loss: 0.0014 - val_mean_absolute_error: 0.0295\n",
      "Epoch 946/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0012 - mean_absolute_error: 0.0259 - val_loss: 0.0011 - val_mean_absolute_error: 0.0268\n",
      "Epoch 947/1000\n",
      "52/52 [==============================] - 0s 320us/step - loss: 0.0012 - mean_absolute_error: 0.0284 - val_loss: 0.0022 - val_mean_absolute_error: 0.0376\n",
      "Epoch 948/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 0.0065 - mean_absolute_error: 0.0600 - val_loss: 0.0016 - val_mean_absolute_error: 0.0323\n",
      "Epoch 949/1000\n",
      "52/52 [==============================] - 0s 336us/step - loss: 0.0146 - mean_absolute_error: 0.0866 - val_loss: 0.0081 - val_mean_absolute_error: 0.0784\n",
      "Epoch 950/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 0.0170 - mean_absolute_error: 0.1004 - val_loss: 0.0074 - val_mean_absolute_error: 0.0679\n",
      "Epoch 951/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 0.0153 - mean_absolute_error: 0.0834 - val_loss: 0.0030 - val_mean_absolute_error: 0.0458\n",
      "Epoch 952/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 0.0072 - mean_absolute_error: 0.0641 - val_loss: 0.0026 - val_mean_absolute_error: 0.0422\n",
      "Epoch 953/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0108 - mean_absolute_error: 0.0727 - val_loss: 0.0033 - val_mean_absolute_error: 0.0444\n",
      "Epoch 954/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 0.0064 - mean_absolute_error: 0.0566 - val_loss: 0.0024 - val_mean_absolute_error: 0.0413\n",
      "Epoch 955/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 0.0024 - mean_absolute_error: 0.0396 - val_loss: 0.0024 - val_mean_absolute_error: 0.0401\n",
      "Epoch 956/1000\n",
      "52/52 [==============================] - 0s 309us/step - loss: 0.0052 - mean_absolute_error: 0.0572 - val_loss: 0.0026 - val_mean_absolute_error: 0.0425\n",
      "Epoch 957/1000\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0022 - mean_absolute_error: 0.0358 - val_loss: 0.0055 - val_mean_absolute_error: 0.0618\n",
      "Epoch 958/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0033 - mean_absolute_error: 0.0408 - val_loss: 0.0024 - val_mean_absolute_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 959/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 0.0053 - mean_absolute_error: 0.0535 - val_loss: 0.0052 - val_mean_absolute_error: 0.0590\n",
      "Epoch 960/1000\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0018 - mean_absolute_error: 0.0340 - val_loss: 0.0019 - val_mean_absolute_error: 0.0356\n",
      "Epoch 961/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 0.0021 - mean_absolute_error: 0.0379 - val_loss: 0.0017 - val_mean_absolute_error: 0.0333\n",
      "Epoch 962/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 9.7593e-04 - mean_absolute_error: 0.0248 - val_loss: 0.0015 - val_mean_absolute_error: 0.0309\n",
      "Epoch 963/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 0.0013 - mean_absolute_error: 0.0286 - val_loss: 0.0012 - val_mean_absolute_error: 0.0273\n",
      "Epoch 964/1000\n",
      "52/52 [==============================] - 0s 321us/step - loss: 0.0014 - mean_absolute_error: 0.0286 - val_loss: 0.0031 - val_mean_absolute_error: 0.0453\n",
      "Epoch 965/1000\n",
      "52/52 [==============================] - 0s 323us/step - loss: 8.7946e-04 - mean_absolute_error: 0.0238 - val_loss: 0.0014 - val_mean_absolute_error: 0.0306\n",
      "Epoch 966/1000\n",
      "52/52 [==============================] - 0s 335us/step - loss: 9.5614e-04 - mean_absolute_error: 0.0237 - val_loss: 0.0015 - val_mean_absolute_error: 0.0309\n",
      "Epoch 967/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 7.8020e-04 - mean_absolute_error: 0.0212 - val_loss: 0.0011 - val_mean_absolute_error: 0.0281\n",
      "Epoch 968/1000\n",
      "52/52 [==============================] - 0s 316us/step - loss: 4.7936e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0013 - val_mean_absolute_error: 0.0299\n",
      "Epoch 969/1000\n",
      "52/52 [==============================] - 0s 329us/step - loss: 5.0445e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0019 - val_mean_absolute_error: 0.0353\n",
      "Epoch 970/1000\n",
      "52/52 [==============================] - 0s 328us/step - loss: 4.1947e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0011 - val_mean_absolute_error: 0.0271\n",
      "Epoch 971/1000\n",
      "52/52 [==============================] - 0s 332us/step - loss: 4.4983e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0015 - val_mean_absolute_error: 0.0316\n",
      "Epoch 972/1000\n",
      "52/52 [==============================] - 0s 343us/step - loss: 4.4140e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0011 - val_mean_absolute_error: 0.0270\n",
      "Epoch 973/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 3.6478e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0014 - val_mean_absolute_error: 0.0310\n",
      "Epoch 974/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 3.0047e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 975/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 3.0151e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0284\n",
      "Epoch 976/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 3.0001e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0015 - val_mean_absolute_error: 0.0311\n",
      "Epoch 977/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 2.9324e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0282\n",
      "Epoch 978/1000\n",
      "52/52 [==============================] - 0s 367us/step - loss: 2.8504e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0014 - val_mean_absolute_error: 0.0308\n",
      "Epoch 979/1000\n",
      "52/52 [==============================] - 0s 324us/step - loss: 2.7018e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0012 - val_mean_absolute_error: 0.0290\n",
      "Epoch 980/1000\n",
      "52/52 [==============================] - 0s 347us/step - loss: 2.5265e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0013 - val_mean_absolute_error: 0.0297\n",
      "Epoch 981/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 2.4916e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0014 - val_mean_absolute_error: 0.0303\n",
      "Epoch 982/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 2.5084e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0012 - val_mean_absolute_error: 0.0283\n",
      "Epoch 983/1000\n",
      "52/52 [==============================] - 0s 307us/step - loss: 2.4233e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 984/1000\n",
      "52/52 [==============================] - 0s 315us/step - loss: 2.4714e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0012 - val_mean_absolute_error: 0.0285\n",
      "Epoch 985/1000\n",
      "52/52 [==============================] - 0s 331us/step - loss: 2.4142e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0013 - val_mean_absolute_error: 0.0298\n",
      "Epoch 986/1000\n",
      "52/52 [==============================] - 0s 317us/step - loss: 2.3473e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0013 - val_mean_absolute_error: 0.0293\n",
      "Epoch 987/1000\n",
      "52/52 [==============================] - 0s 308us/step - loss: 2.3236e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0012 - val_mean_absolute_error: 0.0288\n",
      "Epoch 988/1000\n",
      "52/52 [==============================] - 0s 371us/step - loss: 2.3033e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0295\n",
      "Epoch 989/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 2.3351e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0012 - val_mean_absolute_error: 0.0286\n",
      "Epoch 990/1000\n",
      "52/52 [==============================] - 0s 318us/step - loss: 2.2867e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0013 - val_mean_absolute_error: 0.0296\n",
      "Epoch 991/1000\n",
      "52/52 [==============================] - 0s 334us/step - loss: 2.2885e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0012 - val_mean_absolute_error: 0.0289\n",
      "Epoch 992/1000\n",
      "52/52 [==============================] - 0s 375us/step - loss: 2.2727e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0012 - val_mean_absolute_error: 0.0291\n",
      "Epoch 993/1000\n",
      "52/52 [==============================] - 0s 322us/step - loss: 2.2505e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0289\n",
      "Epoch 994/1000\n",
      "52/52 [==============================] - 0s 326us/step - loss: 2.2646e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0012 - val_mean_absolute_error: 0.0285\n",
      "Epoch 995/1000\n",
      "52/52 [==============================] - 0s 325us/step - loss: 2.2385e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0292\n",
      "Epoch 996/1000\n",
      "52/52 [==============================] - 0s 330us/step - loss: 2.2441e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0288\n",
      "Epoch 997/1000\n",
      "52/52 [==============================] - 0s 333us/step - loss: 2.2348e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0291\n",
      "Epoch 998/1000\n",
      "52/52 [==============================] - 0s 314us/step - loss: 2.2284e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0288\n",
      "Epoch 999/1000\n",
      "52/52 [==============================] - 0s 311us/step - loss: 2.2242e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0012 - val_mean_absolute_error: 0.0289\n",
      "Epoch 1000/1000\n",
      "52/52 [==============================] - 0s 319us/step - loss: 2.2155e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0012 - val_mean_absolute_error: 0.0290\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAADFCAYAAABuHjrdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21dc140fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.188 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 1.761 RMSE\n",
      "Real\t:\n",
      " [[ -1.           0.        ]\n",
      " [  0.71000004  19.39999962]\n",
      " [  0.34000003  19.60000038]\n",
      " [ -1.          19.79999924]],\n",
      "Predict\t:\n",
      " [[ -0.87935549   0.99303907]\n",
      " [  0.5944525   15.51385784]\n",
      " [  0.041713    15.84730339]\n",
      " [ -1.10105813  15.33532619]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in test_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[-1], test_Y_hat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LSTM Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  \n",
    "[Attention Structure 3](https://distill.pub/2016/augmented-rnns/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyLSTMAttention (Feed-Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyLSTMAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='sigmoid',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyLSTMAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None, None]  # y, h, c\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.Wh_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wh_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.Wc_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wc_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # Cell State : 'input gate'(sigmoid) + 'cell state'(tanh)\n",
    "        num = len(['forget_date', 'input_gate', 'output_gate', 'h_tilda'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.Wh_s))\n",
    "        c0 = activations.tanh(K.dot(inputs[:, 0], self.Wc_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0, c0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before, ct_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"f\"; forget gate\n",
    "        # f = sigmoid(xt * Uf + ht-1 * Wf + bf) \n",
    "        # New f = sigmoid(xt * Uf + ht-1 * Wf + bf + context * Vf)\n",
    "        ft = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ct_before, self.U[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # now calculate the \"i\"; input gate\n",
    "        # i = sigmoid(xt * Ui + ht-1 * Wi + bi)\n",
    "        # New i = sigmoid(xt * Ui + ht-1 * Wi + bi + context * Vi)\n",
    "        it = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[1])\n",
    "            + K.dot(ct_before, self.U[1])\n",
    "            + K.dot(context, self.V[1])\n",
    "            + self.b[1])\n",
    "\n",
    "        # now calculate the \"o\"; output gate\n",
    "        # o = sigmoid(xt * Wo + ht-1 * Uo + bo)\n",
    "        # New o = sigmoid(xt * Wo + ht-1 * Uo + bo + context * Vo)\n",
    "        ot = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[2])\n",
    "            + K.dot(ct_before, self.U[2])\n",
    "            + K.dot(context, self.V[2])\n",
    "            + self.b[2])\n",
    "\n",
    "        # calculate the proposal \"c\"; cell state for now(tilda)\n",
    "        # c_tilda = tanh(xt * Wh + (ht-1 * ft) * Uh + bh)\n",
    "        # New c_tilda = tanh(xt * Wh + (ht-1 * ft) * Uh + bh + context * Vh)\n",
    "        c_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[3])\n",
    "            + K.dot(ht_before, self.U[3])\n",
    "            + K.dot(context, self.V[3])\n",
    "            + self.b[3])\n",
    "\n",
    "        # calculate the proposal \"c\"; cell state\n",
    "        # ct = ft * ct-1 + it * ct-tilda\n",
    "        ct = ft * ct_before + it * c_tilda\n",
    "        \n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        #ht = (1 - zt) * h_tilda + zt * ht_before\n",
    "        ht = ot * activations.tanh(ct)\n",
    "\n",
    "        \n",
    "        # Output Activation\n",
    "        y_ = (K.dot(yt_before, self.W_o)\n",
    "              + K.dot(ht_before, self.U_o)\n",
    "              + K.dot(context, self.C_o)\n",
    "              + self.b_o)\n",
    "\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(y_)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(y_)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(y_)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht, ct]\n",
    "        else:\n",
    "            return yt, [yt, ht, ct]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 64)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 4, 64)             9216      \n",
      "_________________________________________________________________\n",
      "MyLSTMAttentionDecoder (MyLS (None, 4, 2)              20102     \n",
      "=================================================================\n",
      "Total params: 29,318\n",
      "Trainable params: 29,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = seq_X.shape\n",
    "_, timestepY, ndimY = padded_seq_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "i = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = Bidirectional(LSTM(latent_dim, return_sequences=True), merge_mode='concat')(i)\n",
    "dec = MyLSTMAttentionDecoder(latent_dim, ndimY)(enc)\n",
    "model = Model(inputs=i, outputs=dec)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/1000\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.1663 - mean_absolute_error: 0.3815 - val_loss: 0.1052 - val_mean_absolute_error: 0.2568\n",
      "Epoch 2/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 0.1558 - mean_absolute_error: 0.3670 - val_loss: 0.0997 - val_mean_absolute_error: 0.2447\n",
      "Epoch 3/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.1447 - mean_absolute_error: 0.3504 - val_loss: 0.0958 - val_mean_absolute_error: 0.2433\n",
      "Epoch 4/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.1341 - mean_absolute_error: 0.3329 - val_loss: 0.0940 - val_mean_absolute_error: 0.2515\n",
      "Epoch 5/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.1244 - mean_absolute_error: 0.3168 - val_loss: 0.0947 - val_mean_absolute_error: 0.2638\n",
      "Epoch 6/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 0.1159 - mean_absolute_error: 0.3030 - val_loss: 0.0976 - val_mean_absolute_error: 0.2771\n",
      "Epoch 7/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.1088 - mean_absolute_error: 0.2920 - val_loss: 0.1022 - val_mean_absolute_error: 0.2895\n",
      "Epoch 8/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.1031 - mean_absolute_error: 0.2831 - val_loss: 0.1076 - val_mean_absolute_error: 0.3002\n",
      "Epoch 9/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0985 - mean_absolute_error: 0.2759 - val_loss: 0.1131 - val_mean_absolute_error: 0.3087\n",
      "Epoch 10/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0950 - mean_absolute_error: 0.2703 - val_loss: 0.1179 - val_mean_absolute_error: 0.3150\n",
      "Epoch 11/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 0.0921 - mean_absolute_error: 0.2657 - val_loss: 0.1216 - val_mean_absolute_error: 0.3192\n",
      "Epoch 12/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0897 - mean_absolute_error: 0.2617 - val_loss: 0.1240 - val_mean_absolute_error: 0.3215\n",
      "Epoch 13/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0876 - mean_absolute_error: 0.2581 - val_loss: 0.1253 - val_mean_absolute_error: 0.3223\n",
      "Epoch 14/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0856 - mean_absolute_error: 0.2548 - val_loss: 0.1255 - val_mean_absolute_error: 0.3218\n",
      "Epoch 15/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 0.0838 - mean_absolute_error: 0.2515 - val_loss: 0.1247 - val_mean_absolute_error: 0.3203\n",
      "Epoch 16/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 0.0819 - mean_absolute_error: 0.2483 - val_loss: 0.1230 - val_mean_absolute_error: 0.3180\n",
      "Epoch 17/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0801 - mean_absolute_error: 0.2452 - val_loss: 0.1207 - val_mean_absolute_error: 0.3149\n",
      "Epoch 18/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0783 - mean_absolute_error: 0.2423 - val_loss: 0.1179 - val_mean_absolute_error: 0.3112\n",
      "Epoch 19/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0766 - mean_absolute_error: 0.2393 - val_loss: 0.1147 - val_mean_absolute_error: 0.3071\n",
      "Epoch 20/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0749 - mean_absolute_error: 0.2363 - val_loss: 0.1112 - val_mean_absolute_error: 0.3025\n",
      "Epoch 21/1000\n",
      "52/52 [==============================] - 0s 636us/step - loss: 0.0733 - mean_absolute_error: 0.2333 - val_loss: 0.1075 - val_mean_absolute_error: 0.2975\n",
      "Epoch 22/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 0.0717 - mean_absolute_error: 0.2303 - val_loss: 0.1036 - val_mean_absolute_error: 0.2921\n",
      "Epoch 23/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 0.0703 - mean_absolute_error: 0.2274 - val_loss: 0.0997 - val_mean_absolute_error: 0.2864\n",
      "Epoch 24/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0689 - mean_absolute_error: 0.2244 - val_loss: 0.0957 - val_mean_absolute_error: 0.2804\n",
      "Epoch 25/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0677 - mean_absolute_error: 0.2215 - val_loss: 0.0919 - val_mean_absolute_error: 0.2740\n",
      "Epoch 26/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0666 - mean_absolute_error: 0.2186 - val_loss: 0.0882 - val_mean_absolute_error: 0.2674\n",
      "Epoch 27/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0655 - mean_absolute_error: 0.2157 - val_loss: 0.0848 - val_mean_absolute_error: 0.2607\n",
      "Epoch 28/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0645 - mean_absolute_error: 0.2129 - val_loss: 0.0816 - val_mean_absolute_error: 0.2540\n",
      "Epoch 29/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 0.0637 - mean_absolute_error: 0.2102 - val_loss: 0.0787 - val_mean_absolute_error: 0.2476\n",
      "Epoch 30/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0628 - mean_absolute_error: 0.2076 - val_loss: 0.0762 - val_mean_absolute_error: 0.2415\n",
      "Epoch 31/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0620 - mean_absolute_error: 0.2051 - val_loss: 0.0740 - val_mean_absolute_error: 0.2359\n",
      "Epoch 32/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0613 - mean_absolute_error: 0.2026 - val_loss: 0.0722 - val_mean_absolute_error: 0.2309\n",
      "Epoch 33/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0606 - mean_absolute_error: 0.2001 - val_loss: 0.0708 - val_mean_absolute_error: 0.2265\n",
      "Epoch 34/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0599 - mean_absolute_error: 0.1976 - val_loss: 0.0696 - val_mean_absolute_error: 0.2226\n",
      "Epoch 35/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0592 - mean_absolute_error: 0.1951 - val_loss: 0.0686 - val_mean_absolute_error: 0.2193\n",
      "Epoch 36/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0586 - mean_absolute_error: 0.1927 - val_loss: 0.0678 - val_mean_absolute_error: 0.2164\n",
      "Epoch 37/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0579 - mean_absolute_error: 0.1903 - val_loss: 0.0670 - val_mean_absolute_error: 0.2137\n",
      "Epoch 38/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0573 - mean_absolute_error: 0.1880 - val_loss: 0.0663 - val_mean_absolute_error: 0.2113\n",
      "Epoch 39/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0568 - mean_absolute_error: 0.1858 - val_loss: 0.0656 - val_mean_absolute_error: 0.2089\n",
      "Epoch 40/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 0.0562 - mean_absolute_error: 0.1836 - val_loss: 0.0648 - val_mean_absolute_error: 0.2063\n",
      "Epoch 41/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0557 - mean_absolute_error: 0.1816 - val_loss: 0.0639 - val_mean_absolute_error: 0.2036\n",
      "Epoch 42/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0552 - mean_absolute_error: 0.1796 - val_loss: 0.0629 - val_mean_absolute_error: 0.2006\n",
      "Epoch 43/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0548 - mean_absolute_error: 0.1777 - val_loss: 0.0619 - val_mean_absolute_error: 0.1974\n",
      "Epoch 44/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0544 - mean_absolute_error: 0.1759 - val_loss: 0.0608 - val_mean_absolute_error: 0.1938\n",
      "Epoch 45/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 0.0540 - mean_absolute_error: 0.1742 - val_loss: 0.0597 - val_mean_absolute_error: 0.1903\n",
      "Epoch 46/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0536 - mean_absolute_error: 0.1725 - val_loss: 0.0586 - val_mean_absolute_error: 0.1866\n",
      "Epoch 47/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0532 - mean_absolute_error: 0.1709 - val_loss: 0.0575 - val_mean_absolute_error: 0.1830\n",
      "Epoch 48/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0529 - mean_absolute_error: 0.1694 - val_loss: 0.0565 - val_mean_absolute_error: 0.1794\n",
      "Epoch 49/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0526 - mean_absolute_error: 0.1680 - val_loss: 0.0556 - val_mean_absolute_error: 0.1761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 0.0523 - mean_absolute_error: 0.1665 - val_loss: 0.0548 - val_mean_absolute_error: 0.1731\n",
      "Epoch 51/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0520 - mean_absolute_error: 0.1651 - val_loss: 0.0541 - val_mean_absolute_error: 0.1705\n",
      "Epoch 52/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0517 - mean_absolute_error: 0.1638 - val_loss: 0.0535 - val_mean_absolute_error: 0.1683\n",
      "Epoch 53/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0514 - mean_absolute_error: 0.1626 - val_loss: 0.0529 - val_mean_absolute_error: 0.1665\n",
      "Epoch 54/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0512 - mean_absolute_error: 0.1614 - val_loss: 0.0525 - val_mean_absolute_error: 0.1648\n",
      "Epoch 55/1000\n",
      "52/52 [==============================] - 0s 631us/step - loss: 0.0509 - mean_absolute_error: 0.1602 - val_loss: 0.0520 - val_mean_absolute_error: 0.1633\n",
      "Epoch 56/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0507 - mean_absolute_error: 0.1591 - val_loss: 0.0517 - val_mean_absolute_error: 0.1618\n",
      "Epoch 57/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0505 - mean_absolute_error: 0.1580 - val_loss: 0.0513 - val_mean_absolute_error: 0.1603\n",
      "Epoch 58/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0503 - mean_absolute_error: 0.1570 - val_loss: 0.0510 - val_mean_absolute_error: 0.1589\n",
      "Epoch 59/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0502 - mean_absolute_error: 0.1560 - val_loss: 0.0507 - val_mean_absolute_error: 0.1576\n",
      "Epoch 60/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0500 - mean_absolute_error: 0.1551 - val_loss: 0.0504 - val_mean_absolute_error: 0.1563\n",
      "Epoch 61/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0499 - mean_absolute_error: 0.1543 - val_loss: 0.0502 - val_mean_absolute_error: 0.1550\n",
      "Epoch 62/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0497 - mean_absolute_error: 0.1535 - val_loss: 0.0500 - val_mean_absolute_error: 0.1540\n",
      "Epoch 63/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0496 - mean_absolute_error: 0.1527 - val_loss: 0.0499 - val_mean_absolute_error: 0.1531\n",
      "Epoch 64/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0495 - mean_absolute_error: 0.1519 - val_loss: 0.0498 - val_mean_absolute_error: 0.1522\n",
      "Epoch 65/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0493 - mean_absolute_error: 0.1512 - val_loss: 0.0496 - val_mean_absolute_error: 0.1514\n",
      "Epoch 66/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0492 - mean_absolute_error: 0.1505 - val_loss: 0.0495 - val_mean_absolute_error: 0.1507\n",
      "Epoch 67/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0491 - mean_absolute_error: 0.1498 - val_loss: 0.0494 - val_mean_absolute_error: 0.1500\n",
      "Epoch 68/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0490 - mean_absolute_error: 0.1491 - val_loss: 0.0493 - val_mean_absolute_error: 0.1493\n",
      "Epoch 69/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0489 - mean_absolute_error: 0.1485 - val_loss: 0.0492 - val_mean_absolute_error: 0.1486\n",
      "Epoch 70/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 0.0489 - mean_absolute_error: 0.1478 - val_loss: 0.0491 - val_mean_absolute_error: 0.1479\n",
      "Epoch 71/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 0.0488 - mean_absolute_error: 0.1472 - val_loss: 0.0489 - val_mean_absolute_error: 0.1472\n",
      "Epoch 72/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0487 - mean_absolute_error: 0.1466 - val_loss: 0.0488 - val_mean_absolute_error: 0.1465\n",
      "Epoch 73/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0486 - mean_absolute_error: 0.1459 - val_loss: 0.0488 - val_mean_absolute_error: 0.1458\n",
      "Epoch 74/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 0.0485 - mean_absolute_error: 0.1453 - val_loss: 0.0487 - val_mean_absolute_error: 0.1451\n",
      "Epoch 75/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0485 - mean_absolute_error: 0.1448 - val_loss: 0.0486 - val_mean_absolute_error: 0.1445\n",
      "Epoch 76/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0484 - mean_absolute_error: 0.1442 - val_loss: 0.0485 - val_mean_absolute_error: 0.1438\n",
      "Epoch 77/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 0.0483 - mean_absolute_error: 0.1436 - val_loss: 0.0484 - val_mean_absolute_error: 0.1432\n",
      "Epoch 78/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0483 - mean_absolute_error: 0.1431 - val_loss: 0.0484 - val_mean_absolute_error: 0.1426\n",
      "Epoch 79/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0482 - mean_absolute_error: 0.1425 - val_loss: 0.0483 - val_mean_absolute_error: 0.1421\n",
      "Epoch 80/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0482 - mean_absolute_error: 0.1420 - val_loss: 0.0482 - val_mean_absolute_error: 0.1415\n",
      "Epoch 81/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0481 - mean_absolute_error: 0.1415 - val_loss: 0.0482 - val_mean_absolute_error: 0.1409\n",
      "Epoch 82/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0481 - mean_absolute_error: 0.1410 - val_loss: 0.0481 - val_mean_absolute_error: 0.1404\n",
      "Epoch 83/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0480 - mean_absolute_error: 0.1405 - val_loss: 0.0480 - val_mean_absolute_error: 0.1399\n",
      "Epoch 84/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 0.0480 - mean_absolute_error: 0.1401 - val_loss: 0.0480 - val_mean_absolute_error: 0.1394\n",
      "Epoch 85/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0479 - mean_absolute_error: 0.1396 - val_loss: 0.0479 - val_mean_absolute_error: 0.1389\n",
      "Epoch 86/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0479 - mean_absolute_error: 0.1392 - val_loss: 0.0478 - val_mean_absolute_error: 0.1385\n",
      "Epoch 87/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0478 - mean_absolute_error: 0.1387 - val_loss: 0.0478 - val_mean_absolute_error: 0.1380\n",
      "Epoch 88/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0478 - mean_absolute_error: 0.1383 - val_loss: 0.0477 - val_mean_absolute_error: 0.1376\n",
      "Epoch 89/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0477 - mean_absolute_error: 0.1379 - val_loss: 0.0477 - val_mean_absolute_error: 0.1372\n",
      "Epoch 90/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0477 - mean_absolute_error: 0.1375 - val_loss: 0.0476 - val_mean_absolute_error: 0.1368\n",
      "Epoch 91/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 0.0477 - mean_absolute_error: 0.1371 - val_loss: 0.0476 - val_mean_absolute_error: 0.1363\n",
      "Epoch 92/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 0.0476 - mean_absolute_error: 0.1367 - val_loss: 0.0475 - val_mean_absolute_error: 0.1359\n",
      "Epoch 93/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 0.0476 - mean_absolute_error: 0.1363 - val_loss: 0.0475 - val_mean_absolute_error: 0.1355\n",
      "Epoch 94/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0476 - mean_absolute_error: 0.1360 - val_loss: 0.0474 - val_mean_absolute_error: 0.1351\n",
      "Epoch 95/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0475 - mean_absolute_error: 0.1356 - val_loss: 0.0474 - val_mean_absolute_error: 0.1347\n",
      "Epoch 96/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 0.0475 - mean_absolute_error: 0.1353 - val_loss: 0.0473 - val_mean_absolute_error: 0.1342\n",
      "Epoch 97/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0475 - mean_absolute_error: 0.1349 - val_loss: 0.0473 - val_mean_absolute_error: 0.1338\n",
      "Epoch 98/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0474 - mean_absolute_error: 0.1346 - val_loss: 0.0472 - val_mean_absolute_error: 0.1334\n",
      "Epoch 99/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 612us/step - loss: 0.0474 - mean_absolute_error: 0.1343 - val_loss: 0.0472 - val_mean_absolute_error: 0.1330\n",
      "Epoch 100/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0474 - mean_absolute_error: 0.1339 - val_loss: 0.0472 - val_mean_absolute_error: 0.1326\n",
      "Epoch 101/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0474 - mean_absolute_error: 0.1336 - val_loss: 0.0471 - val_mean_absolute_error: 0.1322\n",
      "Epoch 102/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0473 - mean_absolute_error: 0.1333 - val_loss: 0.0471 - val_mean_absolute_error: 0.1318\n",
      "Epoch 103/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 0.0473 - mean_absolute_error: 0.1330 - val_loss: 0.0471 - val_mean_absolute_error: 0.1315\n",
      "Epoch 104/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0473 - mean_absolute_error: 0.1328 - val_loss: 0.0470 - val_mean_absolute_error: 0.1311\n",
      "Epoch 105/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0473 - mean_absolute_error: 0.1325 - val_loss: 0.0470 - val_mean_absolute_error: 0.1308\n",
      "Epoch 106/1000\n",
      "52/52 [==============================] - 0s 641us/step - loss: 0.0472 - mean_absolute_error: 0.1322 - val_loss: 0.0470 - val_mean_absolute_error: 0.1304\n",
      "Epoch 107/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0472 - mean_absolute_error: 0.1319 - val_loss: 0.0470 - val_mean_absolute_error: 0.1301\n",
      "Epoch 108/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0472 - mean_absolute_error: 0.1317 - val_loss: 0.0469 - val_mean_absolute_error: 0.1298\n",
      "Epoch 109/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0472 - mean_absolute_error: 0.1314 - val_loss: 0.0469 - val_mean_absolute_error: 0.1295\n",
      "Epoch 110/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0472 - mean_absolute_error: 0.1312 - val_loss: 0.0469 - val_mean_absolute_error: 0.1292\n",
      "Epoch 111/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0471 - mean_absolute_error: 0.1310 - val_loss: 0.0469 - val_mean_absolute_error: 0.1289\n",
      "Epoch 112/1000\n",
      "52/52 [==============================] - 0s 631us/step - loss: 0.0471 - mean_absolute_error: 0.1308 - val_loss: 0.0468 - val_mean_absolute_error: 0.1286\n",
      "Epoch 113/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0471 - mean_absolute_error: 0.1305 - val_loss: 0.0468 - val_mean_absolute_error: 0.1284\n",
      "Epoch 114/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0471 - mean_absolute_error: 0.1303 - val_loss: 0.0468 - val_mean_absolute_error: 0.1281\n",
      "Epoch 115/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0471 - mean_absolute_error: 0.1301 - val_loss: 0.0468 - val_mean_absolute_error: 0.1278\n",
      "Epoch 116/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0470 - mean_absolute_error: 0.1299 - val_loss: 0.0468 - val_mean_absolute_error: 0.1276\n",
      "Epoch 117/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0470 - mean_absolute_error: 0.1297 - val_loss: 0.0467 - val_mean_absolute_error: 0.1273\n",
      "Epoch 118/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0470 - mean_absolute_error: 0.1295 - val_loss: 0.0467 - val_mean_absolute_error: 0.1271\n",
      "Epoch 119/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0470 - mean_absolute_error: 0.1294 - val_loss: 0.0467 - val_mean_absolute_error: 0.1268\n",
      "Epoch 120/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 0.0470 - mean_absolute_error: 0.1292 - val_loss: 0.0467 - val_mean_absolute_error: 0.1266\n",
      "Epoch 121/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0469 - mean_absolute_error: 0.1290 - val_loss: 0.0467 - val_mean_absolute_error: 0.1264\n",
      "Epoch 122/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 0.0469 - mean_absolute_error: 0.1288 - val_loss: 0.0466 - val_mean_absolute_error: 0.1262\n",
      "Epoch 123/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0469 - mean_absolute_error: 0.1287 - val_loss: 0.0466 - val_mean_absolute_error: 0.1260\n",
      "Epoch 124/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0469 - mean_absolute_error: 0.1285 - val_loss: 0.0466 - val_mean_absolute_error: 0.1259\n",
      "Epoch 125/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0469 - mean_absolute_error: 0.1283 - val_loss: 0.0466 - val_mean_absolute_error: 0.1257\n",
      "Epoch 126/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0468 - mean_absolute_error: 0.1282 - val_loss: 0.0466 - val_mean_absolute_error: 0.1255\n",
      "Epoch 127/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 0.0468 - mean_absolute_error: 0.1280 - val_loss: 0.0465 - val_mean_absolute_error: 0.1254\n",
      "Epoch 128/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0468 - mean_absolute_error: 0.1279 - val_loss: 0.0465 - val_mean_absolute_error: 0.1252\n",
      "Epoch 129/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 0.0468 - mean_absolute_error: 0.1277 - val_loss: 0.0465 - val_mean_absolute_error: 0.1250\n",
      "Epoch 130/1000\n",
      "52/52 [==============================] - 0s 646us/step - loss: 0.0467 - mean_absolute_error: 0.1276 - val_loss: 0.0465 - val_mean_absolute_error: 0.1249\n",
      "Epoch 131/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0467 - mean_absolute_error: 0.1275 - val_loss: 0.0465 - val_mean_absolute_error: 0.1248\n",
      "Epoch 132/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0467 - mean_absolute_error: 0.1273 - val_loss: 0.0464 - val_mean_absolute_error: 0.1247\n",
      "Epoch 133/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0467 - mean_absolute_error: 0.1272 - val_loss: 0.0464 - val_mean_absolute_error: 0.1246\n",
      "Epoch 134/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0466 - mean_absolute_error: 0.1271 - val_loss: 0.0464 - val_mean_absolute_error: 0.1245\n",
      "Epoch 135/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0466 - mean_absolute_error: 0.1269 - val_loss: 0.0464 - val_mean_absolute_error: 0.1244\n",
      "Epoch 136/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0466 - mean_absolute_error: 0.1268 - val_loss: 0.0463 - val_mean_absolute_error: 0.1243\n",
      "Epoch 137/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0465 - mean_absolute_error: 0.1267 - val_loss: 0.0463 - val_mean_absolute_error: 0.1242\n",
      "Epoch 138/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0465 - mean_absolute_error: 0.1265 - val_loss: 0.0463 - val_mean_absolute_error: 0.1241\n",
      "Epoch 139/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0464 - mean_absolute_error: 0.1264 - val_loss: 0.0462 - val_mean_absolute_error: 0.1240\n",
      "Epoch 140/1000\n",
      "52/52 [==============================] - 0s 644us/step - loss: 0.0464 - mean_absolute_error: 0.1262 - val_loss: 0.0462 - val_mean_absolute_error: 0.1239\n",
      "Epoch 141/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0464 - mean_absolute_error: 0.1261 - val_loss: 0.0461 - val_mean_absolute_error: 0.1238\n",
      "Epoch 142/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0463 - mean_absolute_error: 0.1260 - val_loss: 0.0461 - val_mean_absolute_error: 0.1237\n",
      "Epoch 143/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0462 - mean_absolute_error: 0.1258 - val_loss: 0.0461 - val_mean_absolute_error: 0.1236\n",
      "Epoch 144/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0462 - mean_absolute_error: 0.1257 - val_loss: 0.0460 - val_mean_absolute_error: 0.1235\n",
      "Epoch 145/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0461 - mean_absolute_error: 0.1255 - val_loss: 0.0459 - val_mean_absolute_error: 0.1234\n",
      "Epoch 146/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0460 - mean_absolute_error: 0.1254 - val_loss: 0.0459 - val_mean_absolute_error: 0.1233\n",
      "Epoch 147/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0460 - mean_absolute_error: 0.1252 - val_loss: 0.0458 - val_mean_absolute_error: 0.1232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0459 - mean_absolute_error: 0.1251 - val_loss: 0.0458 - val_mean_absolute_error: 0.1231\n",
      "Epoch 149/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0458 - mean_absolute_error: 0.1249 - val_loss: 0.0457 - val_mean_absolute_error: 0.1230\n",
      "Epoch 150/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0457 - mean_absolute_error: 0.1247 - val_loss: 0.0456 - val_mean_absolute_error: 0.1229\n",
      "Epoch 151/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0456 - mean_absolute_error: 0.1246 - val_loss: 0.0455 - val_mean_absolute_error: 0.1227\n",
      "Epoch 152/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 0.0455 - mean_absolute_error: 0.1244 - val_loss: 0.0454 - val_mean_absolute_error: 0.1226\n",
      "Epoch 153/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0454 - mean_absolute_error: 0.1242 - val_loss: 0.0453 - val_mean_absolute_error: 0.1225\n",
      "Epoch 154/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0452 - mean_absolute_error: 0.1240 - val_loss: 0.0452 - val_mean_absolute_error: 0.1223\n",
      "Epoch 155/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 0.0451 - mean_absolute_error: 0.1238 - val_loss: 0.0451 - val_mean_absolute_error: 0.1222\n",
      "Epoch 156/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0449 - mean_absolute_error: 0.1236 - val_loss: 0.0450 - val_mean_absolute_error: 0.1221\n",
      "Epoch 157/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0448 - mean_absolute_error: 0.1234 - val_loss: 0.0448 - val_mean_absolute_error: 0.1219\n",
      "Epoch 158/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0446 - mean_absolute_error: 0.1232 - val_loss: 0.0447 - val_mean_absolute_error: 0.1218\n",
      "Epoch 159/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0444 - mean_absolute_error: 0.1229 - val_loss: 0.0445 - val_mean_absolute_error: 0.1216\n",
      "Epoch 160/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0442 - mean_absolute_error: 0.1226 - val_loss: 0.0443 - val_mean_absolute_error: 0.1215\n",
      "Epoch 161/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0440 - mean_absolute_error: 0.1224 - val_loss: 0.0441 - val_mean_absolute_error: 0.1213\n",
      "Epoch 162/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 0.0437 - mean_absolute_error: 0.1221 - val_loss: 0.0438 - val_mean_absolute_error: 0.1210\n",
      "Epoch 163/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0435 - mean_absolute_error: 0.1218 - val_loss: 0.0436 - val_mean_absolute_error: 0.1208\n",
      "Epoch 164/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0432 - mean_absolute_error: 0.1215 - val_loss: 0.0433 - val_mean_absolute_error: 0.1205\n",
      "Epoch 165/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0429 - mean_absolute_error: 0.1212 - val_loss: 0.0430 - val_mean_absolute_error: 0.1203\n",
      "Epoch 166/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0426 - mean_absolute_error: 0.1209 - val_loss: 0.0427 - val_mean_absolute_error: 0.1200\n",
      "Epoch 167/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0422 - mean_absolute_error: 0.1205 - val_loss: 0.0423 - val_mean_absolute_error: 0.1197\n",
      "Epoch 168/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0418 - mean_absolute_error: 0.1202 - val_loss: 0.0419 - val_mean_absolute_error: 0.1194\n",
      "Epoch 169/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0414 - mean_absolute_error: 0.1198 - val_loss: 0.0415 - val_mean_absolute_error: 0.1191\n",
      "Epoch 170/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0410 - mean_absolute_error: 0.1194 - val_loss: 0.0410 - val_mean_absolute_error: 0.1188\n",
      "Epoch 171/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0406 - mean_absolute_error: 0.1189 - val_loss: 0.0406 - val_mean_absolute_error: 0.1185\n",
      "Epoch 172/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0401 - mean_absolute_error: 0.1185 - val_loss: 0.0401 - val_mean_absolute_error: 0.1181\n",
      "Epoch 173/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0396 - mean_absolute_error: 0.1180 - val_loss: 0.0395 - val_mean_absolute_error: 0.1177\n",
      "Epoch 174/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0391 - mean_absolute_error: 0.1174 - val_loss: 0.0390 - val_mean_absolute_error: 0.1174\n",
      "Epoch 175/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0386 - mean_absolute_error: 0.1168 - val_loss: 0.0384 - val_mean_absolute_error: 0.1169\n",
      "Epoch 176/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0380 - mean_absolute_error: 0.1162 - val_loss: 0.0378 - val_mean_absolute_error: 0.1164\n",
      "Epoch 177/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0374 - mean_absolute_error: 0.1156 - val_loss: 0.0372 - val_mean_absolute_error: 0.1159\n",
      "Epoch 178/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 0.0368 - mean_absolute_error: 0.1149 - val_loss: 0.0365 - val_mean_absolute_error: 0.1153\n",
      "Epoch 179/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0361 - mean_absolute_error: 0.1141 - val_loss: 0.0359 - val_mean_absolute_error: 0.1147\n",
      "Epoch 180/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0354 - mean_absolute_error: 0.1134 - val_loss: 0.0352 - val_mean_absolute_error: 0.1141\n",
      "Epoch 181/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0347 - mean_absolute_error: 0.1125 - val_loss: 0.0344 - val_mean_absolute_error: 0.1134\n",
      "Epoch 182/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0339 - mean_absolute_error: 0.1117 - val_loss: 0.0337 - val_mean_absolute_error: 0.1126\n",
      "Epoch 183/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0331 - mean_absolute_error: 0.1107 - val_loss: 0.0329 - val_mean_absolute_error: 0.1118\n",
      "Epoch 184/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0323 - mean_absolute_error: 0.1097 - val_loss: 0.0320 - val_mean_absolute_error: 0.1108\n",
      "Epoch 185/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 0.0314 - mean_absolute_error: 0.1086 - val_loss: 0.0312 - val_mean_absolute_error: 0.1098\n",
      "Epoch 186/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0306 - mean_absolute_error: 0.1075 - val_loss: 0.0303 - val_mean_absolute_error: 0.1087\n",
      "Epoch 187/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0296 - mean_absolute_error: 0.1062 - val_loss: 0.0294 - val_mean_absolute_error: 0.1075\n",
      "Epoch 188/1000\n",
      "52/52 [==============================] - 0s 636us/step - loss: 0.0287 - mean_absolute_error: 0.1049 - val_loss: 0.0285 - val_mean_absolute_error: 0.1062\n",
      "Epoch 189/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0277 - mean_absolute_error: 0.1036 - val_loss: 0.0275 - val_mean_absolute_error: 0.1048\n",
      "Epoch 190/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0267 - mean_absolute_error: 0.1021 - val_loss: 0.0265 - val_mean_absolute_error: 0.1034\n",
      "Epoch 191/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0257 - mean_absolute_error: 0.1006 - val_loss: 0.0255 - val_mean_absolute_error: 0.1020\n",
      "Epoch 192/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0247 - mean_absolute_error: 0.0990 - val_loss: 0.0245 - val_mean_absolute_error: 0.1005\n",
      "Epoch 193/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0236 - mean_absolute_error: 0.0973 - val_loss: 0.0235 - val_mean_absolute_error: 0.0991\n",
      "Epoch 194/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 0.0226 - mean_absolute_error: 0.0955 - val_loss: 0.0225 - val_mean_absolute_error: 0.0976\n",
      "Epoch 195/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0216 - mean_absolute_error: 0.0937 - val_loss: 0.0216 - val_mean_absolute_error: 0.0960\n",
      "Epoch 196/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0206 - mean_absolute_error: 0.0918 - val_loss: 0.0206 - val_mean_absolute_error: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0196 - mean_absolute_error: 0.0898 - val_loss: 0.0197 - val_mean_absolute_error: 0.0924\n",
      "Epoch 198/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0186 - mean_absolute_error: 0.0878 - val_loss: 0.0188 - val_mean_absolute_error: 0.0906\n",
      "Epoch 199/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0177 - mean_absolute_error: 0.0858 - val_loss: 0.0180 - val_mean_absolute_error: 0.0886\n",
      "Epoch 200/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0168 - mean_absolute_error: 0.0839 - val_loss: 0.0172 - val_mean_absolute_error: 0.0867\n",
      "Epoch 201/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0159 - mean_absolute_error: 0.0819 - val_loss: 0.0164 - val_mean_absolute_error: 0.0848\n",
      "Epoch 202/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0151 - mean_absolute_error: 0.0800 - val_loss: 0.0156 - val_mean_absolute_error: 0.0830\n",
      "Epoch 203/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 0.0143 - mean_absolute_error: 0.0781 - val_loss: 0.0149 - val_mean_absolute_error: 0.0812\n",
      "Epoch 204/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0135 - mean_absolute_error: 0.0762 - val_loss: 0.0143 - val_mean_absolute_error: 0.0795\n",
      "Epoch 205/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 0.0129 - mean_absolute_error: 0.0744 - val_loss: 0.0136 - val_mean_absolute_error: 0.0778\n",
      "Epoch 206/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0122 - mean_absolute_error: 0.0726 - val_loss: 0.0130 - val_mean_absolute_error: 0.0761\n",
      "Epoch 207/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0116 - mean_absolute_error: 0.0709 - val_loss: 0.0124 - val_mean_absolute_error: 0.0744\n",
      "Epoch 208/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0110 - mean_absolute_error: 0.0692 - val_loss: 0.0119 - val_mean_absolute_error: 0.0727\n",
      "Epoch 209/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0105 - mean_absolute_error: 0.0676 - val_loss: 0.0113 - val_mean_absolute_error: 0.0711\n",
      "Epoch 210/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 0.0099 - mean_absolute_error: 0.0660 - val_loss: 0.0108 - val_mean_absolute_error: 0.0695\n",
      "Epoch 211/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0095 - mean_absolute_error: 0.0644 - val_loss: 0.0104 - val_mean_absolute_error: 0.0679\n",
      "Epoch 212/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 0.0090 - mean_absolute_error: 0.0629 - val_loss: 0.0099 - val_mean_absolute_error: 0.0664\n",
      "Epoch 213/1000\n",
      "52/52 [==============================] - 0s 637us/step - loss: 0.0086 - mean_absolute_error: 0.0613 - val_loss: 0.0095 - val_mean_absolute_error: 0.0649\n",
      "Epoch 214/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0082 - mean_absolute_error: 0.0598 - val_loss: 0.0091 - val_mean_absolute_error: 0.0635\n",
      "Epoch 215/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0078 - mean_absolute_error: 0.0584 - val_loss: 0.0087 - val_mean_absolute_error: 0.0622\n",
      "Epoch 216/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0074 - mean_absolute_error: 0.0570 - val_loss: 0.0083 - val_mean_absolute_error: 0.0609\n",
      "Epoch 217/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0071 - mean_absolute_error: 0.0556 - val_loss: 0.0080 - val_mean_absolute_error: 0.0596\n",
      "Epoch 218/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 0.0067 - mean_absolute_error: 0.0543 - val_loss: 0.0076 - val_mean_absolute_error: 0.0583\n",
      "Epoch 219/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0064 - mean_absolute_error: 0.0530 - val_loss: 0.0073 - val_mean_absolute_error: 0.0571\n",
      "Epoch 220/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0061 - mean_absolute_error: 0.0518 - val_loss: 0.0070 - val_mean_absolute_error: 0.0558\n",
      "Epoch 221/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0059 - mean_absolute_error: 0.0506 - val_loss: 0.0067 - val_mean_absolute_error: 0.0546\n",
      "Epoch 222/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0056 - mean_absolute_error: 0.0494 - val_loss: 0.0064 - val_mean_absolute_error: 0.0535\n",
      "Epoch 223/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0054 - mean_absolute_error: 0.0484 - val_loss: 0.0062 - val_mean_absolute_error: 0.0523\n",
      "Epoch 224/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0051 - mean_absolute_error: 0.0473 - val_loss: 0.0060 - val_mean_absolute_error: 0.0513\n",
      "Epoch 225/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.0049 - mean_absolute_error: 0.0463 - val_loss: 0.0057 - val_mean_absolute_error: 0.0502\n",
      "Epoch 226/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0047 - mean_absolute_error: 0.0454 - val_loss: 0.0055 - val_mean_absolute_error: 0.0492\n",
      "Epoch 227/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0045 - mean_absolute_error: 0.0444 - val_loss: 0.0053 - val_mean_absolute_error: 0.0483\n",
      "Epoch 228/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0043 - mean_absolute_error: 0.0435 - val_loss: 0.0051 - val_mean_absolute_error: 0.0474\n",
      "Epoch 229/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 0.0042 - mean_absolute_error: 0.0426 - val_loss: 0.0050 - val_mean_absolute_error: 0.0465\n",
      "Epoch 230/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0040 - mean_absolute_error: 0.0417 - val_loss: 0.0048 - val_mean_absolute_error: 0.0457\n",
      "Epoch 231/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 0.0038 - mean_absolute_error: 0.0408 - val_loss: 0.0047 - val_mean_absolute_error: 0.0449\n",
      "Epoch 232/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0037 - mean_absolute_error: 0.0400 - val_loss: 0.0045 - val_mean_absolute_error: 0.0442\n",
      "Epoch 233/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0035 - mean_absolute_error: 0.0391 - val_loss: 0.0044 - val_mean_absolute_error: 0.0437\n",
      "Epoch 234/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0034 - mean_absolute_error: 0.0383 - val_loss: 0.0042 - val_mean_absolute_error: 0.0431\n",
      "Epoch 235/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0033 - mean_absolute_error: 0.0376 - val_loss: 0.0041 - val_mean_absolute_error: 0.0426\n",
      "Epoch 236/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0032 - mean_absolute_error: 0.0368 - val_loss: 0.0040 - val_mean_absolute_error: 0.0421\n",
      "Epoch 237/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0030 - mean_absolute_error: 0.0361 - val_loss: 0.0039 - val_mean_absolute_error: 0.0416\n",
      "Epoch 238/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0029 - mean_absolute_error: 0.0353 - val_loss: 0.0038 - val_mean_absolute_error: 0.0411\n",
      "Epoch 239/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0028 - mean_absolute_error: 0.0347 - val_loss: 0.0037 - val_mean_absolute_error: 0.0407\n",
      "Epoch 240/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0027 - mean_absolute_error: 0.0340 - val_loss: 0.0036 - val_mean_absolute_error: 0.0402\n",
      "Epoch 241/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0026 - mean_absolute_error: 0.0334 - val_loss: 0.0035 - val_mean_absolute_error: 0.0398\n",
      "Epoch 242/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0025 - mean_absolute_error: 0.0328 - val_loss: 0.0034 - val_mean_absolute_error: 0.0394\n",
      "Epoch 243/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0025 - mean_absolute_error: 0.0322 - val_loss: 0.0033 - val_mean_absolute_error: 0.0390\n",
      "Epoch 244/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 0.0024 - mean_absolute_error: 0.0316 - val_loss: 0.0033 - val_mean_absolute_error: 0.0386\n",
      "Epoch 245/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 0.0023 - mean_absolute_error: 0.0310 - val_loss: 0.0032 - val_mean_absolute_error: 0.0383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0022 - mean_absolute_error: 0.0305 - val_loss: 0.0031 - val_mean_absolute_error: 0.0379\n",
      "Epoch 247/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 0.0022 - mean_absolute_error: 0.0300 - val_loss: 0.0030 - val_mean_absolute_error: 0.0376\n",
      "Epoch 248/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0021 - mean_absolute_error: 0.0294 - val_loss: 0.0030 - val_mean_absolute_error: 0.0373\n",
      "Epoch 249/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0020 - mean_absolute_error: 0.0289 - val_loss: 0.0029 - val_mean_absolute_error: 0.0370\n",
      "Epoch 250/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0020 - mean_absolute_error: 0.0285 - val_loss: 0.0029 - val_mean_absolute_error: 0.0367\n",
      "Epoch 251/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 0.0019 - mean_absolute_error: 0.0280 - val_loss: 0.0028 - val_mean_absolute_error: 0.0364\n",
      "Epoch 252/1000\n",
      "52/52 [==============================] - 0s 636us/step - loss: 0.0018 - mean_absolute_error: 0.0276 - val_loss: 0.0027 - val_mean_absolute_error: 0.0361\n",
      "Epoch 253/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 0.0018 - mean_absolute_error: 0.0271 - val_loss: 0.0027 - val_mean_absolute_error: 0.0358\n",
      "Epoch 254/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0017 - mean_absolute_error: 0.0267 - val_loss: 0.0026 - val_mean_absolute_error: 0.0355\n",
      "Epoch 255/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0017 - mean_absolute_error: 0.0264 - val_loss: 0.0026 - val_mean_absolute_error: 0.0352\n",
      "Epoch 256/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 0.0017 - mean_absolute_error: 0.0260 - val_loss: 0.0025 - val_mean_absolute_error: 0.0350\n",
      "Epoch 257/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0016 - mean_absolute_error: 0.0256 - val_loss: 0.0025 - val_mean_absolute_error: 0.0347\n",
      "Epoch 258/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0016 - mean_absolute_error: 0.0253 - val_loss: 0.0025 - val_mean_absolute_error: 0.0345\n",
      "Epoch 259/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 0.0015 - mean_absolute_error: 0.0249 - val_loss: 0.0024 - val_mean_absolute_error: 0.0342\n",
      "Epoch 260/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0015 - mean_absolute_error: 0.0246 - val_loss: 0.0024 - val_mean_absolute_error: 0.0340\n",
      "Epoch 261/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0014 - mean_absolute_error: 0.0243 - val_loss: 0.0023 - val_mean_absolute_error: 0.0338\n",
      "Epoch 262/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0014 - mean_absolute_error: 0.0240 - val_loss: 0.0023 - val_mean_absolute_error: 0.0336\n",
      "Epoch 263/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0014 - mean_absolute_error: 0.0237 - val_loss: 0.0023 - val_mean_absolute_error: 0.0334\n",
      "Epoch 264/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0013 - mean_absolute_error: 0.0234 - val_loss: 0.0022 - val_mean_absolute_error: 0.0332\n",
      "Epoch 265/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0013 - mean_absolute_error: 0.0231 - val_loss: 0.0022 - val_mean_absolute_error: 0.0330\n",
      "Epoch 266/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0013 - mean_absolute_error: 0.0229 - val_loss: 0.0022 - val_mean_absolute_error: 0.0329\n",
      "Epoch 267/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0013 - mean_absolute_error: 0.0226 - val_loss: 0.0022 - val_mean_absolute_error: 0.0327\n",
      "Epoch 268/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0012 - mean_absolute_error: 0.0224 - val_loss: 0.0021 - val_mean_absolute_error: 0.0325\n",
      "Epoch 269/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0012 - mean_absolute_error: 0.0222 - val_loss: 0.0021 - val_mean_absolute_error: 0.0323\n",
      "Epoch 270/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0012 - mean_absolute_error: 0.0220 - val_loss: 0.0021 - val_mean_absolute_error: 0.0321\n",
      "Epoch 271/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 0.0012 - mean_absolute_error: 0.0218 - val_loss: 0.0021 - val_mean_absolute_error: 0.0320\n",
      "Epoch 272/1000\n",
      "52/52 [==============================] - 0s 647us/step - loss: 0.0011 - mean_absolute_error: 0.0216 - val_loss: 0.0020 - val_mean_absolute_error: 0.0318\n",
      "Epoch 273/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0011 - mean_absolute_error: 0.0214 - val_loss: 0.0020 - val_mean_absolute_error: 0.0316\n",
      "Epoch 274/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 0.0011 - mean_absolute_error: 0.0212 - val_loss: 0.0020 - val_mean_absolute_error: 0.0314\n",
      "Epoch 275/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 0.0011 - mean_absolute_error: 0.0210 - val_loss: 0.0020 - val_mean_absolute_error: 0.0313\n",
      "Epoch 276/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0010 - mean_absolute_error: 0.0208 - val_loss: 0.0019 - val_mean_absolute_error: 0.0311\n",
      "Epoch 277/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0010 - mean_absolute_error: 0.0207 - val_loss: 0.0019 - val_mean_absolute_error: 0.0309\n",
      "Epoch 278/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0010 - mean_absolute_error: 0.0205 - val_loss: 0.0019 - val_mean_absolute_error: 0.0308\n",
      "Epoch 279/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 9.8803e-04 - mean_absolute_error: 0.0203 - val_loss: 0.0019 - val_mean_absolute_error: 0.0306\n",
      "Epoch 280/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.7039e-04 - mean_absolute_error: 0.0202 - val_loss: 0.0019 - val_mean_absolute_error: 0.0305\n",
      "Epoch 281/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 9.5336e-04 - mean_absolute_error: 0.0200 - val_loss: 0.0018 - val_mean_absolute_error: 0.0303\n",
      "Epoch 282/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 9.3693e-04 - mean_absolute_error: 0.0199 - val_loss: 0.0018 - val_mean_absolute_error: 0.0302\n",
      "Epoch 283/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 9.2107e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0018 - val_mean_absolute_error: 0.0301\n",
      "Epoch 284/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 9.0577e-04 - mean_absolute_error: 0.0196 - val_loss: 0.0018 - val_mean_absolute_error: 0.0299\n",
      "Epoch 285/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.9100e-04 - mean_absolute_error: 0.0195 - val_loss: 0.0018 - val_mean_absolute_error: 0.0298\n",
      "Epoch 286/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 8.7674e-04 - mean_absolute_error: 0.0194 - val_loss: 0.0018 - val_mean_absolute_error: 0.0297\n",
      "Epoch 287/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 8.6301e-04 - mean_absolute_error: 0.0192 - val_loss: 0.0017 - val_mean_absolute_error: 0.0296\n",
      "Epoch 288/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 8.4979e-04 - mean_absolute_error: 0.0191 - val_loss: 0.0017 - val_mean_absolute_error: 0.0294\n",
      "Epoch 289/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 8.3707e-04 - mean_absolute_error: 0.0190 - val_loss: 0.0017 - val_mean_absolute_error: 0.0293\n",
      "Epoch 290/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 8.2479e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0017 - val_mean_absolute_error: 0.0292\n",
      "Epoch 291/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 8.1296e-04 - mean_absolute_error: 0.0187 - val_loss: 0.0017 - val_mean_absolute_error: 0.0290\n",
      "Epoch 292/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 8.0155e-04 - mean_absolute_error: 0.0186 - val_loss: 0.0017 - val_mean_absolute_error: 0.0289\n",
      "Epoch 293/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 7.9050e-04 - mean_absolute_error: 0.0185 - val_loss: 0.0016 - val_mean_absolute_error: 0.0288\n",
      "Epoch 294/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 7.7981e-04 - mean_absolute_error: 0.0184 - val_loss: 0.0016 - val_mean_absolute_error: 0.0287\n",
      "Epoch 295/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 7.6948e-04 - mean_absolute_error: 0.0183 - val_loss: 0.0016 - val_mean_absolute_error: 0.0285\n",
      "Epoch 296/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 7.5943e-04 - mean_absolute_error: 0.0182 - val_loss: 0.0016 - val_mean_absolute_error: 0.0284\n",
      "Epoch 297/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 7.4967e-04 - mean_absolute_error: 0.0181 - val_loss: 0.0016 - val_mean_absolute_error: 0.0283\n",
      "Epoch 298/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 7.4021e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0016 - val_mean_absolute_error: 0.0282\n",
      "Epoch 299/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 7.3100e-04 - mean_absolute_error: 0.0178 - val_loss: 0.0016 - val_mean_absolute_error: 0.0281\n",
      "Epoch 300/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 7.2203e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0016 - val_mean_absolute_error: 0.0280\n",
      "Epoch 301/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 7.1329e-04 - mean_absolute_error: 0.0176 - val_loss: 0.0015 - val_mean_absolute_error: 0.0279\n",
      "Epoch 302/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 7.0482e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0015 - val_mean_absolute_error: 0.0278\n",
      "Epoch 303/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 6.9655e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0015 - val_mean_absolute_error: 0.0277\n",
      "Epoch 304/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 6.8847e-04 - mean_absolute_error: 0.0174 - val_loss: 0.0015 - val_mean_absolute_error: 0.0276\n",
      "Epoch 305/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.8058e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0015 - val_mean_absolute_error: 0.0275\n",
      "Epoch 306/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 6.7286e-04 - mean_absolute_error: 0.0172 - val_loss: 0.0015 - val_mean_absolute_error: 0.0274\n",
      "Epoch 307/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 6.6532e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0015 - val_mean_absolute_error: 0.0273\n",
      "Epoch 308/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 6.5795e-04 - mean_absolute_error: 0.0170 - val_loss: 0.0015 - val_mean_absolute_error: 0.0272\n",
      "Epoch 309/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.5073e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0015 - val_mean_absolute_error: 0.0272\n",
      "Epoch 310/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.4368e-04 - mean_absolute_error: 0.0168 - val_loss: 0.0015 - val_mean_absolute_error: 0.0271\n",
      "Epoch 311/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 6.3680e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0014 - val_mean_absolute_error: 0.0270\n",
      "Epoch 312/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 6.3006e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0014 - val_mean_absolute_error: 0.0269\n",
      "Epoch 313/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 6.2346e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0014 - val_mean_absolute_error: 0.0268\n",
      "Epoch 314/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 6.1703e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0014 - val_mean_absolute_error: 0.0268\n",
      "Epoch 315/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 6.1077e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0014 - val_mean_absolute_error: 0.0267\n",
      "Epoch 316/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 6.0462e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0014 - val_mean_absolute_error: 0.0266\n",
      "Epoch 317/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 5.9856e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0014 - val_mean_absolute_error: 0.0266\n",
      "Epoch 318/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 5.9264e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0014 - val_mean_absolute_error: 0.0265\n",
      "Epoch 319/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 5.8686e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0014 - val_mean_absolute_error: 0.0264\n",
      "Epoch 320/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 5.8119e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0014 - val_mean_absolute_error: 0.0263\n",
      "Epoch 321/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 5.7560e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0014 - val_mean_absolute_error: 0.0263\n",
      "Epoch 322/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 5.7015e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 323/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 5.6481e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 324/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 5.5954e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 325/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 5.5435e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0013 - val_mean_absolute_error: 0.0260\n",
      "Epoch 326/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 5.4930e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0013 - val_mean_absolute_error: 0.0259\n",
      "Epoch 327/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 5.4435e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0013 - val_mean_absolute_error: 0.0259\n",
      "Epoch 328/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 5.3948e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0013 - val_mean_absolute_error: 0.0258\n",
      "Epoch 329/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 5.3470e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0013 - val_mean_absolute_error: 0.0257\n",
      "Epoch 330/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 5.2999e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0013 - val_mean_absolute_error: 0.0257\n",
      "Epoch 331/1000\n",
      "52/52 [==============================] - 0s 631us/step - loss: 5.2537e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0013 - val_mean_absolute_error: 0.0256\n",
      "Epoch 332/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 5.2085e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0013 - val_mean_absolute_error: 0.0256\n",
      "Epoch 333/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 5.1639e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0013 - val_mean_absolute_error: 0.0255\n",
      "Epoch 334/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 5.1200e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0013 - val_mean_absolute_error: 0.0254\n",
      "Epoch 335/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 5.0769e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0013 - val_mean_absolute_error: 0.0254\n",
      "Epoch 336/1000\n",
      "52/52 [==============================] - 0s 638us/step - loss: 5.0347e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0013 - val_mean_absolute_error: 0.0253\n",
      "Epoch 337/1000\n",
      "52/52 [==============================] - 0s 637us/step - loss: 4.9931e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0013 - val_mean_absolute_error: 0.0253\n",
      "Epoch 338/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 4.9521e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0012 - val_mean_absolute_error: 0.0252\n",
      "Epoch 339/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 4.9120e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0012 - val_mean_absolute_error: 0.0252\n",
      "Epoch 340/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 4.8725e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0012 - val_mean_absolute_error: 0.0251\n",
      "Epoch 341/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 4.8337e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0012 - val_mean_absolute_error: 0.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 4.7955e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0012 - val_mean_absolute_error: 0.0250\n",
      "Epoch 343/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 4.7579e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0012 - val_mean_absolute_error: 0.0249\n",
      "Epoch 344/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 4.7210e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0012 - val_mean_absolute_error: 0.0249\n",
      "Epoch 345/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 4.6846e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0012 - val_mean_absolute_error: 0.0248\n",
      "Epoch 346/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 4.6488e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0012 - val_mean_absolute_error: 0.0248\n",
      "Epoch 347/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 4.6135e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0012 - val_mean_absolute_error: 0.0247\n",
      "Epoch 348/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 4.5787e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0012 - val_mean_absolute_error: 0.0247\n",
      "Epoch 349/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 4.5447e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0012 - val_mean_absolute_error: 0.0246\n",
      "Epoch 350/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 4.5111e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0012 - val_mean_absolute_error: 0.0246\n",
      "Epoch 351/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 4.4779e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0012 - val_mean_absolute_error: 0.0245\n",
      "Epoch 352/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 4.4453e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0012 - val_mean_absolute_error: 0.0245\n",
      "Epoch 353/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 4.4131e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0012 - val_mean_absolute_error: 0.0244\n",
      "Epoch 354/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 4.3813e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0012 - val_mean_absolute_error: 0.0244\n",
      "Epoch 355/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 4.3501e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0012 - val_mean_absolute_error: 0.0243\n",
      "Epoch 356/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 4.3194e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0012 - val_mean_absolute_error: 0.0243\n",
      "Epoch 357/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 4.2891e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0012 - val_mean_absolute_error: 0.0243\n",
      "Epoch 358/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 4.2593e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0011 - val_mean_absolute_error: 0.0242\n",
      "Epoch 359/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 4.2298e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0011 - val_mean_absolute_error: 0.0242\n",
      "Epoch 360/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 4.2007e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0011 - val_mean_absolute_error: 0.0241\n",
      "Epoch 361/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 4.1720e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0011 - val_mean_absolute_error: 0.0241\n",
      "Epoch 362/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 4.1438e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0011 - val_mean_absolute_error: 0.0240\n",
      "Epoch 363/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 4.1158e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0011 - val_mean_absolute_error: 0.0240\n",
      "Epoch 364/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 4.0881e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0011 - val_mean_absolute_error: 0.0239\n",
      "Epoch 365/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 4.0610e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0011 - val_mean_absolute_error: 0.0239\n",
      "Epoch 366/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 4.0343e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0011 - val_mean_absolute_error: 0.0238\n",
      "Epoch 367/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 4.0075e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0011 - val_mean_absolute_error: 0.0238\n",
      "Epoch 368/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 3.9812e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0011 - val_mean_absolute_error: 0.0238\n",
      "Epoch 369/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 3.9556e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0011 - val_mean_absolute_error: 0.0237\n",
      "Epoch 370/1000\n",
      "52/52 [==============================] - 0s 636us/step - loss: 3.9302e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0011 - val_mean_absolute_error: 0.0237\n",
      "Epoch 371/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 3.9048e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0011 - val_mean_absolute_error: 0.0236\n",
      "Epoch 372/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 3.8799e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0011 - val_mean_absolute_error: 0.0236\n",
      "Epoch 373/1000\n",
      "52/52 [==============================] - 0s 629us/step - loss: 3.8555e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0011 - val_mean_absolute_error: 0.0236\n",
      "Epoch 374/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 3.8312e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0011 - val_mean_absolute_error: 0.0235\n",
      "Epoch 375/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 3.8072e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0011 - val_mean_absolute_error: 0.0235\n",
      "Epoch 376/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 3.7836e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0011 - val_mean_absolute_error: 0.0234\n",
      "Epoch 377/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 3.7601e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0011 - val_mean_absolute_error: 0.0234\n",
      "Epoch 378/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 3.7369e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0011 - val_mean_absolute_error: 0.0234\n",
      "Epoch 379/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 3.7140e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0011 - val_mean_absolute_error: 0.0233\n",
      "Epoch 380/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 3.6915e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0233\n",
      "Epoch 381/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 3.6692e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0232\n",
      "Epoch 382/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.6470e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0232\n",
      "Epoch 383/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 3.6253e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0232\n",
      "Epoch 384/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 3.6036e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0231\n",
      "Epoch 385/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 3.5822e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0231\n",
      "Epoch 386/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 3.5611e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0010 - val_mean_absolute_error: 0.0230\n",
      "Epoch 387/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 3.5403e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0010 - val_mean_absolute_error: 0.0230\n",
      "Epoch 388/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 3.5198e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0010 - val_mean_absolute_error: 0.0230\n",
      "Epoch 389/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 3.4993e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0010 - val_mean_absolute_error: 0.0229\n",
      "Epoch 390/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 3.4790e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0010 - val_mean_absolute_error: 0.0229\n",
      "Epoch 391/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 3.4591e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0010 - val_mean_absolute_error: 0.0229\n",
      "Epoch 392/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 3.4395e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0010 - val_mean_absolute_error: 0.0228\n",
      "Epoch 393/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 3.4198e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0010 - val_mean_absolute_error: 0.0228\n",
      "Epoch 394/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 3.4005e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0010 - val_mean_absolute_error: 0.0228\n",
      "Epoch 395/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.3814e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0010 - val_mean_absolute_error: 0.0227\n",
      "Epoch 396/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 3.3625e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0010 - val_mean_absolute_error: 0.0227\n",
      "Epoch 397/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 3.3437e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0010 - val_mean_absolute_error: 0.0227\n",
      "Epoch 398/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 3.3250e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0010 - val_mean_absolute_error: 0.0226\n",
      "Epoch 399/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.3068e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0010 - val_mean_absolute_error: 0.0226\n",
      "Epoch 400/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 3.2887e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0010 - val_mean_absolute_error: 0.0226\n",
      "Epoch 401/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 3.2707e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0010 - val_mean_absolute_error: 0.0225\n",
      "Epoch 402/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.2528e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0010 - val_mean_absolute_error: 0.0225\n",
      "Epoch 403/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 3.2353e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0010 - val_mean_absolute_error: 0.0225\n",
      "Epoch 404/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 3.2179e-04 - mean_absolute_error: 0.0114 - val_loss: 9.9784e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 405/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 3.2005e-04 - mean_absolute_error: 0.0114 - val_loss: 9.9544e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 406/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 3.1834e-04 - mean_absolute_error: 0.0113 - val_loss: 9.9241e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 407/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 3.1666e-04 - mean_absolute_error: 0.0113 - val_loss: 9.9033e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 408/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 3.1499e-04 - mean_absolute_error: 0.0113 - val_loss: 9.8827e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 409/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 3.1331e-04 - mean_absolute_error: 0.0112 - val_loss: 9.8534e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 410/1000\n",
      "52/52 [==============================] - 0s 634us/step - loss: 3.1167e-04 - mean_absolute_error: 0.0112 - val_loss: 9.8358e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 411/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 3.1005e-04 - mean_absolute_error: 0.0112 - val_loss: 9.8138e-04 - val_mean_absolute_error: 0.0222\n",
      "Epoch 412/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 3.0842e-04 - mean_absolute_error: 0.0111 - val_loss: 9.7821e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 413/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 3.0682e-04 - mean_absolute_error: 0.0111 - val_loss: 9.7665e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 414/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 3.0524e-04 - mean_absolute_error: 0.0111 - val_loss: 9.7461e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 415/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 3.0366e-04 - mean_absolute_error: 0.0110 - val_loss: 9.7157e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 416/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 3.0210e-04 - mean_absolute_error: 0.0110 - val_loss: 9.6968e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 417/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 3.0057e-04 - mean_absolute_error: 0.0110 - val_loss: 9.6747e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 418/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.9904e-04 - mean_absolute_error: 0.0109 - val_loss: 9.6546e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 419/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.9751e-04 - mean_absolute_error: 0.0109 - val_loss: 9.6358e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 420/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.9601e-04 - mean_absolute_error: 0.0109 - val_loss: 9.6094e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 421/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 2.9454e-04 - mean_absolute_error: 0.0108 - val_loss: 9.5928e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 422/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 2.9306e-04 - mean_absolute_error: 0.0108 - val_loss: 9.5734e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 423/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 2.9160e-04 - mean_absolute_error: 0.0108 - val_loss: 9.5459e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 424/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.9015e-04 - mean_absolute_error: 0.0107 - val_loss: 9.5317e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 425/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.8872e-04 - mean_absolute_error: 0.0107 - val_loss: 9.5095e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 426/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.8730e-04 - mean_absolute_error: 0.0107 - val_loss: 9.4827e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 427/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.8589e-04 - mean_absolute_error: 0.0106 - val_loss: 9.4682e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 428/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 2.8450e-04 - mean_absolute_error: 0.0106 - val_loss: 9.4514e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 429/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.8311e-04 - mean_absolute_error: 0.0106 - val_loss: 9.4277e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 430/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.8173e-04 - mean_absolute_error: 0.0105 - val_loss: 9.4058e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 431/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 2.8037e-04 - mean_absolute_error: 0.0105 - val_loss: 9.3863e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 432/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.7903e-04 - mean_absolute_error: 0.0105 - val_loss: 9.3663e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 433/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.7769e-04 - mean_absolute_error: 0.0105 - val_loss: 9.3532e-04 - val_mean_absolute_error: 0.0216\n",
      "Epoch 434/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 2.7636e-04 - mean_absolute_error: 0.0104 - val_loss: 9.3312e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 435/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.7505e-04 - mean_absolute_error: 0.0104 - val_loss: 9.3107e-04 - val_mean_absolute_error: 0.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.7374e-04 - mean_absolute_error: 0.0104 - val_loss: 9.2942e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 437/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.7245e-04 - mean_absolute_error: 0.0104 - val_loss: 9.2766e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 438/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.7116e-04 - mean_absolute_error: 0.0103 - val_loss: 9.2527e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 439/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 2.6989e-04 - mean_absolute_error: 0.0103 - val_loss: 9.2349e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 440/1000\n",
      "52/52 [==============================] - 0s 638us/step - loss: 2.6864e-04 - mean_absolute_error: 0.0103 - val_loss: 9.2181e-04 - val_mean_absolute_error: 0.0214\n",
      "Epoch 441/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.6739e-04 - mean_absolute_error: 0.0103 - val_loss: 9.2043e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 442/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.6613e-04 - mean_absolute_error: 0.0102 - val_loss: 9.1845e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 443/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.6490e-04 - mean_absolute_error: 0.0102 - val_loss: 9.1620e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 444/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 2.6369e-04 - mean_absolute_error: 0.0102 - val_loss: 9.1477e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 445/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 2.6249e-04 - mean_absolute_error: 0.0102 - val_loss: 9.1310e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 446/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.6128e-04 - mean_absolute_error: 0.0101 - val_loss: 9.1137e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 447/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.6009e-04 - mean_absolute_error: 0.0101 - val_loss: 9.0975e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 448/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.5891e-04 - mean_absolute_error: 0.0101 - val_loss: 9.0820e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 449/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.5773e-04 - mean_absolute_error: 0.0101 - val_loss: 9.0632e-04 - val_mean_absolute_error: 0.0212\n",
      "Epoch 450/1000\n",
      "52/52 [==============================] - 0s 635us/step - loss: 2.5657e-04 - mean_absolute_error: 0.0100 - val_loss: 9.0494e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 451/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 2.5542e-04 - mean_absolute_error: 0.0100 - val_loss: 9.0301e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 452/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 2.5428e-04 - mean_absolute_error: 0.0100 - val_loss: 9.0095e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 453/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 2.5314e-04 - mean_absolute_error: 0.0100 - val_loss: 8.9975e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 454/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 2.5201e-04 - mean_absolute_error: 0.0099 - val_loss: 8.9770e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 455/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 2.5089e-04 - mean_absolute_error: 0.0099 - val_loss: 8.9608e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 456/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 2.4979e-04 - mean_absolute_error: 0.0099 - val_loss: 8.9487e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 457/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.4869e-04 - mean_absolute_error: 0.0099 - val_loss: 8.9306e-04 - val_mean_absolute_error: 0.0210\n",
      "Epoch 458/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 2.4760e-04 - mean_absolute_error: 0.0098 - val_loss: 8.9189e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 459/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.4651e-04 - mean_absolute_error: 0.0098 - val_loss: 8.9017e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 460/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.4544e-04 - mean_absolute_error: 0.0098 - val_loss: 8.8844e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 461/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 2.4438e-04 - mean_absolute_error: 0.0098 - val_loss: 8.8693e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 462/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.4332e-04 - mean_absolute_error: 0.0097 - val_loss: 8.8559e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 463/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 2.4227e-04 - mean_absolute_error: 0.0097 - val_loss: 8.8389e-04 - val_mean_absolute_error: 0.0208\n",
      "Epoch 464/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.4123e-04 - mean_absolute_error: 0.0097 - val_loss: 8.8273e-04 - val_mean_absolute_error: 0.0208\n",
      "Epoch 465/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.4019e-04 - mean_absolute_error: 0.0097 - val_loss: 8.8090e-04 - val_mean_absolute_error: 0.0208\n",
      "Epoch 466/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.3917e-04 - mean_absolute_error: 0.0096 - val_loss: 8.7916e-04 - val_mean_absolute_error: 0.0208\n",
      "Epoch 467/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.3816e-04 - mean_absolute_error: 0.0096 - val_loss: 8.7829e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 468/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 2.3715e-04 - mean_absolute_error: 0.0096 - val_loss: 8.7664e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 469/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.3615e-04 - mean_absolute_error: 0.0096 - val_loss: 8.7532e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 470/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 2.3516e-04 - mean_absolute_error: 0.0096 - val_loss: 8.7404e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 471/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.3418e-04 - mean_absolute_error: 0.0095 - val_loss: 8.7229e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 472/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 2.3320e-04 - mean_absolute_error: 0.0095 - val_loss: 8.7112e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 473/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 2.3223e-04 - mean_absolute_error: 0.0095 - val_loss: 8.7006e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 474/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.3126e-04 - mean_absolute_error: 0.0095 - val_loss: 8.6814e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 475/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 2.3031e-04 - mean_absolute_error: 0.0094 - val_loss: 8.6714e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 476/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.2936e-04 - mean_absolute_error: 0.0094 - val_loss: 8.6594e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 477/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.2841e-04 - mean_absolute_error: 0.0094 - val_loss: 8.6438e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 478/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.2747e-04 - mean_absolute_error: 0.0094 - val_loss: 8.6314e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 479/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 2.2655e-04 - mean_absolute_error: 0.0094 - val_loss: 8.6160e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 480/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.2564e-04 - mean_absolute_error: 0.0093 - val_loss: 8.6046e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 481/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 2.2473e-04 - mean_absolute_error: 0.0093 - val_loss: 8.5939e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 482/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 2.2382e-04 - mean_absolute_error: 0.0093 - val_loss: 8.5790e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 483/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 2.2292e-04 - mean_absolute_error: 0.0093 - val_loss: 8.5699e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 484/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 2.2204e-04 - mean_absolute_error: 0.0093 - val_loss: 8.5558e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 485/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 2.2115e-04 - mean_absolute_error: 0.0092 - val_loss: 8.5430e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 486/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.2028e-04 - mean_absolute_error: 0.0092 - val_loss: 8.5336e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 487/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 2.1941e-04 - mean_absolute_error: 0.0092 - val_loss: 8.5196e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 488/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 2.1854e-04 - mean_absolute_error: 0.0092 - val_loss: 8.5088e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 489/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 2.1769e-04 - mean_absolute_error: 0.0092 - val_loss: 8.4959e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 490/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.1684e-04 - mean_absolute_error: 0.0091 - val_loss: 8.4833e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 491/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 2.1600e-04 - mean_absolute_error: 0.0091 - val_loss: 8.4710e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 492/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 2.1516e-04 - mean_absolute_error: 0.0091 - val_loss: 8.4621e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 493/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 2.1432e-04 - mean_absolute_error: 0.0091 - val_loss: 8.4511e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 494/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 2.1349e-04 - mean_absolute_error: 0.0091 - val_loss: 8.4375e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 495/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 2.1267e-04 - mean_absolute_error: 0.0090 - val_loss: 8.4277e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 496/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 2.1186e-04 - mean_absolute_error: 0.0090 - val_loss: 8.4132e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 497/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.1104e-04 - mean_absolute_error: 0.0090 - val_loss: 8.4005e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 498/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 2.1023e-04 - mean_absolute_error: 0.0090 - val_loss: 8.3884e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 499/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 2.0944e-04 - mean_absolute_error: 0.0090 - val_loss: 8.3790e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 500/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 2.0864e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3685e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 501/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 2.0785e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3539e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 502/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.0706e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3472e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 503/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 2.0628e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3312e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 504/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 2.0550e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3194e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 505/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 2.0473e-04 - mean_absolute_error: 0.0089 - val_loss: 8.3077e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 506/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.0397e-04 - mean_absolute_error: 0.0088 - val_loss: 8.2963e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 507/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 2.0321e-04 - mean_absolute_error: 0.0088 - val_loss: 8.2907e-04 - val_mean_absolute_error: 0.0201\n",
      "Epoch 508/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.0245e-04 - mean_absolute_error: 0.0088 - val_loss: 8.2733e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 509/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 2.0170e-04 - mean_absolute_error: 0.0088 - val_loss: 8.2642e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 510/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 2.0097e-04 - mean_absolute_error: 0.0088 - val_loss: 8.2525e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 511/1000\n",
      "52/52 [==============================] - 0s 652us/step - loss: 2.0022e-04 - mean_absolute_error: 0.0087 - val_loss: 8.2428e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 512/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.9948e-04 - mean_absolute_error: 0.0087 - val_loss: 8.2289e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 513/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.9875e-04 - mean_absolute_error: 0.0087 - val_loss: 8.2179e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 514/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.9803e-04 - mean_absolute_error: 0.0087 - val_loss: 8.2065e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 515/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.9731e-04 - mean_absolute_error: 0.0087 - val_loss: 8.1950e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 516/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.9659e-04 - mean_absolute_error: 0.0087 - val_loss: 8.1865e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 517/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.9587e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1731e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 518/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.9517e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1643e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 519/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.9446e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1525e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 520/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.9376e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1427e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 521/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.9306e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1308e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 522/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.9237e-04 - mean_absolute_error: 0.0086 - val_loss: 8.1194e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 523/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.9168e-04 - mean_absolute_error: 0.0085 - val_loss: 8.1101e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 524/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.9100e-04 - mean_absolute_error: 0.0085 - val_loss: 8.0978e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 525/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.9031e-04 - mean_absolute_error: 0.0085 - val_loss: 8.0877e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 526/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.8964e-04 - mean_absolute_error: 0.0085 - val_loss: 8.0780e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 527/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.8896e-04 - mean_absolute_error: 0.0085 - val_loss: 8.0656e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 528/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 629us/step - loss: 1.8829e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0541e-04 - val_mean_absolute_error: 0.0197\n",
      "Epoch 529/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.8763e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0450e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 530/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.8696e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0331e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 531/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.8631e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0262e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 532/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.8564e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0117e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 533/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.8499e-04 - mean_absolute_error: 0.0084 - val_loss: 8.0029e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 534/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.8435e-04 - mean_absolute_error: 0.0084 - val_loss: 7.9940e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 535/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.8371e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9819e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 536/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.8307e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9743e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 537/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 1.8243e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9609e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 538/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.8180e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9527e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 539/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.8116e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9408e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 540/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.8054e-04 - mean_absolute_error: 0.0083 - val_loss: 7.9327e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 541/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.7992e-04 - mean_absolute_error: 0.0082 - val_loss: 7.9258e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 542/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 1.7929e-04 - mean_absolute_error: 0.0082 - val_loss: 7.9112e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 543/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.7868e-04 - mean_absolute_error: 0.0082 - val_loss: 7.9013e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 544/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.7807e-04 - mean_absolute_error: 0.0082 - val_loss: 7.8912e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 545/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.7746e-04 - mean_absolute_error: 0.0082 - val_loss: 7.8795e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 546/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.7685e-04 - mean_absolute_error: 0.0082 - val_loss: 7.8690e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 547/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.7624e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8597e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 548/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 1.7565e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8501e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 549/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.7505e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8421e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 550/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.7445e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8314e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 551/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.7386e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8216e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 552/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 1.7327e-04 - mean_absolute_error: 0.0081 - val_loss: 7.8113e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 553/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 1.7269e-04 - mean_absolute_error: 0.0080 - val_loss: 7.8010e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 554/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.7211e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7946e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 555/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.7152e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7811e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 556/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.7095e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7727e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 557/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.7038e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7631e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 558/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.6980e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7527e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 559/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.6924e-04 - mean_absolute_error: 0.0080 - val_loss: 7.7451e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 560/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.6867e-04 - mean_absolute_error: 0.0079 - val_loss: 7.7353e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 561/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.6810e-04 - mean_absolute_error: 0.0079 - val_loss: 7.7243e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 562/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.6755e-04 - mean_absolute_error: 0.0079 - val_loss: 7.7163e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 563/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.6699e-04 - mean_absolute_error: 0.0079 - val_loss: 7.7086e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 564/1000\n",
      "52/52 [==============================] - 0s 976us/step - loss: 1.6643e-04 - mean_absolute_error: 0.0079 - val_loss: 7.6980e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 565/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 1.6587e-04 - mean_absolute_error: 0.0079 - val_loss: 7.6876e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 566/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.6533e-04 - mean_absolute_error: 0.0079 - val_loss: 7.6782e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 567/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.6479e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6692e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 568/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 1.6424e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6600e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 569/1000\n",
      "52/52 [==============================] - 0s 638us/step - loss: 1.6369e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6494e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 570/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.6316e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6410e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 571/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.6262e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6317e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 572/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.6208e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6211e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 573/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.6155e-04 - mean_absolute_error: 0.0078 - val_loss: 7.6123e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 574/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.6102e-04 - mean_absolute_error: 0.0077 - val_loss: 7.6015e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 575/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.6049e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5932e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 576/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.5997e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5847e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 577/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 1.5945e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5768e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 578/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.5893e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5662e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 579/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.5841e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5561e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 580/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.5790e-04 - mean_absolute_error: 0.0077 - val_loss: 7.5466e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 581/1000\n",
      "52/52 [==============================] - 0s 639us/step - loss: 1.5739e-04 - mean_absolute_error: 0.0076 - val_loss: 7.5382e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 582/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 1.5688e-04 - mean_absolute_error: 0.0076 - val_loss: 7.5292e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 583/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.5638e-04 - mean_absolute_error: 0.0076 - val_loss: 7.5212e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 584/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.5587e-04 - mean_absolute_error: 0.0076 - val_loss: 7.5097e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 585/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.5537e-04 - mean_absolute_error: 0.0076 - val_loss: 7.5019e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 586/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.5488e-04 - mean_absolute_error: 0.0076 - val_loss: 7.4939e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 587/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.5437e-04 - mean_absolute_error: 0.0076 - val_loss: 7.4824e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 588/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 1.5388e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4751e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 589/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.5339e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4667e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 590/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.5290e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4595e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 591/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.5241e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4490e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 592/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.5193e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4405e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 593/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.5145e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4324e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 594/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.5096e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4223e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 595/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.5048e-04 - mean_absolute_error: 0.0075 - val_loss: 7.4150e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 596/1000\n",
      "52/52 [==============================] - 0s 643us/step - loss: 1.5001e-04 - mean_absolute_error: 0.0074 - val_loss: 7.4063e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 597/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.4954e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3988e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 598/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 1.4906e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3894e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 599/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.4860e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3808e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 600/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.4813e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3737e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 601/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.4766e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3643e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 602/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.4719e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3561e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 603/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.4674e-04 - mean_absolute_error: 0.0074 - val_loss: 7.3489e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 604/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.4628e-04 - mean_absolute_error: 0.0073 - val_loss: 7.3392e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 605/1000\n",
      "52/52 [==============================] - 0s 639us/step - loss: 1.4582e-04 - mean_absolute_error: 0.0073 - val_loss: 7.3315e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 606/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4537e-04 - mean_absolute_error: 0.0073 - val_loss: 7.3255e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 607/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.4491e-04 - mean_absolute_error: 0.0073 - val_loss: 7.3157e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 608/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.4446e-04 - mean_absolute_error: 0.0073 - val_loss: 7.3077e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 609/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.4401e-04 - mean_absolute_error: 0.0073 - val_loss: 7.2987e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 610/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.4357e-04 - mean_absolute_error: 0.0073 - val_loss: 7.2930e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 611/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.4312e-04 - mean_absolute_error: 0.0073 - val_loss: 7.2834e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 612/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.4268e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2755e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 613/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.4224e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2695e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 614/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.4179e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2584e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 615/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4136e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2518e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 616/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4093e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2453e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 617/1000\n",
      "52/52 [==============================] - 0s 643us/step - loss: 1.4049e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2384e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 618/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4005e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2290e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 619/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.3963e-04 - mean_absolute_error: 0.0072 - val_loss: 7.2217e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 620/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 598us/step - loss: 1.3920e-04 - mean_absolute_error: 0.0071 - val_loss: 7.2145e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 621/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.3877e-04 - mean_absolute_error: 0.0071 - val_loss: 7.2055e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 622/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.3835e-04 - mean_absolute_error: 0.0071 - val_loss: 7.2001e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 623/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 1.3792e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1898e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 624/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.3751e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1843e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 625/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.3709e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1753e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 626/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.3667e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1682e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 627/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.3625e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1595e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 628/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.3584e-04 - mean_absolute_error: 0.0071 - val_loss: 7.1543e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 629/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.3543e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1462e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 630/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 1.3502e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1409e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 631/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.3461e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1309e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 632/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.3421e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1256e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 633/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.3380e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1183e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 634/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.3340e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1134e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 635/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.3299e-04 - mean_absolute_error: 0.0070 - val_loss: 7.1010e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 636/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.3259e-04 - mean_absolute_error: 0.0070 - val_loss: 7.0972e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 637/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 1.3220e-04 - mean_absolute_error: 0.0070 - val_loss: 7.0854e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 638/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.3180e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0856e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 639/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.3140e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0733e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 640/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.3101e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0704e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 641/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.3061e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0575e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 642/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.3023e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0539e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 643/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.2984e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0436e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 644/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.2945e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0411e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 645/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.2906e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0316e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 646/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.2868e-04 - mean_absolute_error: 0.0069 - val_loss: 7.0265e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 647/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.2830e-04 - mean_absolute_error: 0.0068 - val_loss: 7.0150e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 648/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.2792e-04 - mean_absolute_error: 0.0068 - val_loss: 7.0146e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 649/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.2754e-04 - mean_absolute_error: 0.0068 - val_loss: 7.0019e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 650/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 1.2716e-04 - mean_absolute_error: 0.0068 - val_loss: 7.0025e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 651/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 1.2678e-04 - mean_absolute_error: 0.0068 - val_loss: 6.9849e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 652/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.2641e-04 - mean_absolute_error: 0.0068 - val_loss: 6.9894e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 653/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.2603e-04 - mean_absolute_error: 0.0068 - val_loss: 6.9699e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 654/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.2566e-04 - mean_absolute_error: 0.0068 - val_loss: 6.9790e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 655/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.2529e-04 - mean_absolute_error: 0.0068 - val_loss: 6.9582e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 656/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.2492e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9675e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 657/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 1.2454e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9404e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 658/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.2419e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9551e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 659/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.2382e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9250e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 660/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.2346e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9473e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 661/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 1.2309e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9100e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 662/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.2274e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9387e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 663/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 1.2237e-04 - mean_absolute_error: 0.0067 - val_loss: 6.8929e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 664/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.2203e-04 - mean_absolute_error: 0.0067 - val_loss: 6.9280e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 665/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 1.2166e-04 - mean_absolute_error: 0.0066 - val_loss: 6.8748e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 666/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.2132e-04 - mean_absolute_error: 0.0066 - val_loss: 6.9205e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 667/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 1.2095e-04 - mean_absolute_error: 0.0066 - val_loss: 6.8545e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 668/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.2062e-04 - mean_absolute_error: 0.0066 - val_loss: 6.9150e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 669/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.2025e-04 - mean_absolute_error: 0.0066 - val_loss: 6.8328e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 670/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 1.1992e-04 - mean_absolute_error: 0.0066 - val_loss: 6.9069e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 671/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.1955e-04 - mean_absolute_error: 0.0066 - val_loss: 6.8059e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 672/1000\n",
      "52/52 [==============================] - 0s 638us/step - loss: 1.1924e-04 - mean_absolute_error: 0.0066 - val_loss: 6.9049e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 673/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.1885e-04 - mean_absolute_error: 0.0066 - val_loss: 6.7787e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 674/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 1.1856e-04 - mean_absolute_error: 0.0066 - val_loss: 6.9066e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 675/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.1817e-04 - mean_absolute_error: 0.0066 - val_loss: 6.7481e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 676/1000\n",
      "52/52 [==============================] - 0s 642us/step - loss: 1.1790e-04 - mean_absolute_error: 0.0065 - val_loss: 6.9091e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 677/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.1751e-04 - mean_absolute_error: 0.0065 - val_loss: 6.7124e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 678/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.1727e-04 - mean_absolute_error: 0.0065 - val_loss: 6.9201e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 679/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.1688e-04 - mean_absolute_error: 0.0065 - val_loss: 6.6699e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 680/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.1667e-04 - mean_absolute_error: 0.0065 - val_loss: 6.9372e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 681/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.1628e-04 - mean_absolute_error: 0.0065 - val_loss: 6.6206e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 682/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.1612e-04 - mean_absolute_error: 0.0065 - val_loss: 6.9613e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 683/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 1.1573e-04 - mean_absolute_error: 0.0065 - val_loss: 6.5640e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 684/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.1564e-04 - mean_absolute_error: 0.0065 - val_loss: 6.9963e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 685/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 1.1526e-04 - mean_absolute_error: 0.0065 - val_loss: 6.4964e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 686/1000\n",
      "52/52 [==============================] - 0s 638us/step - loss: 1.1527e-04 - mean_absolute_error: 0.0065 - val_loss: 7.0462e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 687/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.1492e-04 - mean_absolute_error: 0.0065 - val_loss: 6.4170e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 688/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.1506e-04 - mean_absolute_error: 0.0065 - val_loss: 7.1085e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 689/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 1.1477e-04 - mean_absolute_error: 0.0065 - val_loss: 6.3218e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 690/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.1512e-04 - mean_absolute_error: 0.0065 - val_loss: 7.1964e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 691/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.1492e-04 - mean_absolute_error: 0.0066 - val_loss: 6.2105e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 692/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.1560e-04 - mean_absolute_error: 0.0066 - val_loss: 7.3069e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 693/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.1556e-04 - mean_absolute_error: 0.0066 - val_loss: 6.0770e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 694/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.1675e-04 - mean_absolute_error: 0.0066 - val_loss: 7.4440e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 695/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 1.1698e-04 - mean_absolute_error: 0.0068 - val_loss: 5.9231e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 696/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.1895e-04 - mean_absolute_error: 0.0068 - val_loss: 7.6090e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 697/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.1961e-04 - mean_absolute_error: 0.0070 - val_loss: 5.7522e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 698/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.2281e-04 - mean_absolute_error: 0.0070 - val_loss: 7.7802e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 699/1000\n",
      "52/52 [==============================] - 0s 635us/step - loss: 1.2413e-04 - mean_absolute_error: 0.0073 - val_loss: 5.5882e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 700/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.2902e-04 - mean_absolute_error: 0.0074 - val_loss: 7.8999e-04 - val_mean_absolute_error: 0.0188\n",
      "Epoch 701/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.3078e-04 - mean_absolute_error: 0.0077 - val_loss: 5.4843e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 702/1000\n",
      "52/52 [==============================] - 0s 652us/step - loss: 1.3714e-04 - mean_absolute_error: 0.0078 - val_loss: 7.8144e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 703/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.3766e-04 - mean_absolute_error: 0.0080 - val_loss: 5.5608e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 704/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.4279e-04 - mean_absolute_error: 0.0080 - val_loss: 7.3606e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 705/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.3886e-04 - mean_absolute_error: 0.0080 - val_loss: 5.9580e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 706/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.3888e-04 - mean_absolute_error: 0.0079 - val_loss: 6.6317e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 707/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.2998e-04 - mean_absolute_error: 0.0076 - val_loss: 6.6615e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 708/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.2607e-04 - mean_absolute_error: 0.0074 - val_loss: 6.0164e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 709/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 1.1842e-04 - mean_absolute_error: 0.0069 - val_loss: 7.3010e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 710/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.1503e-04 - mean_absolute_error: 0.0069 - val_loss: 5.7711e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 711/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.1247e-04 - mean_absolute_error: 0.0064 - val_loss: 7.4432e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 712/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 621us/step - loss: 1.1065e-04 - mean_absolute_error: 0.0065 - val_loss: 5.8330e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 713/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.1093e-04 - mean_absolute_error: 0.0064 - val_loss: 7.1256e-04 - val_mean_absolute_error: 0.0179\n",
      "Epoch 714/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.1011e-04 - mean_absolute_error: 0.0065 - val_loss: 6.0724e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 715/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.1031e-04 - mean_absolute_error: 0.0065 - val_loss: 6.6835e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 716/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.0889e-04 - mean_absolute_error: 0.0065 - val_loss: 6.4105e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 717/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.0849e-04 - mean_absolute_error: 0.0065 - val_loss: 6.3601e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 718/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.0685e-04 - mean_absolute_error: 0.0063 - val_loss: 6.7004e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 719/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.0615e-04 - mean_absolute_error: 0.0063 - val_loss: 6.2054e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 720/1000\n",
      "52/52 [==============================] - 0s 643us/step - loss: 1.0531e-04 - mean_absolute_error: 0.0061 - val_loss: 6.7949e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 721/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.0474e-04 - mean_absolute_error: 0.0062 - val_loss: 6.1773e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 722/1000\n",
      "52/52 [==============================] - 0s 648us/step - loss: 1.0446e-04 - mean_absolute_error: 0.0061 - val_loss: 6.7140e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 723/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.0410e-04 - mean_absolute_error: 0.0062 - val_loss: 6.2483e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 724/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.0397e-04 - mean_absolute_error: 0.0061 - val_loss: 6.5772e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 725/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.0354e-04 - mean_absolute_error: 0.0061 - val_loss: 6.3756e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 726/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 1.0335e-04 - mean_absolute_error: 0.0061 - val_loss: 6.4548e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 727/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.0285e-04 - mean_absolute_error: 0.0061 - val_loss: 6.4805e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 728/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.0260e-04 - mean_absolute_error: 0.0061 - val_loss: 6.3687e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 729/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 1.0215e-04 - mean_absolute_error: 0.0060 - val_loss: 6.5283e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 730/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.0190e-04 - mean_absolute_error: 0.0061 - val_loss: 6.3293e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 731/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 1.0157e-04 - mean_absolute_error: 0.0060 - val_loss: 6.5273e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 732/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.0130e-04 - mean_absolute_error: 0.0060 - val_loss: 6.3339e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 733/1000\n",
      "52/52 [==============================] - 0s 635us/step - loss: 1.0105e-04 - mean_absolute_error: 0.0060 - val_loss: 6.4953e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 734/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.0077e-04 - mean_absolute_error: 0.0060 - val_loss: 6.3572e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 735/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.0056e-04 - mean_absolute_error: 0.0060 - val_loss: 6.4484e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 736/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 1.0026e-04 - mean_absolute_error: 0.0060 - val_loss: 6.3833e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 737/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.0005e-04 - mean_absolute_error: 0.0060 - val_loss: 6.4065e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 738/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 9.9743e-05 - mean_absolute_error: 0.0060 - val_loss: 6.4031e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 739/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 9.9521e-05 - mean_absolute_error: 0.0060 - val_loss: 6.3750e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 740/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 9.9232e-05 - mean_absolute_error: 0.0059 - val_loss: 6.4103e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 741/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 9.8998e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3559e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 742/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 9.8733e-05 - mean_absolute_error: 0.0059 - val_loss: 6.4068e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 743/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 9.8493e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3482e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 744/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 9.8241e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3954e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 745/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 9.7996e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3448e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 746/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 9.7763e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3799e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 747/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.7510e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3445e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 748/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 9.7283e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3640e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 749/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 9.7033e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3435e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 750/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 9.6807e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3485e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 751/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 9.6558e-05 - mean_absolute_error: 0.0059 - val_loss: 6.3418e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 752/1000\n",
      "52/52 [==============================] - 0s 654us/step - loss: 9.6333e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3356e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 753/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 9.6087e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3355e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 754/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 9.5861e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3242e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 755/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 9.5625e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3304e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 756/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 9.5395e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3156e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 757/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.5167e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3221e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 758/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 9.4938e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3077e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 759/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 9.4709e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3124e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 760/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 9.4487e-05 - mean_absolute_error: 0.0058 - val_loss: 6.2999e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 761/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 9.4261e-05 - mean_absolute_error: 0.0058 - val_loss: 6.3032e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 762/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 9.4032e-05 - mean_absolute_error: 0.0058 - val_loss: 6.2912e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 763/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 9.3816e-05 - mean_absolute_error: 0.0058 - val_loss: 6.2937e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 764/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 9.3593e-05 - mean_absolute_error: 0.0058 - val_loss: 6.2847e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 765/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 9.3369e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2841e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 766/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 9.3154e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2764e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 767/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 9.2934e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2740e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 768/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.2711e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2669e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 769/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 9.2502e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2653e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 770/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 9.2285e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2604e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 771/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.2069e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2574e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 772/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 9.1854e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2523e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 773/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 9.1647e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2488e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 774/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 9.1427e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2430e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 775/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 9.1224e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2400e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 776/1000\n",
      "52/52 [==============================] - 0s 634us/step - loss: 9.1010e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2357e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 777/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 9.0802e-05 - mean_absolute_error: 0.0057 - val_loss: 6.2319e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 778/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 9.0596e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2278e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 779/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 9.0387e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2232e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 780/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 9.0183e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2188e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 781/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 8.9982e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2157e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 782/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.9778e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2123e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 783/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 8.9572e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2078e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 784/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 8.9373e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2037e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 785/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 8.9174e-05 - mean_absolute_error: 0.0056 - val_loss: 6.2007e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 786/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 8.8970e-05 - mean_absolute_error: 0.0056 - val_loss: 6.1959e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 787/1000\n",
      "52/52 [==============================] - 0s 657us/step - loss: 8.8777e-05 - mean_absolute_error: 0.0056 - val_loss: 6.1920e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 788/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 8.8579e-05 - mean_absolute_error: 0.0056 - val_loss: 6.1886e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 789/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 8.8379e-05 - mean_absolute_error: 0.0056 - val_loss: 6.1857e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 790/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 8.8184e-05 - mean_absolute_error: 0.0056 - val_loss: 6.1803e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 791/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.7993e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1784e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 792/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 8.7794e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1729e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 793/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 8.7602e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1707e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 794/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 8.7413e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1661e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 795/1000\n",
      "52/52 [==============================] - 0s 655us/step - loss: 8.7218e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1630e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 796/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.7028e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1575e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 797/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 8.6841e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1567e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 798/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 8.6646e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1498e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 799/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 8.6460e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1484e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 800/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 8.6275e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1431e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 801/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 8.6085e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1423e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 802/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.5898e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1374e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 803/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.5713e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1359e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 804/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 602us/step - loss: 8.5526e-05 - mean_absolute_error: 0.0055 - val_loss: 6.1288e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 805/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 8.5346e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1283e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 806/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 8.5163e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1219e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 807/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.4981e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1220e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 808/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 8.4800e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1151e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 809/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 8.4621e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1162e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 810/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.4439e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1076e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 811/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 8.4267e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1100e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 812/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 8.4088e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1009e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 813/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.3913e-05 - mean_absolute_error: 0.0054 - val_loss: 6.1038e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 814/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.3737e-05 - mean_absolute_error: 0.0054 - val_loss: 6.0928e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 815/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 8.3564e-05 - mean_absolute_error: 0.0054 - val_loss: 6.0974e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 816/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 8.3389e-05 - mean_absolute_error: 0.0054 - val_loss: 6.0850e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 817/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 8.3222e-05 - mean_absolute_error: 0.0054 - val_loss: 6.0936e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 818/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 8.3044e-05 - mean_absolute_error: 0.0054 - val_loss: 6.0782e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 819/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 8.2879e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0884e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 820/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 8.2705e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0703e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 821/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.2541e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0845e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 822/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 8.2366e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0617e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 823/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 8.2209e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0802e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 824/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 8.2032e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0529e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 825/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 8.1877e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0770e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 826/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 8.1703e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0419e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 827/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 8.1553e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0746e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 828/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 8.1377e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0315e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 829/1000\n",
      "52/52 [==============================] - 0s 634us/step - loss: 8.1232e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0735e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 830/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 8.1053e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0185e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 831/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 8.0915e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0726e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 832/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 8.0737e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0048e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 833/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 8.0604e-05 - mean_absolute_error: 0.0053 - val_loss: 6.0747e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 834/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 8.0424e-05 - mean_absolute_error: 0.0053 - val_loss: 5.9873e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 835/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 8.0301e-05 - mean_absolute_error: 0.0052 - val_loss: 6.0787e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 836/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 8.0120e-05 - mean_absolute_error: 0.0053 - val_loss: 5.9685e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 837/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 8.0010e-05 - mean_absolute_error: 0.0052 - val_loss: 6.0867e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 838/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 7.9824e-05 - mean_absolute_error: 0.0052 - val_loss: 5.9466e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 839/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 7.9732e-05 - mean_absolute_error: 0.0052 - val_loss: 6.0975e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 840/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 7.9545e-05 - mean_absolute_error: 0.0052 - val_loss: 5.9194e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 841/1000\n",
      "52/52 [==============================] - 0s 656us/step - loss: 7.9479e-05 - mean_absolute_error: 0.0052 - val_loss: 6.1151e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 842/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 7.9288e-05 - mean_absolute_error: 0.0052 - val_loss: 5.8862e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 843/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 7.9260e-05 - mean_absolute_error: 0.0052 - val_loss: 6.1388e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 844/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 7.9072e-05 - mean_absolute_error: 0.0052 - val_loss: 5.8444e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 845/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 7.9092e-05 - mean_absolute_error: 0.0052 - val_loss: 6.1719e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 846/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 7.8920e-05 - mean_absolute_error: 0.0053 - val_loss: 5.7933e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 847/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 7.9019e-05 - mean_absolute_error: 0.0052 - val_loss: 6.2190e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 848/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 7.8872e-05 - mean_absolute_error: 0.0053 - val_loss: 5.7290e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 849/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 7.9093e-05 - mean_absolute_error: 0.0052 - val_loss: 6.2827e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 850/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 7.8999e-05 - mean_absolute_error: 0.0053 - val_loss: 5.6499e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 851/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 7.9399e-05 - mean_absolute_error: 0.0053 - val_loss: 6.3672e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 852/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 7.9412e-05 - mean_absolute_error: 0.0054 - val_loss: 5.5501e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 853/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 8.0099e-05 - mean_absolute_error: 0.0054 - val_loss: 6.4782e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 854/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 8.0298e-05 - mean_absolute_error: 0.0055 - val_loss: 5.4267e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 855/1000\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 8.1451e-05 - mean_absolute_error: 0.0055 - val_loss: 6.6248e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 856/1000\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 8.1962e-05 - mean_absolute_error: 0.0056 - val_loss: 5.2769e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 857/1000\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 8.3869e-05 - mean_absolute_error: 0.0057 - val_loss: 6.8115e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 858/1000\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 8.4926e-05 - mean_absolute_error: 0.0059 - val_loss: 5.0990e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 859/1000\n",
      "52/52 [==============================] - 0s 1000us/step - loss: 8.8060e-05 - mean_absolute_error: 0.0059 - val_loss: 7.0443e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 860/1000\n",
      "52/52 [==============================] - 0s 798us/step - loss: 8.9971e-05 - mean_absolute_error: 0.0062 - val_loss: 4.8955e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 861/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 9.5073e-05 - mean_absolute_error: 0.0063 - val_loss: 7.3053e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 862/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 9.8172e-05 - mean_absolute_error: 0.0067 - val_loss: 4.6926e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 863/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.0607e-04 - mean_absolute_error: 0.0069 - val_loss: 7.5239e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 864/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.1027e-04 - mean_absolute_error: 0.0073 - val_loss: 4.5471e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 865/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.2093e-04 - mean_absolute_error: 0.0075 - val_loss: 7.5113e-04 - val_mean_absolute_error: 0.0185\n",
      "Epoch 866/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.2397e-04 - mean_absolute_error: 0.0079 - val_loss: 4.5793e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 867/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.3333e-04 - mean_absolute_error: 0.0079 - val_loss: 6.9977e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 868/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.2895e-04 - mean_absolute_error: 0.0080 - val_loss: 4.9921e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 869/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 1.2919e-04 - mean_absolute_error: 0.0077 - val_loss: 6.0200e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 870/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.1470e-04 - mean_absolute_error: 0.0073 - val_loss: 5.8804e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 871/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.0755e-04 - mean_absolute_error: 0.0070 - val_loss: 5.1588e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 872/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 9.3900e-05 - mean_absolute_error: 0.0063 - val_loss: 6.8038e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 873/1000\n",
      "52/52 [==============================] - 0s 632us/step - loss: 8.8743e-05 - mean_absolute_error: 0.0062 - val_loss: 4.8391e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 874/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 8.4802e-05 - mean_absolute_error: 0.0057 - val_loss: 7.0346e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 875/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 8.2079e-05 - mean_absolute_error: 0.0056 - val_loss: 4.9338e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 876/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 8.3595e-05 - mean_absolute_error: 0.0056 - val_loss: 6.5578e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 877/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 8.2248e-05 - mean_absolute_error: 0.0058 - val_loss: 5.2544e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 878/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 8.2680e-05 - mean_absolute_error: 0.0057 - val_loss: 5.9167e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 879/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 8.0113e-05 - mean_absolute_error: 0.0056 - val_loss: 5.7298e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 880/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 7.9640e-05 - mean_absolute_error: 0.0056 - val_loss: 5.5052e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 881/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 7.6892e-05 - mean_absolute_error: 0.0053 - val_loss: 6.1409e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 882/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 7.6077e-05 - mean_absolute_error: 0.0053 - val_loss: 5.3654e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 883/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 7.5383e-05 - mean_absolute_error: 0.0051 - val_loss: 6.2225e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 884/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 7.4670e-05 - mean_absolute_error: 0.0051 - val_loss: 5.3869e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 885/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 7.4829e-05 - mean_absolute_error: 0.0051 - val_loss: 6.0336e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 886/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 7.4445e-05 - mean_absolute_error: 0.0052 - val_loss: 5.5305e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 887/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 7.4489e-05 - mean_absolute_error: 0.0051 - val_loss: 5.8159e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 888/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 7.3936e-05 - mean_absolute_error: 0.0051 - val_loss: 5.7413e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 889/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 7.3845e-05 - mean_absolute_error: 0.0051 - val_loss: 5.6770e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 890/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 7.3293e-05 - mean_absolute_error: 0.0050 - val_loss: 5.8897e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 891/1000\n",
      "52/52 [==============================] - 0s 646us/step - loss: 7.3075e-05 - mean_absolute_error: 0.0050 - val_loss: 5.6058e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 892/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 7.2758e-05 - mean_absolute_error: 0.0050 - val_loss: 5.9178e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 893/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 7.2593e-05 - mean_absolute_error: 0.0050 - val_loss: 5.5995e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 894/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 7.2477e-05 - mean_absolute_error: 0.0049 - val_loss: 5.8716e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 895/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 7.2312e-05 - mean_absolute_error: 0.0050 - val_loss: 5.6513e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 896/1000\n",
      "52/52 [==============================] - 0s 630us/step - loss: 7.2258e-05 - mean_absolute_error: 0.0049 - val_loss: 5.8028e-04 - val_mean_absolute_error: 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 7.2065e-05 - mean_absolute_error: 0.0050 - val_loss: 5.7183e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 898/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 7.1984e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7372e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 899/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 7.1779e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7652e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 900/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 7.1683e-05 - mean_absolute_error: 0.0049 - val_loss: 5.6946e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 901/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 7.1486e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7862e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 902/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 7.1383e-05 - mean_absolute_error: 0.0049 - val_loss: 5.6803e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 903/1000\n",
      "52/52 [==============================] - 0s 628us/step - loss: 7.1240e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7836e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 904/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 7.1120e-05 - mean_absolute_error: 0.0049 - val_loss: 5.6847e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 905/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 7.1016e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7628e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 906/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 7.0892e-05 - mean_absolute_error: 0.0049 - val_loss: 5.6970e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 907/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 7.0794e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7373e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 908/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 7.0665e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7116e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 909/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 7.0567e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7179e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 910/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 7.0431e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7229e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 911/1000\n",
      "52/52 [==============================] - 0s 651us/step - loss: 7.0332e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7031e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 912/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 7.0201e-05 - mean_absolute_error: 0.0049 - val_loss: 5.7249e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 913/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 7.0102e-05 - mean_absolute_error: 0.0049 - val_loss: 5.6940e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 914/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.9979e-05 - mean_absolute_error: 0.0048 - val_loss: 5.7215e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 915/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 6.9875e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6907e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 916/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 6.9761e-05 - mean_absolute_error: 0.0048 - val_loss: 5.7145e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 917/1000\n",
      "52/52 [==============================] - 0s 634us/step - loss: 6.9651e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6904e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 918/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 6.9545e-05 - mean_absolute_error: 0.0048 - val_loss: 5.7061e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 919/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 6.9431e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6900e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 920/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 6.9328e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6962e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 921/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 6.9214e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6879e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 922/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 6.9113e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6874e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 923/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 6.9000e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6863e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 924/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 6.8899e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6809e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 925/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 6.8786e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6834e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 926/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 6.8687e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6756e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 927/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 6.8575e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6796e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 928/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 6.8474e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6708e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 929/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 6.8367e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6748e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 930/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 6.8263e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6665e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 931/1000\n",
      "52/52 [==============================] - 0s 629us/step - loss: 6.8160e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6701e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 932/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 6.8054e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6630e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 933/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.7951e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6639e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 934/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 6.7847e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6584e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 935/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 6.7747e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6587e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 936/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 6.7642e-05 - mean_absolute_error: 0.0048 - val_loss: 5.6547e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 937/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 6.7542e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6538e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 938/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.7439e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6507e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 939/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 6.7339e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6488e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 940/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 6.7238e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6462e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 941/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 6.7136e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6434e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 942/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.7036e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6414e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 943/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 6.6936e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6388e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 944/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 6.6835e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6364e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 945/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 6.6737e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6339e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 946/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 6.6638e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6323e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 947/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 6.6538e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6295e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 948/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 6.6440e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6275e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 949/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 6.6341e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6249e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 950/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 6.6243e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6226e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 951/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 6.6146e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6199e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 952/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 6.6049e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6176e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 953/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 6.5952e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6152e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 954/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 6.5855e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6127e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 955/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 6.5760e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6103e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 956/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.5662e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6080e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 957/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 6.5567e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6055e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 958/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 6.5472e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6037e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 959/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 6.5377e-05 - mean_absolute_error: 0.0047 - val_loss: 5.6014e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 960/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 6.5280e-05 - mean_absolute_error: 0.0047 - val_loss: 5.5983e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 961/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.5188e-05 - mean_absolute_error: 0.0047 - val_loss: 5.5955e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 962/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 6.5091e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5932e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 963/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 6.4998e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5906e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 964/1000\n",
      "52/52 [==============================] - 0s 633us/step - loss: 6.4904e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5887e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 965/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 6.4812e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5863e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 966/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 6.4715e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5842e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 967/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.4626e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5820e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 968/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.4531e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5794e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 969/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 6.4439e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5765e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 970/1000\n",
      "52/52 [==============================] - 0s 613us/step - loss: 6.4348e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5738e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 971/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.4256e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5714e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 972/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 6.4163e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5687e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 973/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.4073e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5675e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 974/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 6.3980e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5647e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 975/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 6.3890e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5630e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 976/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 6.3800e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5603e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 977/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.3708e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5579e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 978/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 6.3620e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5547e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 979/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 6.3530e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5527e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 980/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 6.3440e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5497e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 981/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 6.3352e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5486e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 982/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 6.3262e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5451e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 983/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 6.3171e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5430e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 984/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 6.3086e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5401e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 985/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 6.2995e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5386e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 986/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 6.2908e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5342e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 987/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 6.2823e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5346e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 988/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 6.2732e-05 - mean_absolute_error: 0.0046 - val_loss: 5.5297e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 989/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 639us/step - loss: 6.2646e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5303e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 990/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.2558e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5248e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 991/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 6.2471e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5251e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 992/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 6.2386e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5197e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 993/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 6.2299e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5213e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 994/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 6.2211e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5149e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 995/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.2127e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5169e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 996/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 6.2039e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5089e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 997/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.1956e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5123e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 998/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.1870e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5033e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 999/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 6.1786e-05 - mean_absolute_error: 0.0045 - val_loss: 5.5081e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 1000/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 6.1699e-05 - mean_absolute_error: 0.0045 - val_loss: 5.4986e-04 - val_mean_absolute_error: 0.0158\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAADGCAYAAADGz2nhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl41PW59/H3PZPJCiQhAdkFFQUUBUnRavVUbXvQtop9\nUPSxrXrqoVqtXU9LT5/TxW72dLF6Thdp1dYuUkprS6uW01as9lgsCSKrsskSQIgBEiB75n7+mF9g\nMmaZQJLJMJ/Xdc01v/luc/9y5cqd7/e3mbsjIiIi6SuU6gBERETkxCiZi4iIpDklcxERkTSnZC4i\nIpLmlMxFRETSnJK5iIhImksqmZvZLDN7xcw2m9n8DuovNbOVZtZiZnPiyi8zs1VxrwYzmx3U/djM\nXo2rm9Z7uyUiIpI5rLvrzM0sDGwE3g5UAiuAG919fVyb8cAQ4JPAEndf3ME4Q4HNwBh3rzOzHwN/\n6KitiIiIJC8riTYzgc3uvhXAzBYC1wBHk7m7bwvqol2MMwd4yt3rjjtaEREReYNkltlHAzvjPlcG\nZT11A/BYQtlXzGy1md1nZjkddTKzeWZWHrzmHcf3ioiInNSSmZmfMDMbCUwFlsYVfwZ4DcgGFgCf\nBu5J7OvuC4J6SktLvays7ME+D1hERGQAqKioeN3dh3XXLplkvgsYG/d5TFDWE9cDj7t7c1uBu+8J\nNhvN7BFix9u7NH78eMrLy3v41SIiIunJzLYn0y6ZZfYVwEQzm2Bm2cSWy5f0MJ4bSVhiD2brmJkB\ns4G1PRxTRERESCKZu3sLcBexJfINwCJ3X2dm95jZ1QBm9iYzqwSuAx40s3Vt/YMz3ccCf00Y+udm\ntgZYA5QCXz7x3REREck83V6aNpCUlZW5ltlFRCRTmFmFu5d1165fToATEZGTS3NzM5WVlTQ0NKQ6\nlJNCbm4uY8aMIRKJHFf/jE3mj/1jB9uqj/CZKyenOhQRkbRTWVnJ4MGDGT9+PLFTn+R4uTvV1dVU\nVlYyYcKE4xojY+/NXrH9AL9ftTvVYYiIpKWGhgZKSkqUyHuBmVFSUnJCqxwZm8zzs8McaWpNdRgi\nImlLibz3nOjPMoOTeRb1SuYiInISyNhkXpAdpqk1SnNrV7eTFxGRgejgwYN873vf63G/q666ioMH\nD/ZBRKmVsck8LzsMQJ1m5yIiaaezZN7S0tJlvyeffJKioqK+CitlMvZs9oKc2K7XNbVQmHd8lwKI\niAh88ffrWL+7tlfHnDJqCJ9/99md1s+fP58tW7Ywbdo0IpEIubm5FBcX8/LLL7Nx40Zmz57Nzp07\naWho4CMf+Qjz5sWe09V2W/DDhw9z5ZVX8pa3vIXnn3+e0aNH87vf/Y68vLxe3Y/+krEz83zNzEVE\n0ta9997L6aefzqpVq/jGN77BypUruf/++9m4cSMADz/8MBUVFZSXl/PAAw9QXV39hjE2bdrEnXfe\nybp16ygqKuLXv/51f+9Gr8nYmXl+djAzb1QyFxE5EV3NoPvLzJkz212j/cADD/D4448DsHPnTjZt\n2kRJSUm7PhMmTGDatGkAzJgxg23btvVbvL0tg5N528y86+MrIiIy8BUUFBzdfuaZZ/jzn//M3//+\nd/Lz83nrW9/a4TXcOTk5R7fD4TD19fX9Emtf0DK7ltlFRNLO4MGDOXToUId1NTU1FBcXk5+fz8sv\nv8zy5cv7Obr+l7Ez87YT4I5oZi4iknZKSkq4+OKLOeecc8jLy+OUU045Wjdr1ix+8IMfMHnyZM46\n6ywuvPDCFEbaP5JK5mY2C7gfCAM/cvd7E+ovBb4DnAvc4O6L4+paiT3mFGCHu7c9NnUCsBAoASqA\n97l704ntTvLyIpqZi4iks1/84hcdlufk5PDUU091WNd2XLy0tJS1a9ceLf/kJz/Z6/H1p26X2c0s\nDHwXuBKYAtxoZlMSmu0AbgE6+snWu/u04HV1XPnXgfvc/QzgAPCB44j/uB29NK1RM3MREUlvyRwz\nnwlsdvetwcx5IXBNfAN33+buq4GkbqdmsZvQXg60zeB/AsxOOupecPSYebNm5iIikt6SSeajgZ1x\nnyuDsmTlmlm5mS03s7aEXQIcdPe2aXFPxzxhOVkhQqZL00REJP31xwlwp7r7LjM7DXjazNYANcl2\nNrN5wDyAcePG9VpQZkZ+dpaOmYuISNpLZma+Cxgb93lMUJYUd98VvG8FngGmA9VAkZm1/TPR6Zju\nvsDdy9y9bNiwYcl+bVLys8O6zlxERNJeMsl8BTDRzCaYWTZwA7AkmcHNrNjMcoLtUuBiYL27O7AM\nmBM0vRn4XU+DP1EFOVl6prmIiKS9bpN5cFz7LmApsAFY5O7rzOweM2u7zOxNZlYJXAc8aGbrgu6T\ngXIze4lY8r7X3dcHdZ8GPm5mm4kdQ3+oN3csGXmRMPWamYuInPQGDRoEwO7du5kzZ06Hbd761rdS\nXl7e5Tjf+c53qKurO/p5oDxSNalj5u7+JPBkQtnn4rZXEFsqT+z3PDC1kzG3EjtTPmUKcsIc0Qlw\nIiIZY9SoUSxevLj7hp34zne+w3vf+17y8/OB2CNVB4KMvQMcQF52FjX1zakOQ0QkvT01H15b0327\nnhgxFa68t9Pq+fPnM3bsWO68804AvvCFL5CVlcWyZcs4cOAAzc3NfPnLX+aaa9pdSc22bdt417ve\nxdq1a6mvr+fWW2/lpZdeYtKkSe3uzX7HHXewYsUK6uvrmTNnDl/84hd54IEH2L17N5dddhmlpaUs\nW7bs6CNVS0tL+fa3v83DDz8MwG233cZHP/pRtm3b1i+PWs3Ye7MDFGSHddMYEZE0NHfuXBYtWnT0\n86JFi7j55pt5/PHHWblyJcuWLeMTn/gEsVO0Ovb973+f/Px8NmzYwBe/+EUqKiqO1n3lK1+hvLyc\n1atX89e//pXVq1dz9913M2rUKJYtW8ayZcvajVVRUcEjjzzCCy+8wPLly/nhD3/Iiy++CPTPo1Yz\nemauS9NERHpBFzPovjJ9+nT27dvH7t27qaqqori4mBEjRvCxj32MZ599llAoxK5du9i7dy8jRozo\ncIxnn32Wu+++G4Bzzz2Xc88992jdokWLWLBgAS0tLezZs4f169e3q0/0t7/9jWuvvfbo09ve8573\n8Nxzz3H11Vf3y6NWMzyZ69I0EZF0dd1117F48WJee+015s6dy89//nOqqqqoqKggEokwfvz4Dh99\n2p1XX32Vb37zm6xYsYLi4mJuueWW4xqnTX88ajWjl9nzc8KamYuIpKm5c+eycOFCFi9ezHXXXUdN\nTQ3Dhw8nEomwbNkytm/f3mX/Sy+99OjDWtauXcvq1asBqK2tpaCggMLCQvbu3dvuoS2dPXr1kksu\n4be//S11dXUcOXKExx9/nEsuuaQX97ZrmT0zj2TR2BKlpTVKVjij/68REUk7Z599NocOHWL06NGM\nHDmSm266iXe/+91MnTqVsrIyJk2a1GX/O+64g1tvvZXJkyczefJkZsyYAcB5553H9OnTmTRpEmPH\njuXiiy8+2mfevHnMmjXr6LHzNueffz633HILM2fGLtK67bbbmD59ep8sqXfEujo5YKApKyvz7q4B\n7IkfPbeVLz+xgdVfeAdDciO9Nq6IyMluw4YNTJ48OdVhnFQ6+pmaWYW7l3XXN6Ono3nBk9PqtdQu\nIiJpLKOTeUF27CjDEV2eJiIiaSyjk/nRZ5prZi4i0mPpdJh2oDvRn2WGJ/PYzFzJXESkZ3Jzc6mu\nrlZC7wXuTnV1Nbm5ucc9RmafzZ7TNjPXMruISE+MGTOGyspKqqqqUh3KSSE3N5cxY97wiJOkZXYy\n1zK7iMhxiUQiTJgwIdVhSCCjl9kLtMwuIiIngaSSuZnNMrNXzGyzmc3voP5SM1tpZi1mNieufJqZ\n/d3M1pnZajObG1f3YzN71cxWBa9pvbNLycvL1jK7iIikv26X2c0sDHwXeDtQCawwsyXuvj6u2Q7g\nFuCTCd3rgPe7+yYzGwVUmNlSd297kvu/ufvxP1j2BB27NC2Ymbc0waHdUDw+VSGJiIj0WDIz85nA\nZnff6u5NwEKg3QNi3X2bu68GognlG919U7C9G9gHDOuVyHtBbiSEGdS3zcwXvQ/uPw+2/W9qAxMR\nEemBZJL5aGBn3OfKoKxHzGwmkA1siSv+SrD8fp+Z5XTSb56ZlZtZeW+fNWlm5EfCHGlqhdo9sPGP\nsYryh3r1e0RERPpSv5wAZ2YjgZ8Ct7p72+z9M8Ak4E3AUODTHfV19wXuXubuZcOG9f6kPj8neKb5\n7pWxgpIzYMsy0LWTIiKSJpJJ5ruAsXGfxwRlSTGzIcATwGfdfXlbubvv8ZhG4BFiy/n97ugzzV9b\nCxic/36o3w+1Se+iiIhISiWTzFcAE81sgpllAzcAS5IZPGj/OPBo4oluwWwdMzNgNrC2J4H3lvzs\nYGZevRmKxsKY4H+KvetSEY6IiEiPdZvM3b0FuAtYCmwAFrn7OjO7x8yuBjCzN5lZJXAd8KCZtWXC\n64FLgVs6uATt52a2BlgDlAJf7tU9S9LRmXnNTigcB6dMiVUomYuISJpI6g5w7v4k8GRC2efitlcQ\nW35P7Pcz4GedjHl5jyLtI/nZYQ41tEDjDphwKeQWxpK6krmIiKSJjL4DHMSuNW9sbIJDe2BIcJL+\n8Enw+iupDUxERCRJGZ/M87PDZDUdAI/CoFNihcXj4cB2ndEuIiJpIeOT+aDcLHIaqoMPwaVvRadC\nYy3UH0hdYCIiIknK+GRelJ9NXlOQzAuGx97bbud6cHtKYhIREekJJfO8CEOpiX0Y1JbMT429H9iW\nkphERER6Qsk8P0KpBcm8IG6ZHWLHzUVERAY4JfP8CMOslmgoO3ZZGkDuEMgbqmV2ERFJCxmfzAvz\nsim1Gppyh4LZsYriU7XMLiIiaSHjk3lRfoRSamjILkmoOFXL7CIikhaUzPNix8wPZRW3rygeDwd3\nQLQ1JXGJiIgkK3OT+aG9ULWRwiCZ14YSknnRWIg2w+G9qYlPREQkSZmbzP/ns/CL68kyKLFa9ltR\n+/rCcbH3msr+j01ERKQHMjeZ55dCXTU0HCRCK6+TkMyLgke4H9zR/7GJiIj0QFLJ3MxmmdkrZrbZ\nzOZ3UH+pma00sxYzm5NQd7OZbQpeN8eVzzCzNcGYDwTPNe8/BSWxW7bW7ARgd8vg9vWFwUPggnoR\nEZGBqttkbmZh4LvAlcAU4EYzm5LQbAdwC/CLhL5Dgc8DFwAzgc+bWdvB6e8D/wpMDF6zjnsvjkd+\naex93wYAtjcUtK/PGQy5RXBQyVxERAa2ZGbmM4HN7r7V3ZuAhcA18Q3cfZu7rwaiCX3/GfiTu+93\n9wPAn4BZZjYSGOLuy93dgUeB2Se6Mz1S0JbM1wPwypG8N7YpGquZuYiIDHjJJPPRQHxGqwzKktFZ\n39HBdrdjmtk8Mys3s/KqqqokvzYJbbdu3bsOgG0Ngzjc2NK+TeE4zcxFRGTAG/AnwLn7Ancvc/ey\nYcOG9d7AQ0bF3ne/SNSyqKGAPQfr27dpm5nrueYiIjKAJZPMdwFj4z6PCcqS0VnfXcH28YzZOwaP\nBAtBXTXN+cMBY3dNQ/s2hWOh6TA0HOzX0ERERHoimWS+AphoZhPMLBu4AViS5PhLgXeYWXFw4ts7\ngKXuvgeoNbMLg7PY3w/87jjiP37hCAwaAYAPPR2A12oSZuZtZ7RrqV1ERAawbpO5u7cAdxFLzBuA\nRe6+zszuMbOrAczsTWZWCVwHPGhm64K++4EvEfuHYAVwT1AG8CHgR8BmYAvwVK/uWTJKYkk8csok\nzGDXgQ6W2UEnwYmIyICWlUwjd38SeDKh7HNx2ytov2we3+5h4OEOysuBc3oSbK8bfwlse47wWf/M\nuA1ZbKk60r6+7S5wmpmLiMgAllQyP2ld/BGY+DYYPYMzTynn5ddq29cXlEJWnmbmIiIyoA34s9n7\nVCQXRs8AYNKIwWyrrqOhOe4paWax4+a6pauIiAxgmZ3M45x5ymBao86WqsPtK4rG6mErIiIyoCmZ\nByaPjN2bff3uhKX2wjFaZhcRkQFNyTxwWukghuRmsXLHgfYVhePgSBU013fcUUREJMWUzAOhkHH+\nqcWUb0tI5kcvT9NSu4iIDExK5nFmjCtm077D1NQ1Hyss1HPNRURkYFMyjzNjfOzprO2W2nXjGBER\nGeCUzONMG1tEOGSs2Lb/WOGQ0RCKwP6tqQtMRESkC0rmcfKzszh3TCHLt1YfKwyFYehp8Prm1AUm\nIiLSBSXzBBedXsJLlTXtn21eOhGqlcxFRGRgUjJPcNHppbRGvf1Se8npsWX2aGvnHUVERFJEyTzB\njFOLyQ6H+PuWuKX2kjMg2gwHt6cuMBERkU4klczNbJaZvWJmm81sfgf1OWb2y6D+BTMbH5TfZGar\n4l5RM5sW1D0TjNlWN7w3d+x45UbCTB9XxPNbXj9WWHpW7H3fy6kJSkREpAvdJnMzCwPfBa4EpgA3\nmtmUhGYfAA64+xnAfcDXAdz95+4+zd2nAe8DXnX3VXH9bmqrd/d9vbA/veKi00tZt7uWg3VNsYJT\nzgYMXlud0rhEREQ6kszMfCaw2d23unsTsBC4JqHNNcBPgu3FwBVmZgltbgz6DnhvPr0Ed1i+NThu\nnjModhLcnpdSG5iIiEgHkknmo4H4O6ZUBmUdtnH3FqAGKEloMxd4LKHskWCJ/T86SP4pM21sEXmR\nMP+7OW6pfeR5sEczcxERGXj65QQ4M7sAqHP3tXHFN7n7VOCS4PW+TvrOM7NyMyuvqqrqh2ghOyvE\nxWeUsOyVfbh7rHDkNKithNrd/RKDiIhIspJJ5ruAsXGfxwRlHbYxsyygEIg7HZwbSJiVu/uu4P0Q\n8Atiy/lv4O4L3L3M3cuGDRuWRLi94/JJp1B5oJ5N+4Lnm5/21tj7lqf7LQYREZFkJJPMVwATzWyC\nmWUTS8xLEtosAW4OtucAT3swpTWzEHA9ccfLzSzLzEqD7QjwLmAtA8jlk2In1/9lQ3Be3ilnw6AR\nsPnPKYxKRETkjbpN5sEx8LuApcAGYJG7rzOze8zs6qDZQ0CJmW0GPg7EX752KbDT3eNvbp4DLDWz\n1cAqYjP7H57w3vSiEYW5nD1qCH9a/1qswAzOfAds+hM01KY2OBERkThZyTRy9yeBJxPKPhe33QBc\n10nfZ4ALE8qOADN6GGu/u2rqSL6x9BV2VNcxriQfZtwCKx+Flx6DCz6Y6vBEREQA3QGuS7Onx07a\nf/zF4BSB0TNg7IXw3Lc0OxcRkQFDybwLo4vyePNpJfzmxUqi0eCs9llfhcP74JmvpTY4ERGRgJJ5\nN+a+aSzbq+tY9kpwItzoGbHl9hcehL3rUxqbiIgIKJl3653njmR0UR7fe2bLsWvOr/gcZA+CZV9J\nbXAiIiIomXcrEg7xwX86jYrtB/hb2x3h8ofCRXfBy3+AXRWpDVBERDKeknkSri8by7ih+XxhyTqa\nWqKxwgtuh7xieO7bqQ1OREQynpJ5EnIjYT7/7ilsqTrCj59/NSgcAmUfgJefgOotqQ1QREQympJ5\nkq6YfApXTBrOfX/axNaq4BavM/8VwhF44QepDU5ERDKaknkPfOXaqWRnhfjYopdobo3C4BEw9Tp4\n8WdQtz/V4YmISIZSMu+BEYW5fO09U3lp50H+6y+bYoUXfgia66DixymNTUREMpeSeQ9dNXUkc2aM\n4YGnN/PUmj0w4pzYE9X+sQBamlIdnoiIZCAl8+Pw5dnncP64Ij76y1VUbN8Pb/4wHNoD636T6tBE\nRCQDKZkfh9xImB++v4xRRXm890f/YFnrVBg2Cf76n9BUl+rwREQkwyiZH6eSQTks+uCbOW1YAbc9\nWsHvRn4E9m+BP3wMotFUhyciIhkkqWRuZrPM7BUz22xm8zuozzGzXwb1L5jZ+KB8vJnVm9mq4PWD\nuD4zzGxN0OcBM7Pe2qn+MmxwDgvnXchVU0fykX8U8ljBe2H1Qvj93Tp+LiIi/abbZG5mYeC7wJXA\nFOBGM5uS0OwDwAF3PwO4D/h6XN0Wd58WvG6PK/8+8K/AxOA16/h3I3UG50Z44IZp3Df3PL7VcA0P\ntFwLL/6Uuh9cATtXpDo8ERHJAMnMzGcCm919q7s3AQuBaxLaXAP8JNheDFzR1UzbzEYCQ9x9ucee\nXvIoMLvH0Q8QZsa108fw9L9dxpGLPs1Hoh/nSNV2eOht7PvPGWz/1b9zeMOfofFwqkMVEZGTUFYS\nbUYDO+M+VwIXdNbG3VvMrAYoCeommNmLQC3w/9z9uaB9ZcKYozv6cjObB8wDGDduXBLhps6Q3Aif\nuWoytZf/O7//x3W0VjzKlANPM33t9wiv+y6thKjMOYPaIROJlpxF7qjJ5I+cxNBRp1FQMCjV4YuI\nSJpKJpmfiD3AOHevNrMZwG/N7OyeDODuC4AFAGVlZd4HMfa6IbkRbrp0Clx6L4cbWyjfspO9G54j\nvHM5Iw+tZdS+5xlR9QS8fKzPPi9mX3g4ByMjOJxdSnNOMa25xXjeUMgvIatgKFmDiskpGEpuwWAG\n52ZTkBNmUG4Wg3KyyIuEScPTDkREpBckk8x3AWPjPo8JyjpqU2lmWUAhUB0soTcCuHuFmW0Bzgza\nj+lmzJPCoJwsLpgyAaZMAN4PQEtrlB17X2P/tjU079uCH9xB5NBO8ut2MbFpI0MaXyDvUEOnY7Z4\niFryqfUCXiOfQ57PIfKpCw+iMTyIpqzBNEWGQG4h+UOGMqSolNJhIzl98rkMLxrcT3suIiL9JZlk\nvgKYaGYTiCXcG4D/m9BmCXAz8HdgDvC0u7uZDQP2u3urmZ1G7ES3re6+38xqzexC4AViWe6/emeX\nBr6scIhxo0YxbtSozhs1N0D9fvzI6zQfrqa+poqmIwdpOXKQlroDeEMNoYYahjbWMqyxlkhzNdkt\n28hpPUxOY33sX6jDwOvHhmz6Y5iNoXG8PvR8Cs97J2ddcBVZOXl9vbsiItLHuk3mwTHwu4ClQBh4\n2N3Xmdk9QLm7LwEeAn5qZpuB/cQSPsClwD1m1gxEgdvdve2JJB8CfgzkAU8FL2kTyYXIKGzIKLKB\n7J70bW2GhlporMHrazhcU03Vnh0c2vESkX1rmP7678l7+tfsf7qQnWfcxJT3zCeSX9hHOyIiIn3N\nYivh6aGsrMzLy8tTHUbaa6g/wppnl2AVD1HWtIIqK+XwlfczYea7Uh2aiIjEMbMKdy/rrp3uAJeB\ncvMKeNM/38iMz/yJ5Zct5JDnMu6J97Jq8de77ywiIgOOknkGMzMu/KcrGfrR51iZdyHT1n6Vv/3s\nnlSHJSIiPaRkLhQVDWXax3/HqsGX8pbN3+K5X92f6pBERKQHlMwFgEh2Dud8eDEb8s5n5tov8cKz\nS1MdkoiIJEnJXI7Kys5hwu2L2B8uYfxfPsj2ysruO4mISMopmUs7uYXDCN3wU4ZaLdsevYPmVj3O\nVURkoFMylzc45cyZvHr2h/mnpmd55jcPpjocERHphpK5dOjM9/wH23PO5Ly1X2f77r2pDkdERLqg\nZC4dC2cx+Nr7GG4H2LDo86mORkREuqBkLp0aOuktrB/+Ti4/sIh1a19KdTgiItIJJXPp0qnXf52o\nhal64kuk061/RUQyiZK5dKmgdCybx83lkro/89JLFakOR0REOqBkLt06ffZnabJs6v701VSHIiIi\nHUgqmZvZLDN7xcw2m9n8DupzzOyXQf0LZjY+KH+7mVWY2Zrg/fK4Ps8EY64KXsN7a6ekd+UNHcmG\nMddzweGn2bxBx85FRAaabpO5mYWB7wJXAlOAG81sSkKzDwAH3P0M4D6g7fFbrwPvdvepwM3ATxP6\n3eTu04LXvhPYD+ljZ1w9n2ayqPrjvakORUREEiQzM58JbHb3re7eBCwErklocw3wk2B7MXCFmZm7\nv+juu4PydUCemeX0RuDSv4YMH8OaEbMpO7iUXds2pjocERGJk0wyHw3sjPtcGZR12MbdW4AaoCSh\nzf8BVrp7Y1zZI8ES+3+YmXX05WY2z8zKzay8qqoqiXClr5z67s/gQOUfvpbqUEREJE6/nABnZmcT\nW3r/YFzxTcHy+yXB630d9XX3Be5e5u5lw4YN6/tgpVPDx5zOyuIrmVb1ew7u3ZHqcEREJJBMMt8F\njI37PCYo67CNmWUBhUB18HkM8Djwfnff0tbB3XcF74eAXxBbzpcBbviV88miha1Lvt59YxER6RfJ\nJPMVwEQzm2Bm2cANwJKENkuIneAGMAd42t3dzIqAJ4D57v6/bY3NLMvMSoPtCPAuYO2J7Yr0h9PO\nmso/Bl3O5F2/oqFG5yyKiAwE3Sbz4Bj4XcBSYAOwyN3Xmdk9ZnZ10OwhoMTMNgMfB9ouX7sLOAP4\nXMIlaDnAUjNbDawiNrP/YW/umPSdvMs/RR6NbF7yn6kORUREAEunW3SWlZV5eXl5qsPIeO7O8197\nJ+c1vUjep9YTzi9OdUgiIiclM6tw97Lu2ukOcNJjZkb0LZ9gEHVs/cO3Ux2OiEjGUzKX43LRWy7n\nufBMxqxfQOsBndkuIpJKSuZyXMIho+FtX8Xd2fvYXZBGh2tERE42SuZy3N52YRm/HPw+Ru37Kw0r\nH0t1OCIiGUvJXI6bmTFtznxeiE4i9MTHYN/LqQ5JRCQjKZnLCZk+fhjLz/8mNa3ZHPnpDVC3P9Uh\niYhkHCVzOWG3v+sivjHk38mqraT+x9dC4+FUhyQiklGUzOWE5WSF+dhtt/D/Ih8nsm81jY9cDUde\nT3VYIiIZQ8lcesXIwjxuvvVOPsXH8dfW0LTgCnhNd+gVEekPSubSa84ZXcgHb/8od4a/QM3Bg7Qu\nuAye/29obU51aCIiJzUlc+lVZ40YzJc+/C98evj3WdZ8NvzPZ2n+7wth9a+gpSnV4YmInJSUzKXX\njSrKY8Hts3jlsh9yR+un2LG/Dn5zG83fPgf+5z9g5wqIRlMdpojISUMPWpE+taemngeXbWJPxRPM\n5Y9cGl5DFq20Zg8hNO4CbNwFcMo5UHomFI+HUDjVIYuIDBjJPmglqWRuZrOA+4Ew8CN3vzehPgd4\nFJgBVAOTIJMIAAAGxElEQVRz3X1bUPcZ4ANAK3C3uy9NZsyOKJmnrwNHmnj8xV0889JGinc9wwWh\nl5mZtZEzqDzaJhrOgcIxhIaMgiGjYPBIGDwCcosgrwhyC4+9sgdBJA/C2WCWwj0TEek7vZbMzSwM\nbATeDlQCK4Ab3X19XJsPAee6++1mdgNwrbvPNbMpwGPATGAU8GfgzKBbl2N2RMn85PBaTQPLt1bz\nwqv7Wf/qTrL2b+I0KjnddjPGXmdU+ACj7AClvp8sWrocyy2Eh3PxrNxYco/kYpF8LJKHRXIhKw/C\nEQhlBe8RCGfFPocix+qO1se9W+iNr1A42A7HlcW3CSe0tfZlFgr++bAu3ummvrsxSOI7knnvYpz4\nuh5tx+9bfJmIdCTZZJ6VxFgzgc3uvjUYeCFwDRCfeK8BvhBsLwb+28wsKF/o7o3Aq2a2ORiPJMaU\nk9SIwlxmTx/N7Omjgak0tUR59fUjbNx7iMqD9ZTXNLCnpp69B+toOrIfb6ghq7GWIXaEIdRRaEco\noIEcmsi1JvKam8gleFnbdh15VkOuNRGhlYi1kkULEVoJEyVCC1m0kkUrYVoIEyWMjuOnkmN4XOI/\nuh0riLWJ++egXX279rFtA9xIaB8/Pu3at5Ufa0+7f0DajW/H+rYb39rHltNcS6S1DvNWAFpD2TRG\nighHGwlHm2kJ59IaipDsvzQdT7067m1vaB37bEcncN5JO44+OOlY3bF3axdI+5/i0b4WP0Zb0yjh\naBNhbyZqWdQWjCfkLURaDhMNRWiKFAJGKNpMayibkDdjHsUtFIySMF77iDr7McTprkHn9d5ZXQf/\njNaPvohR13+ru2B6XTLJfDSwM+5zJXBBZ23cvcXMaoCSoHx5Qt/RwXZ3YwJgZvOAeQDjxo1LIlxJ\nN9lZIc4aMZizRgzutE1r1Dnc0EJNfTO1Dc3UN7fS0NxKQ3M0eG+loSVKTbDd3Oq0Rp2WqNMajQbv\nwefWTspbWjBvIeStmLfi3kooGsU99gp5FDwK3ooRxd0JRVuDstjLPIoRv90K7rHto+0ciAZ/8xzw\nWJv4P67eltri/6gmljvmANG4dOIQN27sD7e3r49rF59C238H7dse/SN+LJ5O0hzxf3Tbx59QZvF/\nIr3jNp18T/Lt35hsOmsTPw5d7N+x+LtqHyuPEiJElFyaidBCfWsOjc1ZhHBCOLHflhBR7zYTdbiI\n0WEixkn8h6ctf3ti+Ruib5+eu2qbWN5RXXz5IOoZF9rHMA5yiDwO1eTSSphs8mkkQjQ4HztKHiGc\nFvJpJRT8rKJHf1Pjv6/9713nuq/vSmf/QnVcfjgUZXaX4/WNZJJ5Srn7AmABxJbZUxyOpEg4ZBTm\nRyjMj6Q6FAm0HaJzj/tD31ZGfAI51i65cZNo080f5+TG6C6O7gdJZpe6HSbD9tdxBge/M1H3du3f\nsJaQMFhfna/dm+PmZqfmIrFkkvkuYGzc5zFBWUdtKs0sCygkdiJcV327G1NEBjALportZ4w6Bi6S\nCsn8C7ECmGhmE8wsG7gBWJLQZglwc7A9B3jaY/9SLQFuMLMcM5sATAT+keSYIiIikoRuZ+bBMfC7\ngKXELiN72N3Xmdk9QLm7LwEeAn4anOC2n1hyJmi3iNiJbS3Ane6xM0E6GrP3d09EROTkp5vGiIiI\nDFDJXpqm27mKiIikOSVzERGRNJdWy+xmVgVs78UhS4NXI5CT5Ds9aKs+6qM+6qM+vdMnHWJs8yq9\n51R3H9ZdowF/nXm8ZHaoJ8ysnNglcgbkJvlOD9qqj/qoj/qoT+/0SYcYAUjmGHdv0zK7iIhImlMy\nFxERSXNptczeBxYA/wJsInZDm2Te6UFb9VEf9VEf9emdPukQY8qk1QlwIiIi8kZaZhcREUlzSuYi\nIiJpLiOPmZvZL4HrUx2HiIhklN+6+7V9MXCmzsx/CdQGr3jR4L2R3hXtvomIiJwkWok9XAxij2mv\nAyqBq4IniPa6TE3me4BVQE1CeYjYDz7c7xGJiMjJIkwsobfJAhYHZaf3xRdmajIfTSyhl3RQ5/T+\n4YdM/TmLiGSqnODdgGzgruBzYV98WSYnmVFo+VtERPpGW35pW24/Qiyxl/bFl2VqMt8FjAcKOqjL\n1J+JHKObL4jIiWrLJUbsb8qrxBJ7fl9+WaZZAQwFDtP1H+6m/glHBhjrvomISJfqEz7nABHgL33x\nZRl5aRrwFB3PyhNl93UgIiJyUsoL3ttOqD4D+Ia7r+6LL9PtXEVERNJcpi6zi4iInDSUzEVERNKc\nkrmIiEiaUzIXERFJc0rmIiIiaU7JXEREJM0pmYuIiKS5/w8KxbImr876+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21bc2aae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.099 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 1.167 RMSE\n",
      "Real\t:\n",
      " [[ -1.           0.        ]\n",
      " [  0.71000004  19.39999962]\n",
      " [  0.34000003  19.60000038]\n",
      " [ -1.          19.79999924]],\n",
      "Predict\t:\n",
      " [[ -0.99999911   0.12685676]\n",
      " [  0.69991076  16.33728409]\n",
      " [  0.33699822  16.67814064]\n",
      " [ -0.92060983  16.87597466]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in test_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[-1], test_Y_hat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GRU Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  \n",
    "[Attention Structure 3](https://distill.pub/2016/augmented-rnns/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyGRUAttention (Feed-Forward, Not Recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyGRUAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='sigmoid',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyGRUAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, h\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # st = h_tilda\n",
    "        num = len(['reset_gate', 'update_gate', 'h_tilda(proposal)'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"r\"; reset gate\n",
    "        # r = sigmoid(xt * Ur + ht-1 * Wr + br) \n",
    "        # New r = sigmoid(xt * Ur + ht-1 * Wr + br + context * Vr)\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ht_before, self.U[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # now calculate the \"z\"; update gate\n",
    "        # z = sigmoid(xt * Uz + ht-1 * Wz + bz)\n",
    "        # New z = sigmoid(xt * Uz + ht-1 * Wz + bz + context * Vz)\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[1])\n",
    "            + K.dot(ht_before, self.U[1])\n",
    "            + K.dot(context, self.V[1])\n",
    "            + self.b[1])\n",
    "\n",
    "        # calculate the proposal \"g\"; hidden state for now(tilda)\n",
    "        # h_tilda = tanh(xt * Wh + (ht-1 * rt) * Uh + bh)\n",
    "        # New h_tilda = tanh(xt * Wh + (ht-1 * rt) * Uh + bh + context * Vh)\n",
    "        h_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[2])\n",
    "            + K.dot((rt * ht_before), self.U[2])\n",
    "            + K.dot(context, self.V[2])\n",
    "            + self.b[2])\n",
    "\n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        ht = (1 - zt) * h_tilda + zt * ht_before\n",
    "\n",
    "        \n",
    "        # Output Activation\n",
    "        y_ = (K.dot(yt_before, self.W_o)\n",
    "              + K.dot(ht_before, self.U_o)\n",
    "              + K.dot(context, self.C_o)\n",
    "              + self.b_o)\n",
    "\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(y_)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(y_)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(y_)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht]\n",
    "        else:\n",
    "            return yt, [yt, ht]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 64)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 4, 64)             9216      \n",
      "_________________________________________________________________\n",
      "MyGRUAttentionDecoder (MyGRU (None, 4, 2)              14886     \n",
      "=================================================================\n",
      "Total params: 24,102\n",
      "Trainable params: 24,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = seq_X.shape\n",
    "_, timestepY, ndimY = padded_seq_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "i = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = Bidirectional(LSTM(latent_dim, return_sequences=True), merge_mode='concat')(i)\n",
    "dec = MyGRUAttentionDecoder(latent_dim, ndimY)(enc)\n",
    "model = Model(inputs=i, outputs=dec)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/1000\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.1341 - mean_absolute_error: 0.3177 - val_loss: 0.1236 - val_mean_absolute_error: 0.3075\n",
      "Epoch 2/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 0.1274 - mean_absolute_error: 0.3096 - val_loss: 0.1242 - val_mean_absolute_error: 0.3163\n",
      "Epoch 3/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.1211 - mean_absolute_error: 0.3022 - val_loss: 0.1253 - val_mean_absolute_error: 0.3240\n",
      "Epoch 4/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.1156 - mean_absolute_error: 0.2955 - val_loss: 0.1266 - val_mean_absolute_error: 0.3299\n",
      "Epoch 5/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.1109 - mean_absolute_error: 0.2897 - val_loss: 0.1277 - val_mean_absolute_error: 0.3341\n",
      "Epoch 6/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.1067 - mean_absolute_error: 0.2848 - val_loss: 0.1284 - val_mean_absolute_error: 0.3369\n",
      "Epoch 7/1000\n",
      "52/52 [==============================] - 0s 565us/step - loss: 0.1030 - mean_absolute_error: 0.2801 - val_loss: 0.1286 - val_mean_absolute_error: 0.3380\n",
      "Epoch 8/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0997 - mean_absolute_error: 0.2757 - val_loss: 0.1281 - val_mean_absolute_error: 0.3375\n",
      "Epoch 9/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0965 - mean_absolute_error: 0.2714 - val_loss: 0.1269 - val_mean_absolute_error: 0.3356\n",
      "Epoch 10/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0936 - mean_absolute_error: 0.2672 - val_loss: 0.1251 - val_mean_absolute_error: 0.3324\n",
      "Epoch 11/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 0.0907 - mean_absolute_error: 0.2629 - val_loss: 0.1226 - val_mean_absolute_error: 0.3281\n",
      "Epoch 12/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0879 - mean_absolute_error: 0.2585 - val_loss: 0.1197 - val_mean_absolute_error: 0.3228\n",
      "Epoch 13/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0852 - mean_absolute_error: 0.2540 - val_loss: 0.1162 - val_mean_absolute_error: 0.3166\n",
      "Epoch 14/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0826 - mean_absolute_error: 0.2493 - val_loss: 0.1123 - val_mean_absolute_error: 0.3097\n",
      "Epoch 15/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0800 - mean_absolute_error: 0.2446 - val_loss: 0.1080 - val_mean_absolute_error: 0.3020\n",
      "Epoch 16/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0776 - mean_absolute_error: 0.2398 - val_loss: 0.1034 - val_mean_absolute_error: 0.2937\n",
      "Epoch 17/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0752 - mean_absolute_error: 0.2350 - val_loss: 0.0986 - val_mean_absolute_error: 0.2850\n",
      "Epoch 18/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0730 - mean_absolute_error: 0.2302 - val_loss: 0.0936 - val_mean_absolute_error: 0.2758\n",
      "Epoch 19/1000\n",
      "52/52 [==============================] - 0s 565us/step - loss: 0.0708 - mean_absolute_error: 0.2254 - val_loss: 0.0886 - val_mean_absolute_error: 0.2665\n",
      "Epoch 20/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 0.0688 - mean_absolute_error: 0.2208 - val_loss: 0.0838 - val_mean_absolute_error: 0.2570\n",
      "Epoch 21/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0670 - mean_absolute_error: 0.2164 - val_loss: 0.0793 - val_mean_absolute_error: 0.2478\n",
      "Epoch 22/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0654 - mean_absolute_error: 0.2124 - val_loss: 0.0753 - val_mean_absolute_error: 0.2389\n",
      "Epoch 23/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0640 - mean_absolute_error: 0.2088 - val_loss: 0.0719 - val_mean_absolute_error: 0.2308\n",
      "Epoch 24/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0627 - mean_absolute_error: 0.2054 - val_loss: 0.0691 - val_mean_absolute_error: 0.2236\n",
      "Epoch 25/1000\n",
      "52/52 [==============================] - 0s 623us/step - loss: 0.0616 - mean_absolute_error: 0.2023 - val_loss: 0.0670 - val_mean_absolute_error: 0.2174\n",
      "Epoch 26/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0605 - mean_absolute_error: 0.1992 - val_loss: 0.0655 - val_mean_absolute_error: 0.2122\n",
      "Epoch 27/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0595 - mean_absolute_error: 0.1961 - val_loss: 0.0645 - val_mean_absolute_error: 0.2081\n",
      "Epoch 28/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0586 - mean_absolute_error: 0.1930 - val_loss: 0.0639 - val_mean_absolute_error: 0.2048\n",
      "Epoch 29/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0577 - mean_absolute_error: 0.1899 - val_loss: 0.0637 - val_mean_absolute_error: 0.2024\n",
      "Epoch 30/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0568 - mean_absolute_error: 0.1868 - val_loss: 0.0635 - val_mean_absolute_error: 0.2004\n",
      "Epoch 31/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0560 - mean_absolute_error: 0.1839 - val_loss: 0.0634 - val_mean_absolute_error: 0.1986\n",
      "Epoch 32/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0553 - mean_absolute_error: 0.1811 - val_loss: 0.0631 - val_mean_absolute_error: 0.1967\n",
      "Epoch 33/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0547 - mean_absolute_error: 0.1786 - val_loss: 0.0626 - val_mean_absolute_error: 0.1943\n",
      "Epoch 34/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0541 - mean_absolute_error: 0.1762 - val_loss: 0.0617 - val_mean_absolute_error: 0.1915\n",
      "Epoch 35/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0536 - mean_absolute_error: 0.1740 - val_loss: 0.0606 - val_mean_absolute_error: 0.1880\n",
      "Epoch 36/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0531 - mean_absolute_error: 0.1719 - val_loss: 0.0592 - val_mean_absolute_error: 0.1840\n",
      "Epoch 37/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0527 - mean_absolute_error: 0.1700 - val_loss: 0.0578 - val_mean_absolute_error: 0.1797\n",
      "Epoch 38/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0524 - mean_absolute_error: 0.1683 - val_loss: 0.0565 - val_mean_absolute_error: 0.1754\n",
      "Epoch 39/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0521 - mean_absolute_error: 0.1668 - val_loss: 0.0554 - val_mean_absolute_error: 0.1715\n",
      "Epoch 40/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0518 - mean_absolute_error: 0.1653 - val_loss: 0.0545 - val_mean_absolute_error: 0.1682\n",
      "Epoch 41/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0515 - mean_absolute_error: 0.1639 - val_loss: 0.0539 - val_mean_absolute_error: 0.1657\n",
      "Epoch 42/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0512 - mean_absolute_error: 0.1625 - val_loss: 0.0534 - val_mean_absolute_error: 0.1636\n",
      "Epoch 43/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0510 - mean_absolute_error: 0.1611 - val_loss: 0.0530 - val_mean_absolute_error: 0.1620\n",
      "Epoch 44/1000\n",
      "52/52 [==============================] - 0s 565us/step - loss: 0.0507 - mean_absolute_error: 0.1598 - val_loss: 0.0527 - val_mean_absolute_error: 0.1605\n",
      "Epoch 45/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0505 - mean_absolute_error: 0.1585 - val_loss: 0.0524 - val_mean_absolute_error: 0.1588\n",
      "Epoch 46/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0503 - mean_absolute_error: 0.1572 - val_loss: 0.0521 - val_mean_absolute_error: 0.1570\n",
      "Epoch 47/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0501 - mean_absolute_error: 0.1561 - val_loss: 0.0517 - val_mean_absolute_error: 0.1551\n",
      "Epoch 48/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0499 - mean_absolute_error: 0.1551 - val_loss: 0.0513 - val_mean_absolute_error: 0.1534\n",
      "Epoch 49/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0497 - mean_absolute_error: 0.1541 - val_loss: 0.0509 - val_mean_absolute_error: 0.1518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 0.0496 - mean_absolute_error: 0.1532 - val_loss: 0.0506 - val_mean_absolute_error: 0.1504\n",
      "Epoch 51/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 0.0494 - mean_absolute_error: 0.1523 - val_loss: 0.0503 - val_mean_absolute_error: 0.1493\n",
      "Epoch 52/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0493 - mean_absolute_error: 0.1514 - val_loss: 0.0501 - val_mean_absolute_error: 0.1483\n",
      "Epoch 53/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 0.0492 - mean_absolute_error: 0.1506 - val_loss: 0.0500 - val_mean_absolute_error: 0.1475\n",
      "Epoch 54/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0491 - mean_absolute_error: 0.1498 - val_loss: 0.0499 - val_mean_absolute_error: 0.1467\n",
      "Epoch 55/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0489 - mean_absolute_error: 0.1490 - val_loss: 0.0498 - val_mean_absolute_error: 0.1460\n",
      "Epoch 56/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0488 - mean_absolute_error: 0.1483 - val_loss: 0.0497 - val_mean_absolute_error: 0.1453\n",
      "Epoch 57/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 0.0487 - mean_absolute_error: 0.1476 - val_loss: 0.0496 - val_mean_absolute_error: 0.1445\n",
      "Epoch 58/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0487 - mean_absolute_error: 0.1469 - val_loss: 0.0494 - val_mean_absolute_error: 0.1438\n",
      "Epoch 59/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0486 - mean_absolute_error: 0.1462 - val_loss: 0.0493 - val_mean_absolute_error: 0.1431\n",
      "Epoch 60/1000\n",
      "52/52 [==============================] - 0s 564us/step - loss: 0.0485 - mean_absolute_error: 0.1456 - val_loss: 0.0492 - val_mean_absolute_error: 0.1424\n",
      "Epoch 61/1000\n",
      "52/52 [==============================] - 0s 559us/step - loss: 0.0484 - mean_absolute_error: 0.1450 - val_loss: 0.0491 - val_mean_absolute_error: 0.1418\n",
      "Epoch 62/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0484 - mean_absolute_error: 0.1444 - val_loss: 0.0491 - val_mean_absolute_error: 0.1412\n",
      "Epoch 63/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0483 - mean_absolute_error: 0.1438 - val_loss: 0.0490 - val_mean_absolute_error: 0.1407\n",
      "Epoch 64/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0482 - mean_absolute_error: 0.1432 - val_loss: 0.0490 - val_mean_absolute_error: 0.1402\n",
      "Epoch 65/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 0.0482 - mean_absolute_error: 0.1427 - val_loss: 0.0489 - val_mean_absolute_error: 0.1397\n",
      "Epoch 66/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0481 - mean_absolute_error: 0.1421 - val_loss: 0.0489 - val_mean_absolute_error: 0.1392\n",
      "Epoch 67/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 0.0481 - mean_absolute_error: 0.1416 - val_loss: 0.0488 - val_mean_absolute_error: 0.1388\n",
      "Epoch 68/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0480 - mean_absolute_error: 0.1411 - val_loss: 0.0488 - val_mean_absolute_error: 0.1383\n",
      "Epoch 69/1000\n",
      "52/52 [==============================] - 0s 565us/step - loss: 0.0480 - mean_absolute_error: 0.1406 - val_loss: 0.0487 - val_mean_absolute_error: 0.1379\n",
      "Epoch 70/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0479 - mean_absolute_error: 0.1401 - val_loss: 0.0487 - val_mean_absolute_error: 0.1375\n",
      "Epoch 71/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0479 - mean_absolute_error: 0.1396 - val_loss: 0.0487 - val_mean_absolute_error: 0.1372\n",
      "Epoch 72/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0478 - mean_absolute_error: 0.1392 - val_loss: 0.0486 - val_mean_absolute_error: 0.1369\n",
      "Epoch 73/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0478 - mean_absolute_error: 0.1387 - val_loss: 0.0486 - val_mean_absolute_error: 0.1365\n",
      "Epoch 74/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 0.0477 - mean_absolute_error: 0.1383 - val_loss: 0.0485 - val_mean_absolute_error: 0.1362\n",
      "Epoch 75/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0477 - mean_absolute_error: 0.1379 - val_loss: 0.0485 - val_mean_absolute_error: 0.1359\n",
      "Epoch 76/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 0.0477 - mean_absolute_error: 0.1375 - val_loss: 0.0485 - val_mean_absolute_error: 0.1356\n",
      "Epoch 77/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0476 - mean_absolute_error: 0.1371 - val_loss: 0.0484 - val_mean_absolute_error: 0.1354\n",
      "Epoch 78/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 0.0476 - mean_absolute_error: 0.1367 - val_loss: 0.0484 - val_mean_absolute_error: 0.1351\n",
      "Epoch 79/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0476 - mean_absolute_error: 0.1363 - val_loss: 0.0483 - val_mean_absolute_error: 0.1349\n",
      "Epoch 80/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0475 - mean_absolute_error: 0.1359 - val_loss: 0.0483 - val_mean_absolute_error: 0.1346\n",
      "Epoch 81/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0475 - mean_absolute_error: 0.1356 - val_loss: 0.0483 - val_mean_absolute_error: 0.1344\n",
      "Epoch 82/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0475 - mean_absolute_error: 0.1352 - val_loss: 0.0482 - val_mean_absolute_error: 0.1342\n",
      "Epoch 83/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 0.0474 - mean_absolute_error: 0.1349 - val_loss: 0.0482 - val_mean_absolute_error: 0.1339\n",
      "Epoch 84/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 0.0474 - mean_absolute_error: 0.1345 - val_loss: 0.0481 - val_mean_absolute_error: 0.1337\n",
      "Epoch 85/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0474 - mean_absolute_error: 0.1342 - val_loss: 0.0481 - val_mean_absolute_error: 0.1335\n",
      "Epoch 86/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0473 - mean_absolute_error: 0.1339 - val_loss: 0.0481 - val_mean_absolute_error: 0.1333\n",
      "Epoch 87/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 0.0473 - mean_absolute_error: 0.1336 - val_loss: 0.0480 - val_mean_absolute_error: 0.1331\n",
      "Epoch 88/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0473 - mean_absolute_error: 0.1333 - val_loss: 0.0480 - val_mean_absolute_error: 0.1330\n",
      "Epoch 89/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0473 - mean_absolute_error: 0.1330 - val_loss: 0.0480 - val_mean_absolute_error: 0.1328\n",
      "Epoch 90/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0473 - mean_absolute_error: 0.1327 - val_loss: 0.0480 - val_mean_absolute_error: 0.1326\n",
      "Epoch 91/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 0.0472 - mean_absolute_error: 0.1325 - val_loss: 0.0479 - val_mean_absolute_error: 0.1325\n",
      "Epoch 92/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 0.0472 - mean_absolute_error: 0.1322 - val_loss: 0.0479 - val_mean_absolute_error: 0.1323\n",
      "Epoch 93/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0472 - mean_absolute_error: 0.1319 - val_loss: 0.0479 - val_mean_absolute_error: 0.1322\n",
      "Epoch 94/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0472 - mean_absolute_error: 0.1317 - val_loss: 0.0479 - val_mean_absolute_error: 0.1321\n",
      "Epoch 95/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0471 - mean_absolute_error: 0.1314 - val_loss: 0.0478 - val_mean_absolute_error: 0.1320\n",
      "Epoch 96/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0471 - mean_absolute_error: 0.1312 - val_loss: 0.0478 - val_mean_absolute_error: 0.1320\n",
      "Epoch 97/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0471 - mean_absolute_error: 0.1310 - val_loss: 0.0478 - val_mean_absolute_error: 0.1319\n",
      "Epoch 98/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0471 - mean_absolute_error: 0.1307 - val_loss: 0.0478 - val_mean_absolute_error: 0.1319\n",
      "Epoch 99/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 573us/step - loss: 0.0471 - mean_absolute_error: 0.1305 - val_loss: 0.0478 - val_mean_absolute_error: 0.1319\n",
      "Epoch 100/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 0.0471 - mean_absolute_error: 0.1303 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 101/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 0.0470 - mean_absolute_error: 0.1301 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 102/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0470 - mean_absolute_error: 0.1299 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 103/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0470 - mean_absolute_error: 0.1297 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 104/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0470 - mean_absolute_error: 0.1295 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 105/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0470 - mean_absolute_error: 0.1294 - val_loss: 0.0477 - val_mean_absolute_error: 0.1318\n",
      "Epoch 106/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0469 - mean_absolute_error: 0.1292 - val_loss: 0.0476 - val_mean_absolute_error: 0.1318\n",
      "Epoch 107/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0469 - mean_absolute_error: 0.1290 - val_loss: 0.0476 - val_mean_absolute_error: 0.1317\n",
      "Epoch 108/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0469 - mean_absolute_error: 0.1289 - val_loss: 0.0476 - val_mean_absolute_error: 0.1317\n",
      "Epoch 109/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0469 - mean_absolute_error: 0.1287 - val_loss: 0.0476 - val_mean_absolute_error: 0.1318\n",
      "Epoch 110/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 0.0469 - mean_absolute_error: 0.1285 - val_loss: 0.0476 - val_mean_absolute_error: 0.1318\n",
      "Epoch 111/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0469 - mean_absolute_error: 0.1284 - val_loss: 0.0476 - val_mean_absolute_error: 0.1318\n",
      "Epoch 112/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0468 - mean_absolute_error: 0.1283 - val_loss: 0.0476 - val_mean_absolute_error: 0.1318\n",
      "Epoch 113/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0468 - mean_absolute_error: 0.1281 - val_loss: 0.0475 - val_mean_absolute_error: 0.1318\n",
      "Epoch 114/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 0.0468 - mean_absolute_error: 0.1280 - val_loss: 0.0475 - val_mean_absolute_error: 0.1318\n",
      "Epoch 115/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0468 - mean_absolute_error: 0.1278 - val_loss: 0.0475 - val_mean_absolute_error: 0.1319\n",
      "Epoch 116/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 0.0468 - mean_absolute_error: 0.1277 - val_loss: 0.0475 - val_mean_absolute_error: 0.1319\n",
      "Epoch 117/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 0.0467 - mean_absolute_error: 0.1276 - val_loss: 0.0475 - val_mean_absolute_error: 0.1319\n",
      "Epoch 118/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0467 - mean_absolute_error: 0.1274 - val_loss: 0.0475 - val_mean_absolute_error: 0.1319\n",
      "Epoch 119/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0467 - mean_absolute_error: 0.1273 - val_loss: 0.0475 - val_mean_absolute_error: 0.1319\n",
      "Epoch 120/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0467 - mean_absolute_error: 0.1272 - val_loss: 0.0474 - val_mean_absolute_error: 0.1320\n",
      "Epoch 121/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0467 - mean_absolute_error: 0.1271 - val_loss: 0.0474 - val_mean_absolute_error: 0.1320\n",
      "Epoch 122/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0466 - mean_absolute_error: 0.1270 - val_loss: 0.0474 - val_mean_absolute_error: 0.1320\n",
      "Epoch 123/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0466 - mean_absolute_error: 0.1269 - val_loss: 0.0474 - val_mean_absolute_error: 0.1320\n",
      "Epoch 124/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 0.0466 - mean_absolute_error: 0.1267 - val_loss: 0.0474 - val_mean_absolute_error: 0.1321\n",
      "Epoch 125/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0466 - mean_absolute_error: 0.1266 - val_loss: 0.0474 - val_mean_absolute_error: 0.1321\n",
      "Epoch 126/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0466 - mean_absolute_error: 0.1265 - val_loss: 0.0474 - val_mean_absolute_error: 0.1321\n",
      "Epoch 127/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 0.0465 - mean_absolute_error: 0.1264 - val_loss: 0.0473 - val_mean_absolute_error: 0.1322\n",
      "Epoch 128/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0465 - mean_absolute_error: 0.1263 - val_loss: 0.0473 - val_mean_absolute_error: 0.1322\n",
      "Epoch 129/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0465 - mean_absolute_error: 0.1262 - val_loss: 0.0473 - val_mean_absolute_error: 0.1322\n",
      "Epoch 130/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 0.0465 - mean_absolute_error: 0.1261 - val_loss: 0.0473 - val_mean_absolute_error: 0.1323\n",
      "Epoch 131/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 0.0464 - mean_absolute_error: 0.1260 - val_loss: 0.0473 - val_mean_absolute_error: 0.1323\n",
      "Epoch 132/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 0.0464 - mean_absolute_error: 0.1259 - val_loss: 0.0473 - val_mean_absolute_error: 0.1323\n",
      "Epoch 133/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 0.0464 - mean_absolute_error: 0.1258 - val_loss: 0.0472 - val_mean_absolute_error: 0.1324\n",
      "Epoch 134/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0463 - mean_absolute_error: 0.1258 - val_loss: 0.0472 - val_mean_absolute_error: 0.1324\n",
      "Epoch 135/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0463 - mean_absolute_error: 0.1257 - val_loss: 0.0472 - val_mean_absolute_error: 0.1324\n",
      "Epoch 136/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0463 - mean_absolute_error: 0.1256 - val_loss: 0.0472 - val_mean_absolute_error: 0.1324\n",
      "Epoch 137/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0462 - mean_absolute_error: 0.1255 - val_loss: 0.0471 - val_mean_absolute_error: 0.1325\n",
      "Epoch 138/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0462 - mean_absolute_error: 0.1254 - val_loss: 0.0471 - val_mean_absolute_error: 0.1325\n",
      "Epoch 139/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0461 - mean_absolute_error: 0.1253 - val_loss: 0.0471 - val_mean_absolute_error: 0.1325\n",
      "Epoch 140/1000\n",
      "52/52 [==============================] - 0s 565us/step - loss: 0.0461 - mean_absolute_error: 0.1252 - val_loss: 0.0470 - val_mean_absolute_error: 0.1325\n",
      "Epoch 141/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 0.0460 - mean_absolute_error: 0.1251 - val_loss: 0.0470 - val_mean_absolute_error: 0.1326\n",
      "Epoch 142/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 0.0460 - mean_absolute_error: 0.1250 - val_loss: 0.0470 - val_mean_absolute_error: 0.1326\n",
      "Epoch 143/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0459 - mean_absolute_error: 0.1249 - val_loss: 0.0469 - val_mean_absolute_error: 0.1326\n",
      "Epoch 144/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0459 - mean_absolute_error: 0.1248 - val_loss: 0.0469 - val_mean_absolute_error: 0.1326\n",
      "Epoch 145/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0458 - mean_absolute_error: 0.1247 - val_loss: 0.0468 - val_mean_absolute_error: 0.1326\n",
      "Epoch 146/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0457 - mean_absolute_error: 0.1245 - val_loss: 0.0467 - val_mean_absolute_error: 0.1326\n",
      "Epoch 147/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 0.0456 - mean_absolute_error: 0.1244 - val_loss: 0.0467 - val_mean_absolute_error: 0.1326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 0.0456 - mean_absolute_error: 0.1243 - val_loss: 0.0466 - val_mean_absolute_error: 0.1326\n",
      "Epoch 149/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0455 - mean_absolute_error: 0.1242 - val_loss: 0.0465 - val_mean_absolute_error: 0.1325\n",
      "Epoch 150/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 0.0453 - mean_absolute_error: 0.1240 - val_loss: 0.0464 - val_mean_absolute_error: 0.1325\n",
      "Epoch 151/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0452 - mean_absolute_error: 0.1238 - val_loss: 0.0463 - val_mean_absolute_error: 0.1324\n",
      "Epoch 152/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0451 - mean_absolute_error: 0.1237 - val_loss: 0.0462 - val_mean_absolute_error: 0.1323\n",
      "Epoch 153/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0449 - mean_absolute_error: 0.1235 - val_loss: 0.0460 - val_mean_absolute_error: 0.1322\n",
      "Epoch 154/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0448 - mean_absolute_error: 0.1233 - val_loss: 0.0459 - val_mean_absolute_error: 0.1320\n",
      "Epoch 155/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0446 - mean_absolute_error: 0.1230 - val_loss: 0.0457 - val_mean_absolute_error: 0.1318\n",
      "Epoch 156/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 0.0444 - mean_absolute_error: 0.1228 - val_loss: 0.0455 - val_mean_absolute_error: 0.1316\n",
      "Epoch 157/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0441 - mean_absolute_error: 0.1225 - val_loss: 0.0452 - val_mean_absolute_error: 0.1313\n",
      "Epoch 158/1000\n",
      "52/52 [==============================] - 0s 622us/step - loss: 0.0438 - mean_absolute_error: 0.1221 - val_loss: 0.0449 - val_mean_absolute_error: 0.1310\n",
      "Epoch 159/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0435 - mean_absolute_error: 0.1218 - val_loss: 0.0446 - val_mean_absolute_error: 0.1305\n",
      "Epoch 160/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0431 - mean_absolute_error: 0.1213 - val_loss: 0.0442 - val_mean_absolute_error: 0.1300\n",
      "Epoch 161/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0426 - mean_absolute_error: 0.1208 - val_loss: 0.0437 - val_mean_absolute_error: 0.1293\n",
      "Epoch 162/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0421 - mean_absolute_error: 0.1202 - val_loss: 0.0431 - val_mean_absolute_error: 0.1286\n",
      "Epoch 163/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0415 - mean_absolute_error: 0.1194 - val_loss: 0.0424 - val_mean_absolute_error: 0.1277\n",
      "Epoch 164/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0407 - mean_absolute_error: 0.1186 - val_loss: 0.0416 - val_mean_absolute_error: 0.1267\n",
      "Epoch 165/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0398 - mean_absolute_error: 0.1175 - val_loss: 0.0406 - val_mean_absolute_error: 0.1256\n",
      "Epoch 166/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0387 - mean_absolute_error: 0.1163 - val_loss: 0.0395 - val_mean_absolute_error: 0.1242\n",
      "Epoch 167/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0375 - mean_absolute_error: 0.1148 - val_loss: 0.0382 - val_mean_absolute_error: 0.1227\n",
      "Epoch 168/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0360 - mean_absolute_error: 0.1130 - val_loss: 0.0368 - val_mean_absolute_error: 0.1211\n",
      "Epoch 169/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0342 - mean_absolute_error: 0.1110 - val_loss: 0.0353 - val_mean_absolute_error: 0.1193\n",
      "Epoch 170/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 0.0323 - mean_absolute_error: 0.1086 - val_loss: 0.0336 - val_mean_absolute_error: 0.1172\n",
      "Epoch 171/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 0.0303 - mean_absolute_error: 0.1059 - val_loss: 0.0318 - val_mean_absolute_error: 0.1149\n",
      "Epoch 172/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0281 - mean_absolute_error: 0.1029 - val_loss: 0.0300 - val_mean_absolute_error: 0.1119\n",
      "Epoch 173/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0259 - mean_absolute_error: 0.0997 - val_loss: 0.0280 - val_mean_absolute_error: 0.1084\n",
      "Epoch 174/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0237 - mean_absolute_error: 0.0963 - val_loss: 0.0259 - val_mean_absolute_error: 0.1044\n",
      "Epoch 175/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 0.0216 - mean_absolute_error: 0.0929 - val_loss: 0.0238 - val_mean_absolute_error: 0.0999\n",
      "Epoch 176/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 0.0197 - mean_absolute_error: 0.0894 - val_loss: 0.0217 - val_mean_absolute_error: 0.0954\n",
      "Epoch 177/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0179 - mean_absolute_error: 0.0857 - val_loss: 0.0197 - val_mean_absolute_error: 0.0907\n",
      "Epoch 178/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0163 - mean_absolute_error: 0.0821 - val_loss: 0.0177 - val_mean_absolute_error: 0.0858\n",
      "Epoch 179/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0148 - mean_absolute_error: 0.0786 - val_loss: 0.0159 - val_mean_absolute_error: 0.0815\n",
      "Epoch 180/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0135 - mean_absolute_error: 0.0754 - val_loss: 0.0144 - val_mean_absolute_error: 0.0788\n",
      "Epoch 181/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0123 - mean_absolute_error: 0.0724 - val_loss: 0.0131 - val_mean_absolute_error: 0.0763\n",
      "Epoch 182/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0113 - mean_absolute_error: 0.0698 - val_loss: 0.0121 - val_mean_absolute_error: 0.0741\n",
      "Epoch 183/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 0.0104 - mean_absolute_error: 0.0674 - val_loss: 0.0112 - val_mean_absolute_error: 0.0721\n",
      "Epoch 184/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0096 - mean_absolute_error: 0.0652 - val_loss: 0.0105 - val_mean_absolute_error: 0.0701\n",
      "Epoch 185/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0089 - mean_absolute_error: 0.0631 - val_loss: 0.0099 - val_mean_absolute_error: 0.0682\n",
      "Epoch 186/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0082 - mean_absolute_error: 0.0610 - val_loss: 0.0093 - val_mean_absolute_error: 0.0662\n",
      "Epoch 187/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0076 - mean_absolute_error: 0.0590 - val_loss: 0.0088 - val_mean_absolute_error: 0.0643\n",
      "Epoch 188/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 0.0070 - mean_absolute_error: 0.0569 - val_loss: 0.0083 - val_mean_absolute_error: 0.0624\n",
      "Epoch 189/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 0.0065 - mean_absolute_error: 0.0549 - val_loss: 0.0078 - val_mean_absolute_error: 0.0608\n",
      "Epoch 190/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 0.0060 - mean_absolute_error: 0.0528 - val_loss: 0.0074 - val_mean_absolute_error: 0.0591\n",
      "Epoch 191/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0056 - mean_absolute_error: 0.0509 - val_loss: 0.0070 - val_mean_absolute_error: 0.0575\n",
      "Epoch 192/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 0.0052 - mean_absolute_error: 0.0491 - val_loss: 0.0066 - val_mean_absolute_error: 0.0559\n",
      "Epoch 193/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0048 - mean_absolute_error: 0.0474 - val_loss: 0.0062 - val_mean_absolute_error: 0.0544\n",
      "Epoch 194/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 0.0045 - mean_absolute_error: 0.0458 - val_loss: 0.0059 - val_mean_absolute_error: 0.0531\n",
      "Epoch 195/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0042 - mean_absolute_error: 0.0443 - val_loss: 0.0056 - val_mean_absolute_error: 0.0520\n",
      "Epoch 196/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0039 - mean_absolute_error: 0.0429 - val_loss: 0.0053 - val_mean_absolute_error: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 0.0036 - mean_absolute_error: 0.0415 - val_loss: 0.0050 - val_mean_absolute_error: 0.0497\n",
      "Epoch 198/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0034 - mean_absolute_error: 0.0402 - val_loss: 0.0048 - val_mean_absolute_error: 0.0487\n",
      "Epoch 199/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 0.0032 - mean_absolute_error: 0.0389 - val_loss: 0.0047 - val_mean_absolute_error: 0.0479\n",
      "Epoch 200/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 0.0030 - mean_absolute_error: 0.0377 - val_loss: 0.0045 - val_mean_absolute_error: 0.0471\n",
      "Epoch 201/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 0.0029 - mean_absolute_error: 0.0366 - val_loss: 0.0044 - val_mean_absolute_error: 0.0465\n",
      "Epoch 202/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 0.0027 - mean_absolute_error: 0.0355 - val_loss: 0.0043 - val_mean_absolute_error: 0.0458\n",
      "Epoch 203/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 0.0026 - mean_absolute_error: 0.0345 - val_loss: 0.0042 - val_mean_absolute_error: 0.0452\n",
      "Epoch 204/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 0.0024 - mean_absolute_error: 0.0335 - val_loss: 0.0041 - val_mean_absolute_error: 0.0446\n",
      "Epoch 205/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 0.0023 - mean_absolute_error: 0.0326 - val_loss: 0.0040 - val_mean_absolute_error: 0.0440\n",
      "Epoch 206/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 0.0022 - mean_absolute_error: 0.0317 - val_loss: 0.0039 - val_mean_absolute_error: 0.0436\n",
      "Epoch 207/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0021 - mean_absolute_error: 0.0309 - val_loss: 0.0039 - val_mean_absolute_error: 0.0431\n",
      "Epoch 208/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0020 - mean_absolute_error: 0.0302 - val_loss: 0.0038 - val_mean_absolute_error: 0.0426\n",
      "Epoch 209/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 0.0019 - mean_absolute_error: 0.0295 - val_loss: 0.0037 - val_mean_absolute_error: 0.0422\n",
      "Epoch 210/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0018 - mean_absolute_error: 0.0289 - val_loss: 0.0036 - val_mean_absolute_error: 0.0417\n",
      "Epoch 211/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 0.0017 - mean_absolute_error: 0.0283 - val_loss: 0.0036 - val_mean_absolute_error: 0.0412\n",
      "Epoch 212/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 0.0016 - mean_absolute_error: 0.0277 - val_loss: 0.0035 - val_mean_absolute_error: 0.0407\n",
      "Epoch 213/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 0.0016 - mean_absolute_error: 0.0271 - val_loss: 0.0034 - val_mean_absolute_error: 0.0403\n",
      "Epoch 214/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0015 - mean_absolute_error: 0.0266 - val_loss: 0.0033 - val_mean_absolute_error: 0.0398\n",
      "Epoch 215/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 0.0015 - mean_absolute_error: 0.0261 - val_loss: 0.0033 - val_mean_absolute_error: 0.0393\n",
      "Epoch 216/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0014 - mean_absolute_error: 0.0257 - val_loss: 0.0032 - val_mean_absolute_error: 0.0387\n",
      "Epoch 217/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 0.0014 - mean_absolute_error: 0.0252 - val_loss: 0.0031 - val_mean_absolute_error: 0.0382\n",
      "Epoch 218/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 0.0013 - mean_absolute_error: 0.0248 - val_loss: 0.0030 - val_mean_absolute_error: 0.0377\n",
      "Epoch 219/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 0.0013 - mean_absolute_error: 0.0244 - val_loss: 0.0030 - val_mean_absolute_error: 0.0373\n",
      "Epoch 220/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 0.0012 - mean_absolute_error: 0.0241 - val_loss: 0.0029 - val_mean_absolute_error: 0.0368\n",
      "Epoch 221/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 0.0012 - mean_absolute_error: 0.0237 - val_loss: 0.0028 - val_mean_absolute_error: 0.0364\n",
      "Epoch 222/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 0.0012 - mean_absolute_error: 0.0234 - val_loss: 0.0028 - val_mean_absolute_error: 0.0361\n",
      "Epoch 223/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 0.0011 - mean_absolute_error: 0.0230 - val_loss: 0.0027 - val_mean_absolute_error: 0.0357\n",
      "Epoch 224/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0011 - mean_absolute_error: 0.0227 - val_loss: 0.0027 - val_mean_absolute_error: 0.0354\n",
      "Epoch 225/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 0.0011 - mean_absolute_error: 0.0224 - val_loss: 0.0026 - val_mean_absolute_error: 0.0350\n",
      "Epoch 226/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 0.0010 - mean_absolute_error: 0.0221 - val_loss: 0.0026 - val_mean_absolute_error: 0.0347\n",
      "Epoch 227/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 0.0010 - mean_absolute_error: 0.0218 - val_loss: 0.0026 - val_mean_absolute_error: 0.0343\n",
      "Epoch 228/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 9.8963e-04 - mean_absolute_error: 0.0216 - val_loss: 0.0025 - val_mean_absolute_error: 0.0340\n",
      "Epoch 229/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 9.6748e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0025 - val_mean_absolute_error: 0.0338\n",
      "Epoch 230/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 9.4667e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0024 - val_mean_absolute_error: 0.0335\n",
      "Epoch 231/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 9.2720e-04 - mean_absolute_error: 0.0208 - val_loss: 0.0024 - val_mean_absolute_error: 0.0333\n",
      "Epoch 232/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 9.0882e-04 - mean_absolute_error: 0.0206 - val_loss: 0.0024 - val_mean_absolute_error: 0.0330\n",
      "Epoch 233/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 8.9157e-04 - mean_absolute_error: 0.0204 - val_loss: 0.0023 - val_mean_absolute_error: 0.0328\n",
      "Epoch 234/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 8.7526e-04 - mean_absolute_error: 0.0202 - val_loss: 0.0023 - val_mean_absolute_error: 0.0325\n",
      "Epoch 235/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 8.5996e-04 - mean_absolute_error: 0.0200 - val_loss: 0.0023 - val_mean_absolute_error: 0.0323\n",
      "Epoch 236/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 8.4552e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0022 - val_mean_absolute_error: 0.0322\n",
      "Epoch 237/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 8.3187e-04 - mean_absolute_error: 0.0197 - val_loss: 0.0022 - val_mean_absolute_error: 0.0320\n",
      "Epoch 238/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 8.1902e-04 - mean_absolute_error: 0.0195 - val_loss: 0.0022 - val_mean_absolute_error: 0.0319\n",
      "Epoch 239/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 8.0682e-04 - mean_absolute_error: 0.0194 - val_loss: 0.0022 - val_mean_absolute_error: 0.0317\n",
      "Epoch 240/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 7.9528e-04 - mean_absolute_error: 0.0192 - val_loss: 0.0021 - val_mean_absolute_error: 0.0316\n",
      "Epoch 241/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 7.8436e-04 - mean_absolute_error: 0.0191 - val_loss: 0.0021 - val_mean_absolute_error: 0.0315\n",
      "Epoch 242/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 7.7397e-04 - mean_absolute_error: 0.0190 - val_loss: 0.0021 - val_mean_absolute_error: 0.0314\n",
      "Epoch 243/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 7.6411e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0021 - val_mean_absolute_error: 0.0312\n",
      "Epoch 244/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 7.5471e-04 - mean_absolute_error: 0.0187 - val_loss: 0.0020 - val_mean_absolute_error: 0.0311\n",
      "Epoch 245/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 7.4575e-04 - mean_absolute_error: 0.0186 - val_loss: 0.0020 - val_mean_absolute_error: 0.0310\n",
      "Epoch 246/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 7.3725e-04 - mean_absolute_error: 0.0185 - val_loss: 0.0020 - val_mean_absolute_error: 0.0309\n",
      "Epoch 247/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 7.2911e-04 - mean_absolute_error: 0.0184 - val_loss: 0.0020 - val_mean_absolute_error: 0.0308\n",
      "Epoch 248/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 7.2130e-04 - mean_absolute_error: 0.0183 - val_loss: 0.0020 - val_mean_absolute_error: 0.0307\n",
      "Epoch 249/1000\n",
      "52/52 [==============================] - 0s 621us/step - loss: 7.1388e-04 - mean_absolute_error: 0.0182 - val_loss: 0.0019 - val_mean_absolute_error: 0.0306\n",
      "Epoch 250/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 7.0678e-04 - mean_absolute_error: 0.0181 - val_loss: 0.0019 - val_mean_absolute_error: 0.0305\n",
      "Epoch 251/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 6.9996e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0019 - val_mean_absolute_error: 0.0304\n",
      "Epoch 252/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 6.9345e-04 - mean_absolute_error: 0.0179 - val_loss: 0.0019 - val_mean_absolute_error: 0.0303\n",
      "Epoch 253/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 6.8720e-04 - mean_absolute_error: 0.0178 - val_loss: 0.0019 - val_mean_absolute_error: 0.0302\n",
      "Epoch 254/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 6.8117e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0019 - val_mean_absolute_error: 0.0301\n",
      "Epoch 255/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.7538e-04 - mean_absolute_error: 0.0177 - val_loss: 0.0018 - val_mean_absolute_error: 0.0300\n",
      "Epoch 256/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 6.6982e-04 - mean_absolute_error: 0.0176 - val_loss: 0.0018 - val_mean_absolute_error: 0.0299\n",
      "Epoch 257/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.6445e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0018 - val_mean_absolute_error: 0.0298\n",
      "Epoch 258/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 6.5920e-04 - mean_absolute_error: 0.0174 - val_loss: 0.0018 - val_mean_absolute_error: 0.0297\n",
      "Epoch 259/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 6.5419e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0018 - val_mean_absolute_error: 0.0296\n",
      "Epoch 260/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 6.4929e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0018 - val_mean_absolute_error: 0.0295\n",
      "Epoch 261/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 6.4451e-04 - mean_absolute_error: 0.0172 - val_loss: 0.0018 - val_mean_absolute_error: 0.0294\n",
      "Epoch 262/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 6.3994e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0017 - val_mean_absolute_error: 0.0294\n",
      "Epoch 263/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 6.3550e-04 - mean_absolute_error: 0.0171 - val_loss: 0.0017 - val_mean_absolute_error: 0.0293\n",
      "Epoch 264/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 6.3119e-04 - mean_absolute_error: 0.0170 - val_loss: 0.0017 - val_mean_absolute_error: 0.0292\n",
      "Epoch 265/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 6.2701e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0017 - val_mean_absolute_error: 0.0291\n",
      "Epoch 266/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 6.2295e-04 - mean_absolute_error: 0.0169 - val_loss: 0.0017 - val_mean_absolute_error: 0.0291\n",
      "Epoch 267/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 6.1895e-04 - mean_absolute_error: 0.0168 - val_loss: 0.0017 - val_mean_absolute_error: 0.0290\n",
      "Epoch 268/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 6.1510e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0017 - val_mean_absolute_error: 0.0289\n",
      "Epoch 269/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 6.1132e-04 - mean_absolute_error: 0.0167 - val_loss: 0.0017 - val_mean_absolute_error: 0.0289\n",
      "Epoch 270/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 6.0760e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0017 - val_mean_absolute_error: 0.0288\n",
      "Epoch 271/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.0401e-04 - mean_absolute_error: 0.0166 - val_loss: 0.0016 - val_mean_absolute_error: 0.0287\n",
      "Epoch 272/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 6.0050e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0016 - val_mean_absolute_error: 0.0287\n",
      "Epoch 273/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 5.9705e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0016 - val_mean_absolute_error: 0.0286\n",
      "Epoch 274/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 5.9370e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0016 - val_mean_absolute_error: 0.0286\n",
      "Epoch 275/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 5.9042e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0016 - val_mean_absolute_error: 0.0285\n",
      "Epoch 276/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 5.8719e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0016 - val_mean_absolute_error: 0.0284\n",
      "Epoch 277/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 5.8403e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0016 - val_mean_absolute_error: 0.0284\n",
      "Epoch 278/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 5.8096e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0016 - val_mean_absolute_error: 0.0283\n",
      "Epoch 279/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 5.7794e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0016 - val_mean_absolute_error: 0.0283\n",
      "Epoch 280/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 5.7498e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0016 - val_mean_absolute_error: 0.0283\n",
      "Epoch 281/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 5.7208e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0016 - val_mean_absolute_error: 0.0282\n",
      "Epoch 282/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 5.6920e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0016 - val_mean_absolute_error: 0.0282\n",
      "Epoch 283/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 5.6641e-04 - mean_absolute_error: 0.0160 - val_loss: 0.0015 - val_mean_absolute_error: 0.0281\n",
      "Epoch 284/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 5.6366e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0015 - val_mean_absolute_error: 0.0281\n",
      "Epoch 285/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 5.6095e-04 - mean_absolute_error: 0.0159 - val_loss: 0.0015 - val_mean_absolute_error: 0.0280\n",
      "Epoch 286/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 5.5832e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0015 - val_mean_absolute_error: 0.0280\n",
      "Epoch 287/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 5.5573e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0015 - val_mean_absolute_error: 0.0280\n",
      "Epoch 288/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 5.5316e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0015 - val_mean_absolute_error: 0.0279\n",
      "Epoch 289/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 5.5063e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0015 - val_mean_absolute_error: 0.0279\n",
      "Epoch 290/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 5.4817e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0015 - val_mean_absolute_error: 0.0279\n",
      "Epoch 291/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 5.4574e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0015 - val_mean_absolute_error: 0.0278\n",
      "Epoch 292/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 5.4331e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0015 - val_mean_absolute_error: 0.0278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 5.4094e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0015 - val_mean_absolute_error: 0.0278\n",
      "Epoch 294/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 5.3862e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0015 - val_mean_absolute_error: 0.0277\n",
      "Epoch 295/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 5.3634e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0015 - val_mean_absolute_error: 0.0277\n",
      "Epoch 296/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 5.3406e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0015 - val_mean_absolute_error: 0.0277\n",
      "Epoch 297/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 5.3185e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0015 - val_mean_absolute_error: 0.0276\n",
      "Epoch 298/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 5.2966e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0015 - val_mean_absolute_error: 0.0276\n",
      "Epoch 299/1000\n",
      "52/52 [==============================] - 0s 617us/step - loss: 5.2746e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0014 - val_mean_absolute_error: 0.0276\n",
      "Epoch 300/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 5.2531e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0014 - val_mean_absolute_error: 0.0275\n",
      "Epoch 301/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 5.2319e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0014 - val_mean_absolute_error: 0.0275\n",
      "Epoch 302/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 5.2111e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0014 - val_mean_absolute_error: 0.0275\n",
      "Epoch 303/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 5.1907e-04 - mean_absolute_error: 0.0152 - val_loss: 0.0014 - val_mean_absolute_error: 0.0274\n",
      "Epoch 304/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 5.1704e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0014 - val_mean_absolute_error: 0.0274\n",
      "Epoch 305/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 5.1506e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0014 - val_mean_absolute_error: 0.0274\n",
      "Epoch 306/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 5.1308e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0014 - val_mean_absolute_error: 0.0273\n",
      "Epoch 307/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 5.1115e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0014 - val_mean_absolute_error: 0.0273\n",
      "Epoch 308/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 5.0921e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0014 - val_mean_absolute_error: 0.0273\n",
      "Epoch 309/1000\n",
      "52/52 [==============================] - 0s 566us/step - loss: 5.0729e-04 - mean_absolute_error: 0.0150 - val_loss: 0.0014 - val_mean_absolute_error: 0.0273\n",
      "Epoch 310/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 5.0546e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0014 - val_mean_absolute_error: 0.0272\n",
      "Epoch 311/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 5.0358e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0014 - val_mean_absolute_error: 0.0272\n",
      "Epoch 312/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 5.0176e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0014 - val_mean_absolute_error: 0.0272\n",
      "Epoch 313/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 4.9998e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0014 - val_mean_absolute_error: 0.0271\n",
      "Epoch 314/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 4.9818e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0014 - val_mean_absolute_error: 0.0271\n",
      "Epoch 315/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 4.9639e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0014 - val_mean_absolute_error: 0.0271\n",
      "Epoch 316/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 4.9463e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0014 - val_mean_absolute_error: 0.0270\n",
      "Epoch 317/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 4.9291e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0014 - val_mean_absolute_error: 0.0270\n",
      "Epoch 318/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 4.9120e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0014 - val_mean_absolute_error: 0.0270\n",
      "Epoch 319/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 4.8949e-04 - mean_absolute_error: 0.0147 - val_loss: 0.0014 - val_mean_absolute_error: 0.0270\n",
      "Epoch 320/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 4.8783e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0014 - val_mean_absolute_error: 0.0269\n",
      "Epoch 321/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 4.8616e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0014 - val_mean_absolute_error: 0.0269\n",
      "Epoch 322/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 4.8453e-04 - mean_absolute_error: 0.0146 - val_loss: 0.0014 - val_mean_absolute_error: 0.0269\n",
      "Epoch 323/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 4.8290e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0013 - val_mean_absolute_error: 0.0268\n",
      "Epoch 324/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 4.8130e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0013 - val_mean_absolute_error: 0.0268\n",
      "Epoch 325/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 4.7971e-04 - mean_absolute_error: 0.0145 - val_loss: 0.0013 - val_mean_absolute_error: 0.0268\n",
      "Epoch 326/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 4.7810e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0013 - val_mean_absolute_error: 0.0267\n",
      "Epoch 327/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 4.7654e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0013 - val_mean_absolute_error: 0.0267\n",
      "Epoch 328/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 4.7500e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0013 - val_mean_absolute_error: 0.0267\n",
      "Epoch 329/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 4.7344e-04 - mean_absolute_error: 0.0144 - val_loss: 0.0013 - val_mean_absolute_error: 0.0267\n",
      "Epoch 330/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 4.7192e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0013 - val_mean_absolute_error: 0.0266\n",
      "Epoch 331/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 4.7040e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0013 - val_mean_absolute_error: 0.0266\n",
      "Epoch 332/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 4.6890e-04 - mean_absolute_error: 0.0143 - val_loss: 0.0013 - val_mean_absolute_error: 0.0266\n",
      "Epoch 333/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 4.6740e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0013 - val_mean_absolute_error: 0.0265\n",
      "Epoch 334/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 4.6593e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0013 - val_mean_absolute_error: 0.0265\n",
      "Epoch 335/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 4.6449e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0013 - val_mean_absolute_error: 0.0265\n",
      "Epoch 336/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 4.6302e-04 - mean_absolute_error: 0.0142 - val_loss: 0.0013 - val_mean_absolute_error: 0.0265\n",
      "Epoch 337/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 4.6158e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0013 - val_mean_absolute_error: 0.0264\n",
      "Epoch 338/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 4.6018e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0013 - val_mean_absolute_error: 0.0264\n",
      "Epoch 339/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 4.5873e-04 - mean_absolute_error: 0.0141 - val_loss: 0.0013 - val_mean_absolute_error: 0.0264\n",
      "Epoch 340/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 4.5731e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0013 - val_mean_absolute_error: 0.0264\n",
      "Epoch 341/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 4.5593e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0013 - val_mean_absolute_error: 0.0263\n",
      "Epoch 342/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 4.5455e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0013 - val_mean_absolute_error: 0.0263\n",
      "Epoch 343/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 4.5314e-04 - mean_absolute_error: 0.0140 - val_loss: 0.0013 - val_mean_absolute_error: 0.0263\n",
      "Epoch 344/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 4.5178e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0013 - val_mean_absolute_error: 0.0263\n",
      "Epoch 345/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 4.5042e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0013 - val_mean_absolute_error: 0.0263\n",
      "Epoch 346/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 4.4906e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 347/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 4.4771e-04 - mean_absolute_error: 0.0139 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 348/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 4.4637e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 349/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 4.4502e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 350/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 4.4369e-04 - mean_absolute_error: 0.0138 - val_loss: 0.0013 - val_mean_absolute_error: 0.0262\n",
      "Epoch 351/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 4.4235e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 352/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 4.4102e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 353/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 4.3973e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 354/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 4.3841e-04 - mean_absolute_error: 0.0137 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 355/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 4.3712e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0013 - val_mean_absolute_error: 0.0261\n",
      "Epoch 356/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 4.3584e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 357/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 4.3454e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 358/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 4.3327e-04 - mean_absolute_error: 0.0136 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 359/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 4.3199e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 360/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 4.3072e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 361/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 4.2946e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0012 - val_mean_absolute_error: 0.0259\n",
      "Epoch 362/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 4.2819e-04 - mean_absolute_error: 0.0135 - val_loss: 0.0012 - val_mean_absolute_error: 0.0259\n",
      "Epoch 363/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 4.2696e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0012 - val_mean_absolute_error: 0.0259\n",
      "Epoch 364/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 4.2570e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0012 - val_mean_absolute_error: 0.0259\n",
      "Epoch 365/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 4.2447e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0012 - val_mean_absolute_error: 0.0259\n",
      "Epoch 366/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 4.2323e-04 - mean_absolute_error: 0.0134 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 367/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 4.2202e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 368/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 4.2079e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 369/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 4.1956e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 370/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 4.1836e-04 - mean_absolute_error: 0.0133 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 371/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 4.1715e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 372/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 4.1594e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0012 - val_mean_absolute_error: 0.0257\n",
      "Epoch 373/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 4.1475e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0012 - val_mean_absolute_error: 0.0257\n",
      "Epoch 374/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 4.1355e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0012 - val_mean_absolute_error: 0.0257\n",
      "Epoch 375/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 4.1238e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0012 - val_mean_absolute_error: 0.0257\n",
      "Epoch 376/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 4.1119e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0012 - val_mean_absolute_error: 0.0257\n",
      "Epoch 377/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 4.1003e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 378/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 4.0887e-04 - mean_absolute_error: 0.0131 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 379/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 4.0768e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 380/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 4.0655e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 381/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 4.0539e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 382/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 4.0424e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 383/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 4.0310e-04 - mean_absolute_error: 0.0130 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 384/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 4.0196e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 385/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 4.0084e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 386/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.9969e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 387/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 3.9861e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 3.9745e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 389/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.9635e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 390/1000\n",
      "52/52 [==============================] - 0s 566us/step - loss: 3.9522e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 391/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 3.9413e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 392/1000\n",
      "52/52 [==============================] - 0s 564us/step - loss: 3.9304e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 393/1000\n",
      "52/52 [==============================] - ETA: 0s - loss: 4.2105e-04 - mean_absolute_error: 0.013 - 0s 573us/step - loss: 3.9193e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 394/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 3.9088e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 395/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.8978e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 396/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 3.8872e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0012 - val_mean_absolute_error: 0.0254\n",
      "Epoch 397/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 3.8763e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 398/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.8659e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 399/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 3.8553e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 400/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 3.8446e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 401/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 3.8343e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 402/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 3.8240e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 403/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 3.8137e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 404/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 3.8035e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 405/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 3.7932e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 406/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 3.7830e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 407/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.7727e-04 - mean_absolute_error: 0.0125 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 408/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 3.7628e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 409/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.7526e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 410/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.7426e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 411/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.7328e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 412/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 3.7227e-04 - mean_absolute_error: 0.0124 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 413/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 3.7130e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 414/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.7033e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 415/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 3.6937e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 416/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 3.6839e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 417/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 3.6743e-04 - mean_absolute_error: 0.0123 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 418/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 3.6650e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 419/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 3.6549e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 420/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.6461e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 421/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 3.6365e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 422/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 3.6273e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 423/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 3.6178e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 424/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 3.6089e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 425/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 3.5994e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 426/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 3.5903e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 427/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 3.5814e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 428/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 3.5722e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 429/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.5632e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 430/1000\n",
      "52/52 [==============================] - 0s 611us/step - loss: 3.5543e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 431/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 3.5456e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 432/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.5366e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 433/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 3.5280e-04 - mean_absolute_error: 0.0120 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 434/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 3.5189e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 435/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 3.5102e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 436/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 3.5012e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 437/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.4929e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 438/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 3.4839e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 439/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.4757e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 440/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.4669e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 441/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 3.4586e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 442/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 3.4496e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 443/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 3.4419e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 444/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 3.4325e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 445/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 3.4252e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 446/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 3.4159e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 447/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 3.4086e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 448/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 3.3993e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 449/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 3.3927e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 450/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 3.3831e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 451/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 3.3764e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 452/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 3.3672e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0247\n",
      "Epoch 453/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 3.3609e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 454/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 3.3511e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0247\n",
      "Epoch 455/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 3.3457e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 456/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 3.3353e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0247\n",
      "Epoch 457/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.3312e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 458/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 3.3195e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0246\n",
      "Epoch 459/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.3179e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 460/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 3.3047e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 461/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 3.3065e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 462/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 3.2919e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0244\n",
      "Epoch 463/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 3.2990e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0250\n",
      "Epoch 464/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 3.2830e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0243\n",
      "Epoch 465/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 3.2992e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0011 - val_mean_absolute_error: 0.0252\n",
      "Epoch 466/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.2830e-04 - mean_absolute_error: 0.0115 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 467/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 3.3145e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0011 - val_mean_absolute_error: 0.0254\n",
      "Epoch 468/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 3.3017e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 469/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 3.3587e-04 - mean_absolute_error: 0.0118 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 470/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 3.3546e-04 - mean_absolute_error: 0.0119 - val_loss: 0.0010 - val_mean_absolute_error: 0.0236\n",
      "Epoch 471/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 3.4490e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 472/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 3.4547e-04 - mean_absolute_error: 0.0124 - val_loss: 9.9303e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 473/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 3.5830e-04 - mean_absolute_error: 0.0126 - val_loss: 0.0012 - val_mean_absolute_error: 0.0258\n",
      "Epoch 474/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 3.5704e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0010 - val_mean_absolute_error: 0.0236\n",
      "Epoch 475/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 3.6803e-04 - mean_absolute_error: 0.0129 - val_loss: 0.0011 - val_mean_absolute_error: 0.0254\n",
      "Epoch 476/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 3.5817e-04 - mean_absolute_error: 0.0128 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 477/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.5990e-04 - mean_absolute_error: 0.0127 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 478/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 3.4085e-04 - mean_absolute_error: 0.0122 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 479/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 3.3652e-04 - mean_absolute_error: 0.0121 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238\n",
      "Epoch 480/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 3.2073e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 481/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 3.1975e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0010 - val_mean_absolute_error: 0.0236\n",
      "Epoch 482/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 3.1515e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0011 - val_mean_absolute_error: 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 3.1624e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238\n",
      "Epoch 484/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 3.1857e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0011 - val_mean_absolute_error: 0.0251\n",
      "Epoch 485/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 3.1590e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 486/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.1885e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0011 - val_mean_absolute_error: 0.0246\n",
      "Epoch 487/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 3.1277e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0011 - val_mean_absolute_error: 0.0246\n",
      "Epoch 488/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.1446e-04 - mean_absolute_error: 0.0112 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 489/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.0902e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 490/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 3.1003e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 491/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 3.0737e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 492/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 3.0754e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 493/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 3.0718e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0011 - val_mean_absolute_error: 0.0247\n",
      "Epoch 494/1000\n",
      "52/52 [==============================] - 0s 634us/step - loss: 3.0594e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 495/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 3.0655e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 496/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 3.0428e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0010 - val_mean_absolute_error: 0.0245\n",
      "Epoch 497/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 3.0500e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 498/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 3.0262e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 499/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 3.0305e-04 - mean_absolute_error: 0.0109 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 500/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 3.0127e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 501/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 3.0128e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 502/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 3.0016e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0010 - val_mean_absolute_error: 0.0245\n",
      "Epoch 503/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 2.9973e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 504/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 2.9909e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 505/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 2.9830e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 506/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 2.9792e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 507/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.9692e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 508/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 2.9661e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 509/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.9558e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 510/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.9523e-04 - mean_absolute_error: 0.0107 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 511/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.9428e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 512/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.9386e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 513/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 2.9301e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 514/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 2.9252e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 515/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 2.9176e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 516/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.9119e-04 - mean_absolute_error: 0.0106 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 517/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 2.9051e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 518/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.8990e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 519/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.8923e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 520/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.8862e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 521/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.8798e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 522/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.8733e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 523/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.8671e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 524/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 2.8606e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 525/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 2.8545e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 526/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.8479e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 527/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.8419e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 528/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.8353e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 529/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.8292e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 530/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.8227e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 531/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.8168e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 532/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.8101e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 533/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 2.8042e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 534/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.7977e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 535/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 2.7918e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 536/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.7853e-04 - mean_absolute_error: 0.0103 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 537/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 2.7793e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 538/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.7731e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 539/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 2.7669e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 540/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.7609e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 541/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 2.7546e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 542/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 2.7485e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 543/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.7421e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 544/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.7362e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 545/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.7300e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 546/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.7238e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 547/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 2.7181e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 548/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.7115e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 549/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.7059e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 550/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.6994e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 551/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 2.6942e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 552/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 2.6875e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 553/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.6829e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 554/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.6759e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 555/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.6720e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 556/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.6650e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 557/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.6624e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 558/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.6554e-04 - mean_absolute_error: 0.0100 - val_loss: 9.9751e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 559/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.6544e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 560/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 2.6480e-04 - mean_absolute_error: 0.0100 - val_loss: 9.9380e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 561/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 2.6499e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 562/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.6451e-04 - mean_absolute_error: 0.0100 - val_loss: 9.8747e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 563/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.6525e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 564/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.6506e-04 - mean_absolute_error: 0.0101 - val_loss: 9.8050e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 565/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 2.6667e-04 - mean_absolute_error: 0.0102 - val_loss: 0.0010 - val_mean_absolute_error: 0.0246\n",
      "Epoch 566/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.6722e-04 - mean_absolute_error: 0.0102 - val_loss: 9.7206e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 567/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 2.7035e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 568/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 2.7222e-04 - mean_absolute_error: 0.0105 - val_loss: 9.6323e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 569/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.7765e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 570/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.8138e-04 - mean_absolute_error: 0.0110 - val_loss: 9.5847e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 571/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 2.8936e-04 - mean_absolute_error: 0.0113 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 572/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.9426e-04 - mean_absolute_error: 0.0115 - val_loss: 9.6553e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 573/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 3.0259e-04 - mean_absolute_error: 0.0117 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 574/1000\n",
      "52/52 [==============================] - 0s 643us/step - loss: 3.0450e-04 - mean_absolute_error: 0.0118 - val_loss: 9.9430e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 575/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 3.0690e-04 - mean_absolute_error: 0.0119 - val_loss: 9.9374e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 576/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 3.0039e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 577/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 586us/step - loss: 2.9304e-04 - mean_absolute_error: 0.0116 - val_loss: 9.4202e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 578/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.8012e-04 - mean_absolute_error: 0.0108 - val_loss: 0.0011 - val_mean_absolute_error: 0.0254\n",
      "Epoch 579/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.6997e-04 - mean_absolute_error: 0.0109 - val_loss: 9.1033e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 580/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.6075e-04 - mean_absolute_error: 0.0099 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 581/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 2.5599e-04 - mean_absolute_error: 0.0101 - val_loss: 9.1028e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 582/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 2.5478e-04 - mean_absolute_error: 0.0096 - val_loss: 0.0011 - val_mean_absolute_error: 0.0253\n",
      "Epoch 583/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.5343e-04 - mean_absolute_error: 0.0100 - val_loss: 9.3344e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 584/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.5614e-04 - mean_absolute_error: 0.0099 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 585/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.5343e-04 - mean_absolute_error: 0.0100 - val_loss: 9.6675e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 586/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.5539e-04 - mean_absolute_error: 0.0099 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 587/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 2.5103e-04 - mean_absolute_error: 0.0098 - val_loss: 9.9651e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 588/1000\n",
      "52/52 [==============================] - 0s 618us/step - loss: 2.5143e-04 - mean_absolute_error: 0.0098 - val_loss: 9.8524e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 589/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.4763e-04 - mean_absolute_error: 0.0096 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 590/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.4761e-04 - mean_absolute_error: 0.0097 - val_loss: 9.7382e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 591/1000\n",
      "52/52 [==============================] - 0s 614us/step - loss: 2.4565e-04 - mean_absolute_error: 0.0095 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 592/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.4553e-04 - mean_absolute_error: 0.0096 - val_loss: 9.7426e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 593/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.4503e-04 - mean_absolute_error: 0.0095 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 594/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.4449e-04 - mean_absolute_error: 0.0095 - val_loss: 9.8173e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 595/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 2.4452e-04 - mean_absolute_error: 0.0095 - val_loss: 9.9399e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 596/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.4348e-04 - mean_absolute_error: 0.0095 - val_loss: 9.9122e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 597/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.4339e-04 - mean_absolute_error: 0.0095 - val_loss: 9.8499e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 598/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 2.4224e-04 - mean_absolute_error: 0.0094 - val_loss: 9.9845e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 599/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.4186e-04 - mean_absolute_error: 0.0095 - val_loss: 9.7922e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 600/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.4096e-04 - mean_absolute_error: 0.0094 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 601/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.4035e-04 - mean_absolute_error: 0.0095 - val_loss: 9.7586e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 602/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.3984e-04 - mean_absolute_error: 0.0094 - val_loss: 9.9958e-04 - val_mean_absolute_error: 0.0242\n",
      "Epoch 603/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 2.3902e-04 - mean_absolute_error: 0.0094 - val_loss: 9.7532e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 604/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.3879e-04 - mean_absolute_error: 0.0093 - val_loss: 9.9651e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 605/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.3782e-04 - mean_absolute_error: 0.0094 - val_loss: 9.7676e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 606/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 2.3776e-04 - mean_absolute_error: 0.0093 - val_loss: 9.9293e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 607/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.3670e-04 - mean_absolute_error: 0.0093 - val_loss: 9.7896e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 608/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.3664e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8937e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 609/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 2.3562e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8095e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 610/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.3550e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8631e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 611/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.3456e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8215e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 612/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 2.3435e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8294e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 613/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.3354e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8315e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 614/1000\n",
      "52/52 [==============================] - 0s 612us/step - loss: 2.3316e-04 - mean_absolute_error: 0.0093 - val_loss: 9.8097e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 615/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.3250e-04 - mean_absolute_error: 0.0092 - val_loss: 9.8392e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 616/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.3199e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7909e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 617/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 2.3150e-04 - mean_absolute_error: 0.0092 - val_loss: 9.8299e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 618/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.3088e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7753e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 619/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.3046e-04 - mean_absolute_error: 0.0092 - val_loss: 9.8245e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 620/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.2978e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7746e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 621/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 2.2940e-04 - mean_absolute_error: 0.0091 - val_loss: 9.8207e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 622/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 2.2871e-04 - mean_absolute_error: 0.0092 - val_loss: 9.7706e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 623/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.2833e-04 - mean_absolute_error: 0.0091 - val_loss: 9.8052e-04 - val_mean_absolute_error: 0.0239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 624/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.2764e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7672e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 625/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.2726e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7980e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 626/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.2660e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7643e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 627/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.2617e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7822e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 628/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 2.2558e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7607e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 629/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.2510e-04 - mean_absolute_error: 0.0091 - val_loss: 9.7702e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 630/1000\n",
      "52/52 [==============================] - 0s 567us/step - loss: 2.2454e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7680e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 631/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.2398e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7627e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 632/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.2353e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7710e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 633/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.2291e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7516e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 634/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.2251e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7657e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 635/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.2185e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7449e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 636/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.2146e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7624e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 637/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.2081e-04 - mean_absolute_error: 0.0090 - val_loss: 9.7436e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 638/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 2.2040e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7514e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 639/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 2.1979e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7473e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 640/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.1934e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7436e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 641/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.1879e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7553e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 642/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 2.1825e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7310e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 643/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 2.1780e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7573e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 644/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 2.1718e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7214e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 645/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.1680e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7612e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 646/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 2.1614e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7173e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 647/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.1576e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7609e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 648/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 2.1514e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7150e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 649/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 2.1474e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7580e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 650/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 2.1416e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7140e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 651/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 2.1369e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7546e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 652/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 2.1322e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7171e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 653/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 2.1267e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7554e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 654/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.1234e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7062e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 655/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.1169e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7562e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 656/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 2.1140e-04 - mean_absolute_error: 0.0087 - val_loss: 9.6956e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 657/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.1080e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7780e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 658/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 2.1051e-04 - mean_absolute_error: 0.0087 - val_loss: 9.6687e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 659/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 2.1003e-04 - mean_absolute_error: 0.0087 - val_loss: 9.8101e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 660/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.0967e-04 - mean_absolute_error: 0.0087 - val_loss: 9.6234e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 661/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.0944e-04 - mean_absolute_error: 0.0087 - val_loss: 9.8610e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 662/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.0895e-04 - mean_absolute_error: 0.0087 - val_loss: 9.5636e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 663/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.0915e-04 - mean_absolute_error: 0.0088 - val_loss: 9.9316e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 664/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 2.0866e-04 - mean_absolute_error: 0.0088 - val_loss: 9.4862e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 665/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 2.0947e-04 - mean_absolute_error: 0.0088 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 666/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.0932e-04 - mean_absolute_error: 0.0089 - val_loss: 9.4033e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 667/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.1098e-04 - mean_absolute_error: 0.0090 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 668/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.1200e-04 - mean_absolute_error: 0.0091 - val_loss: 9.3176e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 669/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.1516e-04 - mean_absolute_error: 0.0093 - val_loss: 0.0010 - val_mean_absolute_error: 0.0246\n",
      "Epoch 670/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 2.1859e-04 - mean_absolute_error: 0.0095 - val_loss: 9.2289e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 671/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 2.2428e-04 - mean_absolute_error: 0.0097 - val_loss: 0.0010 - val_mean_absolute_error: 0.0248\n",
      "Epoch 672/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.3173e-04 - mean_absolute_error: 0.0102 - val_loss: 9.1859e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 673/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 2.4073e-04 - mean_absolute_error: 0.0104 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 674/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 2.5244e-04 - mean_absolute_error: 0.0109 - val_loss: 9.2519e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 675/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 2.6268e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 676/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 2.7334e-04 - mean_absolute_error: 0.0115 - val_loss: 9.5895e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 677/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 2.7549e-04 - mean_absolute_error: 0.0114 - val_loss: 9.8468e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 678/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 2.7421e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0010 - val_mean_absolute_error: 0.0245\n",
      "Epoch 679/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.5866e-04 - mean_absolute_error: 0.0110 - val_loss: 9.0811e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 680/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.4625e-04 - mean_absolute_error: 0.0105 - val_loss: 0.0011 - val_mean_absolute_error: 0.0257\n",
      "Epoch 681/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 2.2396e-04 - mean_absolute_error: 0.0101 - val_loss: 8.5465e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 682/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.1870e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0011 - val_mean_absolute_error: 0.0262\n",
      "Epoch 683/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 2.0865e-04 - mean_absolute_error: 0.0093 - val_loss: 8.5225e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 684/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 2.1706e-04 - mean_absolute_error: 0.0093 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 685/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.1809e-04 - mean_absolute_error: 0.0097 - val_loss: 9.0745e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 686/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 2.2901e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 687/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 2.3022e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 688/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 2.3166e-04 - mean_absolute_error: 0.0103 - val_loss: 9.1943e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 689/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.3035e-04 - mean_absolute_error: 0.0100 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 690/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 2.2128e-04 - mean_absolute_error: 0.0100 - val_loss: 8.5577e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 691/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 2.2417e-04 - mean_absolute_error: 0.0094 - val_loss: 0.0012 - val_mean_absolute_error: 0.0262\n",
      "Epoch 692/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 2.1178e-04 - mean_absolute_error: 0.0094 - val_loss: 8.4392e-04 - val_mean_absolute_error: 0.0219\n",
      "Epoch 693/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 2.1884e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0011 - val_mean_absolute_error: 0.0257\n",
      "Epoch 694/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.0686e-04 - mean_absolute_error: 0.0093 - val_loss: 8.8041e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 695/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 2.1182e-04 - mean_absolute_error: 0.0093 - val_loss: 0.0010 - val_mean_absolute_error: 0.0247\n",
      "Epoch 696/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.0116e-04 - mean_absolute_error: 0.0090 - val_loss: 9.4271e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 697/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 2.0259e-04 - mean_absolute_error: 0.0089 - val_loss: 9.7658e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 698/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.9493e-04 - mean_absolute_error: 0.0086 - val_loss: 9.9473e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 699/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 1.9505e-04 - mean_absolute_error: 0.0086 - val_loss: 9.4536e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 700/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.9130e-04 - mean_absolute_error: 0.0083 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 701/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.9135e-04 - mean_absolute_error: 0.0084 - val_loss: 9.4500e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 702/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.9008e-04 - mean_absolute_error: 0.0082 - val_loss: 9.9680e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 703/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.9010e-04 - mean_absolute_error: 0.0083 - val_loss: 9.6174e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 704/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.8947e-04 - mean_absolute_error: 0.0083 - val_loss: 9.7584e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 705/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.8928e-04 - mean_absolute_error: 0.0082 - val_loss: 9.7950e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 706/1000\n",
      "52/52 [==============================] - 0s 571us/step - loss: 1.8849e-04 - mean_absolute_error: 0.0083 - val_loss: 9.6224e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 707/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.8832e-04 - mean_absolute_error: 0.0082 - val_loss: 9.8937e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 708/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.8737e-04 - mean_absolute_error: 0.0082 - val_loss: 9.5933e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 709/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 1.8743e-04 - mean_absolute_error: 0.0082 - val_loss: 9.8848e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 710/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.8652e-04 - mean_absolute_error: 0.0082 - val_loss: 9.6364e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 711/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.8663e-04 - mean_absolute_error: 0.0082 - val_loss: 9.8091e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 712/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.8590e-04 - mean_absolute_error: 0.0082 - val_loss: 9.7205e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 713/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.8577e-04 - mean_absolute_error: 0.0082 - val_loss: 9.7094e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 714/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 1.8528e-04 - mean_absolute_error: 0.0081 - val_loss: 9.8149e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 715/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.8474e-04 - mean_absolute_error: 0.0082 - val_loss: 9.6361e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 716/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.8462e-04 - mean_absolute_error: 0.0081 - val_loss: 9.8817e-04 - val_mean_absolute_error: 0.0239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 717/1000\n",
      "52/52 [==============================] - 0s 562us/step - loss: 1.8368e-04 - mean_absolute_error: 0.0082 - val_loss: 9.5921e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 718/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.8389e-04 - mean_absolute_error: 0.0081 - val_loss: 9.8983e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 719/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 1.8271e-04 - mean_absolute_error: 0.0081 - val_loss: 9.5906e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 720/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.8313e-04 - mean_absolute_error: 0.0081 - val_loss: 9.8780e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 721/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.8186e-04 - mean_absolute_error: 0.0081 - val_loss: 9.6282e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 722/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.8224e-04 - mean_absolute_error: 0.0081 - val_loss: 9.8209e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 723/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.8116e-04 - mean_absolute_error: 0.0081 - val_loss: 9.6933e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 724/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.8126e-04 - mean_absolute_error: 0.0081 - val_loss: 9.7481e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 725/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.8060e-04 - mean_absolute_error: 0.0080 - val_loss: 9.7746e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 726/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 1.8026e-04 - mean_absolute_error: 0.0081 - val_loss: 9.6750e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 727/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.8007e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8382e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 728/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.7927e-04 - mean_absolute_error: 0.0081 - val_loss: 9.6200e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 729/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.7949e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8789e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 730/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.7830e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6027e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 731/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.7877e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8863e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 732/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.7746e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6187e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 733/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.7789e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8491e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 734/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.7682e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6727e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 735/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.7699e-04 - mean_absolute_error: 0.0080 - val_loss: 9.7829e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 736/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.7645e-04 - mean_absolute_error: 0.0079 - val_loss: 9.7613e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 737/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.7608e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6921e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 738/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.7622e-04 - mean_absolute_error: 0.0079 - val_loss: 9.8557e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 739/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.7520e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6062e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 740/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.7590e-04 - mean_absolute_error: 0.0079 - val_loss: 9.9414e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 741/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.7429e-04 - mean_absolute_error: 0.0080 - val_loss: 9.5502e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 742/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.7526e-04 - mean_absolute_error: 0.0079 - val_loss: 9.9682e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 743/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.7349e-04 - mean_absolute_error: 0.0079 - val_loss: 9.5505e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 744/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.7435e-04 - mean_absolute_error: 0.0079 - val_loss: 9.9290e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 745/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.7315e-04 - mean_absolute_error: 0.0079 - val_loss: 9.6328e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 746/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.7359e-04 - mean_absolute_error: 0.0080 - val_loss: 9.8149e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 747/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.7357e-04 - mean_absolute_error: 0.0079 - val_loss: 9.7898e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 748/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.7314e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6536e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 749/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.7448e-04 - mean_absolute_error: 0.0079 - val_loss: 9.9886e-04 - val_mean_absolute_error: 0.0240\n",
      "Epoch 750/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.7275e-04 - mean_absolute_error: 0.0081 - val_loss: 9.4871e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 751/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.7501e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 752/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.7200e-04 - mean_absolute_error: 0.0081 - val_loss: 9.3783e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 753/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.7445e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 754/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.7149e-04 - mean_absolute_error: 0.0080 - val_loss: 9.4014e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 755/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.7366e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 756/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.7306e-04 - mean_absolute_error: 0.0080 - val_loss: 9.6272e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 757/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.7467e-04 - mean_absolute_error: 0.0082 - val_loss: 9.8334e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 758/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.7777e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 759/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.7765e-04 - mean_absolute_error: 0.0085 - val_loss: 9.4987e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 760/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.8356e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 761/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.7965e-04 - mean_absolute_error: 0.0086 - val_loss: 9.1559e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 762/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.8629e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 763/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.7910e-04 - mean_absolute_error: 0.0086 - val_loss: 8.9649e-04 - val_mean_absolute_error: 0.0228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 764/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 1.8511e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 765/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.8008e-04 - mean_absolute_error: 0.0084 - val_loss: 9.1215e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 766/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 1.8561e-04 - mean_absolute_error: 0.0085 - val_loss: 0.0010 - val_mean_absolute_error: 0.0245\n",
      "Epoch 767/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.8793e-04 - mean_absolute_error: 0.0087 - val_loss: 9.7206e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 768/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.9372e-04 - mean_absolute_error: 0.0092 - val_loss: 9.8921e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 769/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 2.0313e-04 - mean_absolute_error: 0.0095 - val_loss: 0.0011 - val_mean_absolute_error: 0.0241\n",
      "Epoch 770/1000\n",
      "52/52 [==============================] - 0s 569us/step - loss: 2.0576e-04 - mean_absolute_error: 0.0097 - val_loss: 9.3483e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 771/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 2.1855e-04 - mean_absolute_error: 0.0098 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 772/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 2.1031e-04 - mean_absolute_error: 0.0098 - val_loss: 8.8557e-04 - val_mean_absolute_error: 0.0226\n",
      "Epoch 773/1000\n",
      "52/52 [==============================] - 0s 625us/step - loss: 2.2084e-04 - mean_absolute_error: 0.0098 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 774/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.0098e-04 - mean_absolute_error: 0.0096 - val_loss: 8.5903e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 775/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 2.0503e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0011 - val_mean_absolute_error: 0.0256\n",
      "Epoch 776/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.8467e-04 - mean_absolute_error: 0.0090 - val_loss: 8.6267e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 777/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.8690e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0011 - val_mean_absolute_error: 0.0254\n",
      "Epoch 778/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.7459e-04 - mean_absolute_error: 0.0086 - val_loss: 8.8519e-04 - val_mean_absolute_error: 0.0223\n",
      "Epoch 779/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.8082e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0011 - val_mean_absolute_error: 0.0249\n",
      "Epoch 780/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 1.7462e-04 - mean_absolute_error: 0.0087 - val_loss: 9.1646e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 781/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.8167e-04 - mean_absolute_error: 0.0087 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 782/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.7751e-04 - mean_absolute_error: 0.0087 - val_loss: 9.5793e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 783/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.8061e-04 - mean_absolute_error: 0.0088 - val_loss: 9.7192e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 784/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.7712e-04 - mean_absolute_error: 0.0086 - val_loss: 0.0010 - val_mean_absolute_error: 0.0240\n",
      "Epoch 785/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.7595e-04 - mean_absolute_error: 0.0088 - val_loss: 9.3438e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 786/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.7374e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0010 - val_mean_absolute_error: 0.0246\n",
      "Epoch 787/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.7017e-04 - mean_absolute_error: 0.0085 - val_loss: 9.0939e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 788/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.7001e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 789/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.6550e-04 - mean_absolute_error: 0.0082 - val_loss: 9.0218e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 790/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.6677e-04 - mean_absolute_error: 0.0078 - val_loss: 0.0011 - val_mean_absolute_error: 0.0247\n",
      "Epoch 791/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.6215e-04 - mean_absolute_error: 0.0080 - val_loss: 9.1213e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 792/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.6378e-04 - mean_absolute_error: 0.0078 - val_loss: 0.0010 - val_mean_absolute_error: 0.0244\n",
      "Epoch 793/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.5952e-04 - mean_absolute_error: 0.0078 - val_loss: 9.3279e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 794/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.6098e-04 - mean_absolute_error: 0.0077 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 795/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.5741e-04 - mean_absolute_error: 0.0076 - val_loss: 9.5364e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 796/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 1.5852e-04 - mean_absolute_error: 0.0076 - val_loss: 9.9622e-04 - val_mean_absolute_error: 0.0239\n",
      "Epoch 797/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.5582e-04 - mean_absolute_error: 0.0075 - val_loss: 9.6834e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 798/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.5660e-04 - mean_absolute_error: 0.0075 - val_loss: 9.8249e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 799/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.5474e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7592e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 800/1000\n",
      "52/52 [==============================] - 0s 631us/step - loss: 1.5525e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7536e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 801/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.5405e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7955e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 802/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.5430e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7421e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 803/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.5353e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7957e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 804/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.5355e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7638e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 805/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.5302e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7748e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 806/1000\n",
      "52/52 [==============================] - 0s 624us/step - loss: 1.5293e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7964e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 807/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.5243e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7472e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 808/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.5236e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8268e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 809/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.5182e-04 - mean_absolute_error: 0.0074 - val_loss: 9.7305e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 810/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.5180e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8431e-04 - val_mean_absolute_error: 0.0238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.5121e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7229e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 812/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.5123e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8426e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 813/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.5063e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7374e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 814/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.5063e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8331e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 815/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.5008e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7562e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 816/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.5002e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8120e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 817/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 1.4958e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7798e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 818/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.4939e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7884e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 819/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.4912e-04 - mean_absolute_error: 0.0073 - val_loss: 9.8059e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 820/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.4877e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7677e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 821/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.4867e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8348e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 822/1000\n",
      "52/52 [==============================] - 0s 645us/step - loss: 1.4817e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7477e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 823/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 1.4820e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8551e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 824/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.4756e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7316e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 825/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.4766e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8670e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 826/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.4696e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7247e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 827/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.4710e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8702e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 828/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.4638e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7322e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 829/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.4648e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8619e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 830/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.4588e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7467e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 831/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.4587e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8339e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 832/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.4547e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7789e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 833/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.4529e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8082e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 834/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.4517e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8195e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 835/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.4476e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7699e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 836/1000\n",
      "52/52 [==============================] - 0s 593us/step - loss: 1.4493e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8631e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 837/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.4425e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7299e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 838/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.4462e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8963e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 839/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.4370e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7015e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 840/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 1.4416e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9179e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 841/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.4310e-04 - mean_absolute_error: 0.0072 - val_loss: 9.6882e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 842/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.4352e-04 - mean_absolute_error: 0.0071 - val_loss: 9.9120e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 843/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.4258e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7002e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 844/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.4279e-04 - mean_absolute_error: 0.0071 - val_loss: 9.8814e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 845/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.4224e-04 - mean_absolute_error: 0.0071 - val_loss: 9.7530e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 846/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 1.4218e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8247e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 847/1000\n",
      "52/52 [==============================] - 0s 604us/step - loss: 1.4220e-04 - mean_absolute_error: 0.0071 - val_loss: 9.8212e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 848/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.4178e-04 - mean_absolute_error: 0.0072 - val_loss: 9.7565e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 849/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.4243e-04 - mean_absolute_error: 0.0071 - val_loss: 9.9056e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 850/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.4152e-04 - mean_absolute_error: 0.0072 - val_loss: 9.6854e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 851/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.4261e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9632e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 852/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.4121e-04 - mean_absolute_error: 0.0073 - val_loss: 9.6325e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 853/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.4235e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9852e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 854/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.4078e-04 - mean_absolute_error: 0.0072 - val_loss: 9.6291e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 855/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.4158e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9567e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 856/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.4047e-04 - mean_absolute_error: 0.0072 - val_loss: 9.6900e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 857/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.4078e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8711e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 858/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.4076e-04 - mean_absolute_error: 0.0072 - val_loss: 9.8174e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 859/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.4043e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7453e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 860/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 1.4167e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9763e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 861/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.4048e-04 - mean_absolute_error: 0.0073 - val_loss: 9.5998e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 862/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.4253e-04 - mean_absolute_error: 0.0073 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 863/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.4033e-04 - mean_absolute_error: 0.0073 - val_loss: 9.4813e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 864/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.4246e-04 - mean_absolute_error: 0.0073 - val_loss: 0.0010 - val_mean_absolute_error: 0.0240\n",
      "Epoch 865/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.4001e-04 - mean_absolute_error: 0.0073 - val_loss: 9.4566e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 866/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.4182e-04 - mean_absolute_error: 0.0072 - val_loss: 0.0010 - val_mean_absolute_error: 0.0240\n",
      "Epoch 867/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.4092e-04 - mean_absolute_error: 0.0073 - val_loss: 9.5846e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 868/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 1.4250e-04 - mean_absolute_error: 0.0074 - val_loss: 9.9935e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 869/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.4474e-04 - mean_absolute_error: 0.0075 - val_loss: 9.8700e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 870/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4590e-04 - mean_absolute_error: 0.0077 - val_loss: 9.7767e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 871/1000\n",
      "52/52 [==============================] - 0s 626us/step - loss: 1.5112e-04 - mean_absolute_error: 0.0077 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238\n",
      "Epoch 872/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.5048e-04 - mean_absolute_error: 0.0079 - val_loss: 9.4970e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 873/1000\n",
      "52/52 [==============================] - 0s 568us/step - loss: 1.5697e-04 - mean_absolute_error: 0.0081 - val_loss: 0.0011 - val_mean_absolute_error: 0.0241\n",
      "Epoch 874/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.5328e-04 - mean_absolute_error: 0.0081 - val_loss: 9.2522e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 875/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.5924e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0011 - val_mean_absolute_error: 0.0242\n",
      "Epoch 876/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.5396e-04 - mean_absolute_error: 0.0081 - val_loss: 9.1875e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 877/1000\n",
      "52/52 [==============================] - 0s 608us/step - loss: 1.5781e-04 - mean_absolute_error: 0.0079 - val_loss: 0.0010 - val_mean_absolute_error: 0.0243\n",
      "Epoch 878/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.5421e-04 - mean_absolute_error: 0.0078 - val_loss: 9.3903e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 879/1000\n",
      "52/52 [==============================] - 0s 620us/step - loss: 1.5551e-04 - mean_absolute_error: 0.0078 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 880/1000\n",
      "52/52 [==============================] - 0s 605us/step - loss: 1.5605e-04 - mean_absolute_error: 0.0079 - val_loss: 9.8541e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 881/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.5769e-04 - mean_absolute_error: 0.0081 - val_loss: 9.9664e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 882/1000\n",
      "52/52 [==============================] - 0s 619us/step - loss: 1.6473e-04 - mean_absolute_error: 0.0085 - val_loss: 0.0010 - val_mean_absolute_error: 0.0234\n",
      "Epoch 883/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.6884e-04 - mean_absolute_error: 0.0087 - val_loss: 9.7763e-04 - val_mean_absolute_error: 0.0237\n",
      "Epoch 884/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.8347e-04 - mean_absolute_error: 0.0092 - val_loss: 0.0011 - val_mean_absolute_error: 0.0237\n",
      "Epoch 885/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.8607e-04 - mean_absolute_error: 0.0093 - val_loss: 9.6381e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 886/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 2.0343e-04 - mean_absolute_error: 0.0099 - val_loss: 0.0011 - val_mean_absolute_error: 0.0240\n",
      "Epoch 887/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.9859e-04 - mean_absolute_error: 0.0098 - val_loss: 9.4723e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 888/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 2.1026e-04 - mean_absolute_error: 0.0101 - val_loss: 0.0011 - val_mean_absolute_error: 0.0245\n",
      "Epoch 889/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.9618e-04 - mean_absolute_error: 0.0098 - val_loss: 9.2173e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 890/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 2.0145e-04 - mean_absolute_error: 0.0097 - val_loss: 0.0012 - val_mean_absolute_error: 0.0253\n",
      "Epoch 891/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.8168e-04 - mean_absolute_error: 0.0094 - val_loss: 8.8558e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 892/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.8761e-04 - mean_absolute_error: 0.0090 - val_loss: 0.0012 - val_mean_absolute_error: 0.0260\n",
      "Epoch 893/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.6810e-04 - mean_absolute_error: 0.0087 - val_loss: 8.4940e-04 - val_mean_absolute_error: 0.0215\n",
      "Epoch 894/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.7668e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0012 - val_mean_absolute_error: 0.0261\n",
      "Epoch 895/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.6020e-04 - mean_absolute_error: 0.0083 - val_loss: 8.3282e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 896/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 1.6748e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0012 - val_mean_absolute_error: 0.0256\n",
      "Epoch 897/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.5422e-04 - mean_absolute_error: 0.0084 - val_loss: 8.5128e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 898/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.5846e-04 - mean_absolute_error: 0.0084 - val_loss: 0.0011 - val_mean_absolute_error: 0.0248\n",
      "Epoch 899/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.4928e-04 - mean_absolute_error: 0.0083 - val_loss: 9.0378e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 900/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.5107e-04 - mean_absolute_error: 0.0081 - val_loss: 0.0010 - val_mean_absolute_error: 0.0240\n",
      "Epoch 901/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.4629e-04 - mean_absolute_error: 0.0079 - val_loss: 9.7349e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 902/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.4651e-04 - mean_absolute_error: 0.0079 - val_loss: 9.4783e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 903/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.4462e-04 - mean_absolute_error: 0.0078 - val_loss: 0.0010 - val_mean_absolute_error: 0.0241\n",
      "Epoch 904/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 597us/step - loss: 1.4347e-04 - mean_absolute_error: 0.0079 - val_loss: 9.1274e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 905/1000\n",
      "52/52 [==============================] - 0s 597us/step - loss: 1.4330e-04 - mean_absolute_error: 0.0077 - val_loss: 0.0011 - val_mean_absolute_error: 0.0244\n",
      "Epoch 906/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.4048e-04 - mean_absolute_error: 0.0078 - val_loss: 9.0375e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 907/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.4133e-04 - mean_absolute_error: 0.0075 - val_loss: 0.0011 - val_mean_absolute_error: 0.0244\n",
      "Epoch 908/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.3743e-04 - mean_absolute_error: 0.0076 - val_loss: 9.1303e-04 - val_mean_absolute_error: 0.0227\n",
      "Epoch 909/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.3835e-04 - mean_absolute_error: 0.0073 - val_loss: 0.0010 - val_mean_absolute_error: 0.0242\n",
      "Epoch 910/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.3464e-04 - mean_absolute_error: 0.0074 - val_loss: 9.3141e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 911/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.3530e-04 - mean_absolute_error: 0.0072 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 912/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.3259e-04 - mean_absolute_error: 0.0073 - val_loss: 9.5001e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 913/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.3330e-04 - mean_absolute_error: 0.0071 - val_loss: 9.9967e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 914/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.3185e-04 - mean_absolute_error: 0.0072 - val_loss: 9.6247e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 915/1000\n",
      "52/52 [==============================] - 0s 598us/step - loss: 1.3284e-04 - mean_absolute_error: 0.0072 - val_loss: 9.9102e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 916/1000\n",
      "52/52 [==============================] - 0s 609us/step - loss: 1.3266e-04 - mean_absolute_error: 0.0073 - val_loss: 9.6648e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 917/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.3405e-04 - mean_absolute_error: 0.0073 - val_loss: 9.9277e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 918/1000\n",
      "52/52 [==============================] - 0s 607us/step - loss: 1.3477e-04 - mean_absolute_error: 0.0074 - val_loss: 9.6457e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 919/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.3667e-04 - mean_absolute_error: 0.0075 - val_loss: 0.0010 - val_mean_absolute_error: 0.0235\n",
      "Epoch 920/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.3773e-04 - mean_absolute_error: 0.0077 - val_loss: 9.5931e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 921/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.4014e-04 - mean_absolute_error: 0.0076 - val_loss: 0.0010 - val_mean_absolute_error: 0.0236\n",
      "Epoch 922/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.4109e-04 - mean_absolute_error: 0.0078 - val_loss: 9.5316e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 923/1000\n",
      "52/52 [==============================] - 0s 627us/step - loss: 1.4385e-04 - mean_absolute_error: 0.0078 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238\n",
      "Epoch 924/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.4444e-04 - mean_absolute_error: 0.0080 - val_loss: 9.4654e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 925/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.4722e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 926/1000\n",
      "52/52 [==============================] - 0s 570us/step - loss: 1.4735e-04 - mean_absolute_error: 0.0082 - val_loss: 9.4094e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 927/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.5001e-04 - mean_absolute_error: 0.0081 - val_loss: 0.0010 - val_mean_absolute_error: 0.0240\n",
      "Epoch 928/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.4963e-04 - mean_absolute_error: 0.0083 - val_loss: 9.3664e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 929/1000\n",
      "52/52 [==============================] - 0s 577us/step - loss: 1.5181e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0011 - val_mean_absolute_error: 0.0240\n",
      "Epoch 930/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.5118e-04 - mean_absolute_error: 0.0083 - val_loss: 9.3385e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 931/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.5248e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0011 - val_mean_absolute_error: 0.0240\n",
      "Epoch 932/1000\n",
      "52/52 [==============================] - 0s 582us/step - loss: 1.5173e-04 - mean_absolute_error: 0.0084 - val_loss: 9.3381e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 933/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 1.5203e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0010 - val_mean_absolute_error: 0.0239\n",
      "Epoch 934/1000\n",
      "52/52 [==============================] - 0s 587us/step - loss: 1.5136e-04 - mean_absolute_error: 0.0084 - val_loss: 9.3573e-04 - val_mean_absolute_error: 0.0229\n",
      "Epoch 935/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.5051e-04 - mean_absolute_error: 0.0082 - val_loss: 0.0010 - val_mean_absolute_error: 0.0238\n",
      "Epoch 936/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.5007e-04 - mean_absolute_error: 0.0083 - val_loss: 9.3875e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 937/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.4793e-04 - mean_absolute_error: 0.0081 - val_loss: 0.0010 - val_mean_absolute_error: 0.0237\n",
      "Epoch 938/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.4787e-04 - mean_absolute_error: 0.0082 - val_loss: 9.4199e-04 - val_mean_absolute_error: 0.0230\n",
      "Epoch 939/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.4447e-04 - mean_absolute_error: 0.0080 - val_loss: 0.0010 - val_mean_absolute_error: 0.0236\n",
      "Epoch 940/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.4466e-04 - mean_absolute_error: 0.0081 - val_loss: 9.4553e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 941/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.4014e-04 - mean_absolute_error: 0.0078 - val_loss: 9.9439e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 942/1000\n",
      "52/52 [==============================] - 0s 610us/step - loss: 1.4050e-04 - mean_absolute_error: 0.0079 - val_loss: 9.4925e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 943/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.3529e-04 - mean_absolute_error: 0.0075 - val_loss: 9.8450e-04 - val_mean_absolute_error: 0.0235\n",
      "Epoch 944/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.3589e-04 - mean_absolute_error: 0.0076 - val_loss: 9.5431e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 945/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.3059e-04 - mean_absolute_error: 0.0073 - val_loss: 9.7649e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 946/1000\n",
      "52/52 [==============================] - 0s 600us/step - loss: 1.3152e-04 - mean_absolute_error: 0.0074 - val_loss: 9.6041e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 947/1000\n",
      "52/52 [==============================] - 0s 574us/step - loss: 1.2662e-04 - mean_absolute_error: 0.0070 - val_loss: 9.6967e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 948/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.2788e-04 - mean_absolute_error: 0.0071 - val_loss: 9.6666e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 949/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.2369e-04 - mean_absolute_error: 0.0069 - val_loss: 9.6492e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 950/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.2517e-04 - mean_absolute_error: 0.0069 - val_loss: 9.7226e-04 - val_mean_absolute_error: 0.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951/1000\n",
      "52/52 [==============================] - 0s 599us/step - loss: 1.2172e-04 - mean_absolute_error: 0.0068 - val_loss: 9.6161e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 952/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.2323e-04 - mean_absolute_error: 0.0068 - val_loss: 9.7556e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 953/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.2046e-04 - mean_absolute_error: 0.0067 - val_loss: 9.6054e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 954/1000\n",
      "52/52 [==============================] - 0s 580us/step - loss: 1.2183e-04 - mean_absolute_error: 0.0067 - val_loss: 9.7734e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 955/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.1965e-04 - mean_absolute_error: 0.0066 - val_loss: 9.6081e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 956/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.2077e-04 - mean_absolute_error: 0.0067 - val_loss: 9.7715e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 957/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.1910e-04 - mean_absolute_error: 0.0066 - val_loss: 9.6201e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 958/1000\n",
      "52/52 [==============================] - 0s 601us/step - loss: 1.1996e-04 - mean_absolute_error: 0.0066 - val_loss: 9.7595e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 959/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.1870e-04 - mean_absolute_error: 0.0066 - val_loss: 9.6330e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 960/1000\n",
      "52/52 [==============================] - 0s 576us/step - loss: 1.1929e-04 - mean_absolute_error: 0.0066 - val_loss: 9.7426e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 961/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.1836e-04 - mean_absolute_error: 0.0066 - val_loss: 9.6463e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 962/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.1874e-04 - mean_absolute_error: 0.0066 - val_loss: 9.7301e-04 - val_mean_absolute_error: 0.0234\n",
      "Epoch 963/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.1805e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6628e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 964/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.1827e-04 - mean_absolute_error: 0.0065 - val_loss: 9.7179e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 965/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.1776e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6680e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 966/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.1784e-04 - mean_absolute_error: 0.0065 - val_loss: 9.7073e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 967/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.1747e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6742e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 968/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.1746e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6932e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 969/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.1718e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6706e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 970/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.1710e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6891e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 971/1000\n",
      "52/52 [==============================] - 0s 606us/step - loss: 1.1688e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6734e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 972/1000\n",
      "52/52 [==============================] - 0s 578us/step - loss: 1.1676e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6877e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 973/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.1656e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6712e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 974/1000\n",
      "52/52 [==============================] - 0s 585us/step - loss: 1.1642e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6876e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 975/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.1625e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6715e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 976/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 1.1611e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6854e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 977/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.1594e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6671e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 978/1000\n",
      "52/52 [==============================] - 0s 602us/step - loss: 1.1578e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6845e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 979/1000\n",
      "52/52 [==============================] - 0s 615us/step - loss: 1.1562e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6661e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 980/1000\n",
      "52/52 [==============================] - 0s 572us/step - loss: 1.1548e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6822e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 981/1000\n",
      "52/52 [==============================] - 0s 590us/step - loss: 1.1531e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6606e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 982/1000\n",
      "52/52 [==============================] - 0s 573us/step - loss: 1.1516e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6848e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 983/1000\n",
      "52/52 [==============================] - 0s 595us/step - loss: 1.1500e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6625e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 984/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.1486e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6830e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 985/1000\n",
      "52/52 [==============================] - 0s 589us/step - loss: 1.1468e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6565e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 986/1000\n",
      "52/52 [==============================] - 0s 588us/step - loss: 1.1455e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6800e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 987/1000\n",
      "52/52 [==============================] - 0s 592us/step - loss: 1.1437e-04 - mean_absolute_error: 0.0065 - val_loss: 9.6524e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 988/1000\n",
      "52/52 [==============================] - 0s 616us/step - loss: 1.1425e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6831e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 989/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.1407e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6508e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 990/1000\n",
      "52/52 [==============================] - 0s 575us/step - loss: 1.1395e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6823e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 991/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.1377e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6503e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 992/1000\n",
      "52/52 [==============================] - 0s 581us/step - loss: 1.1367e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6827e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 993/1000\n",
      "52/52 [==============================] - 0s 594us/step - loss: 1.1346e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6444e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 994/1000\n",
      "52/52 [==============================] - 0s 603us/step - loss: 1.1337e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6794e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 995/1000\n",
      "52/52 [==============================] - 0s 591us/step - loss: 1.1318e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6397e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 996/1000\n",
      "52/52 [==============================] - 0s 586us/step - loss: 1.1309e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6826e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 997/1000\n",
      "52/52 [==============================] - 0s 596us/step - loss: 1.1288e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6390e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 998/1000\n",
      "52/52 [==============================] - 0s 579us/step - loss: 1.1281e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6844e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 999/1000\n",
      "52/52 [==============================] - 0s 583us/step - loss: 1.1259e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6333e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 1000/1000\n",
      "52/52 [==============================] - 0s 584us/step - loss: 1.1253e-04 - mean_absolute_error: 0.0064 - val_loss: 9.6827e-04 - val_mean_absolute_error: 0.0232\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAADFCAYAAAB5PKoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAdca2eR9IeV3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21bc387be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.140 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 1.550 RMSE\n",
      "Real\t:\n",
      " [[ -1.           0.        ]\n",
      " [  0.71000004  19.39999962]\n",
      " [  0.34000003  19.60000038]\n",
      " [ -1.          19.79999924]],\n",
      "Predict\t:\n",
      " [[ -0.99999434   0.04557582]\n",
      " [  0.6368953   15.74719906]\n",
      " [  0.51218081  15.414361  ]\n",
      " [ -0.94450557  15.97248554]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in test_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[-1], test_Y_hat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "gist": {
   "data": {
    "description": "tensorflow/konlpy.ipynb",
    "public": false
   },
   "id": "5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "kernelspec": {
   "display_name": "Tensorflow: Python3.6 (conda env)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "906px",
    "left": "0px",
    "right": "763.4px",
    "top": "159px",
    "width": "259px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
