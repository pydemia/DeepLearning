{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#LSTM-Lecture\" data-toc-modified-id=\"LSTM-Lecture-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>LSTM Lecture</a></div><div class=\"lev2 toc-item\"><a href=\"#Network-Learning-Parameter\" data-toc-modified-id=\"Network-Learning-Parameter-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Network Learning Parameter</a></div><div class=\"lev1 toc-item\"><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Vectorizing\" data-toc-modified-id=\"Vectorizing-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Vectorizing</a></div><div class=\"lev3 toc-item\"><a href=\"#Padding\" data-toc-modified-id=\"Padding-202\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Padding</a></div><div class=\"lev3 toc-item\"><a href=\"#Splitting-(Train-&amp;-Test)\" data-toc-modified-id=\"Splitting-(Train-&amp;-Test)-203\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Splitting (Train &amp; Test)</a></div><div class=\"lev3 toc-item\"><a href=\"#Decoder-:-the-Embeddings-to-String\" data-toc-modified-id=\"Decoder-:-the-Embeddings-to-String-204\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Decoder : the Embeddings to String</a></div><div class=\"lev1 toc-item\"><a href=\"#Generate-Data-with-Function-for-Memory-Saving\" data-toc-modified-id=\"Generate-Data-with-Function-for-Memory-Saving-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generate Data with Function for Memory Saving</a></div><div class=\"lev1 toc-item\"><a href=\"#Encoder-Decoder-(Seq2Seq)---It-can-be-used-for-Text-Summarization\" data-toc-modified-id=\"Encoder-Decoder-(Seq2Seq)---It-can-be-used-for-Text-Summarization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Encoder-Decoder (Seq2Seq) - It can be used for Text Summarization</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-401\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-402\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-403\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-404\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model\" data-toc-modified-id=\"Inference-Model-4041\"><span class=\"toc-item-num\">4.0.4.1&nbsp;&nbsp;</span>Inference Model</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model-within-a-Function\" data-toc-modified-id=\"Inference-Model-within-a-Function-4042\"><span class=\"toc-item-num\">4.0.4.2&nbsp;&nbsp;</span>Inference Model within a Function</a></div><div class=\"lev1 toc-item\"><a href=\"#Stateful\" data-toc-modified-id=\"Stateful-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stateful</a></div><div class=\"lev1 toc-item\"><a href=\"#RNN-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"RNN-Decoder-with-Attention-(encoder:-return_sequence=True)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>RNN Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyRNNAttention-(Feed-Forward)\" data-toc-modified-id=\"MyRNNAttention-(Feed-Forward)-601\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>MyRNNAttention (Feed-Forward)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-602\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-603\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-604\"><span class=\"toc-item-num\">6.0.4&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-605\"><span class=\"toc-item-num\">6.0.5&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#LSTM-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"LSTM-Decoder-with-Attention-(encoder:-return_sequence=True)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>LSTM Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyLSTMAttention-(Feed-Forward)\" data-toc-modified-id=\"MyLSTMAttention-(Feed-Forward)-701\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>MyLSTMAttention (Feed-Forward)</a></div><div class=\"lev3 toc-item\"><a href=\"#NewLSTMAttention\" data-toc-modified-id=\"NewLSTMAttention-702\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>NewLSTMAttention</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-703\"><span class=\"toc-item-num\">7.0.3&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-704\"><span class=\"toc-item-num\">7.0.4&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-705\"><span class=\"toc-item-num\">7.0.5&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-706\"><span class=\"toc-item-num\">7.0.6&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#GRU-Decoder-with-Attention-(encoder:-return_sequence=True)\" data-toc-modified-id=\"GRU-Decoder-with-Attention-(encoder:-return_sequence=True)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>GRU Decoder with Attention (encoder: <code>return_sequence=True</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#MyGRUAttention-(Feed-Forward,-Not-Recurrent)\" data-toc-modified-id=\"MyGRUAttention-(Feed-Forward,-Not-Recurrent)-801\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span>MyGRUAttention (Feed-Forward, Not Recurrent)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-802\"><span class=\"toc-item-num\">8.0.2&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-803\"><span class=\"toc-item-num\">8.0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-804\"><span class=\"toc-item-num\">8.0.4&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-805\"><span class=\"toc-item-num\">8.0.5&nbsp;&nbsp;</span>Testing</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "\n",
    "```txt\n",
    "xt = xt\n",
    "ht = fw(ht-1, xt)  \n",
    "   = tanh(Whh * ht-1 + Wxh * xt)\n",
    "\n",
    "ot = Why * ht\n",
    "   = Why * tanh(Whh * ht-1 + Wxh * xt)\n",
    "   \n",
    "yt = argmax(softmax(ot))\n",
    "or\n",
    "yt = sigmoid(ot)\n",
    "   \n",
    "```\n",
    "\n",
    "```txt\n",
    "weights : Wxh, Whh, Wyh\n",
    "biases  : bxh, bhh, byh\n",
    "```\n",
    "\n",
    "* LSTM\n",
    "\n",
    "```txt\n",
    "xt = xt\n",
    "ht = fw(ht-1, xt)  \n",
    "   = tanh(Whh * ht-1 + Wxh * xt)\n",
    "\n",
    "yt = Why * ht\n",
    "   = Why * tanh(Whh * ht-1 + Wxh * xt)\n",
    "```\n",
    "\n",
    "```txt\n",
    "weights : Wxh, Whh, Wyh\n",
    "biases  : bxh, bhh, byh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recurrent Neural Networks, we are quickly confronted to the so-called __gradient vanishing problem__:\n",
    "\n",
    "In machine learning, __the vanishing gradient problem__ is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation.  \n",
    "In such methods, each of the neural network’s weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training.   \n",
    "_Traditional activation functions such as the hyperbolic tangent function have gradients in the range `(−1,1)` or `(0,1)`_, and backpropagation computes gradients by the chain rule.  \n",
    "This has the effect of multiplying n of these small numbers to compute gradients of the “front” layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.\n",
    "\n",
    "One solution is __to consider *adding the updates* instead of multiplying them__, and this is exactly what the LSTM does. The state of every cell is updated in an additive way (Equation 9) such that the gradient hardly vanishes.\n",
    "\n",
    ">선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다.  \n",
    ">그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "* Input  Gate  \n",
    "* Forget Gate  \n",
    "* Cell   State\n",
    "* Output Gate \n",
    "* Hidden State \n",
    "\n",
    "![lstm](keras_stateful_lstm_2.png)\n",
    "![lstm](lstm_basic.png)\n",
    "![lstm](lstm_module.jpg)\n",
    "![](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm'](lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime as dt\n",
    "import itertools as it\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (SimpleRNN, RNN, LSTM, GRU,\n",
    "                          Input, Reshape, Dense, Flatten, Permute, Lambda,\n",
    "                          Embedding, RepeatVector, Activation,\n",
    "                          TimeDistributed, Bidirectional,\n",
    "                          dot, multiply, concatenate, merge)\n",
    "from keras.callbacks import Callback, LambdaCallback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.weights = []\n",
    "        self.states = []\n",
    "\n",
    "#    def on_batch_begin(self, batch, logs={}):\n",
    "#        self.weights.append([{'begin_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.weights.append([{'end_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "        \n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[0].get_weights()))\n",
    "#print_outputs = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].output))\n",
    "#print_states = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Learning Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = HIDDEN_SIZE = 64\n",
    "EPOCH_NUM = 100\n",
    "BATCH_SIZE = 256\n",
    "#GPU_NUM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model.  \n",
    "We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "* Summary of the algorithm\n",
    "    - We start with input sequences from a domain (e.g. English sentences)\n",
    "        and correspding target sequences from another domain\n",
    "        (e.g. French sentences).\n",
    "    - An encoder LSTM turns input sequences to 2 state vectors\n",
    "        (we keep the last LSTM state and discard the outputs).\n",
    "    - A decoder LSTM is trained to turn the target sequences into\n",
    "        the same sequence but offset by one timestep in the future,\n",
    "        a training process called \"teacher forcing\" in this context.  \n",
    "        Is uses as initial state the state vectors from the encoder.\n",
    "        Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "        given `targets[...t]`, conditioned on the input sequence.\n",
    "    - In inference mode, when we want to decode unknown input sequences, we:\n",
    "        - Encode the input sequence into state vectors\n",
    "        - Start with a target sequence of size 1\n",
    "            (just the start-of-sequence character)\n",
    "        - Feed the state vectors and 1-char target sequence\n",
    "            to the decoder to produce predictions for the next character\n",
    "        - Sample the next character using these predictions\n",
    "            (we simply use argmax).\n",
    "        - Append the sampled character to the target sequence\n",
    "        - Repeat until we generate the end-of-sequence character or we\n",
    "            hit the character limit.\n",
    "* Data download\n",
    "    English to French sentence pairs.\n",
    "    http://www.manythings.org/anki/fra-eng.zip\n",
    "    Lots of neat sentence pairs datasets can be found at:\n",
    "    http://www.manythings.org/anki/\n",
    "\n",
    "* References\n",
    "    - Sequence to Sequence Learning with Neural Networks\n",
    "        https://arxiv.org/abs/1409.3215\n",
    "    - Learning Phrase Representations using\n",
    "        RNN Encoder-Decoder for Statistical Machine Translation\n",
    "        https://arxiv.org/abs/1406.1078\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "num_samples = 100000  # Number of samples to train on. Max=145437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "-----------------------------------\n",
      "Max sequence length for inputs: 34\n",
      "Number of unique input tokens: 80\n",
      "-----------------------------------\n",
      "Max sequence length for outputs: 79\n",
      "Number of unique output tokens: 110\n",
      "-----------------------------------\n",
      "Eng: My best friend dances really well.\n",
      "Fre: \tMon meilleur ami danse vraiment bien.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data_path).read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('-'*35)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('-'*35)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('-'*35)\n",
    "print('Eng: %s' % input_texts[-1])\n",
    "print('Fre: %s' % target_texts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(test_size=.2):\n",
    "    \n",
    "    input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "    input_token_dict = dict(\n",
    "        [(i, char) for i, char in enumerate(input_characters)])\n",
    "    target_token_dict = dict(\n",
    "        [(i, char) for i, char in enumerate(target_characters)])\n",
    "\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "                \n",
    "    print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)\n",
    "    \n",
    "    \n",
    "    # Padding\n",
    "    max_len = max(encoder_input_data.shape[1], decoder_target_data.shape[1])\n",
    "    print('Max Length :', max_len)\n",
    "\n",
    "    seq_X = pad_sequences(encoder_input_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "    decoder_input_Y = pad_sequences(decoder_input_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "    decoder_target_Y = pad_sequences(decoder_target_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "    print(seq_X.shape, decoder_input_Y.shape, decoder_target_Y.shape)\n",
    "    \n",
    "    \n",
    "    test_size = test_size\n",
    "\n",
    "    (train_X, test_X,\n",
    "     train_Y, test_Y) = train_test_split(seq_X, decoder_input_Y,\n",
    "                                        test_size=test_size,\n",
    "                                        shuffle=False,\n",
    "                                        random_state=99)\n",
    "\n",
    "    (train_decoder_input_Y,\n",
    "     test_decoder_input_Y,\n",
    "     train_decoder_target_Y,\n",
    "     test_decoder_target_Y) = train_test_split(decoder_input_Y,\n",
    "                                               decoder_target_Y,\n",
    "                                               test_size=test_size,\n",
    "                                               shuffle=False,\n",
    "                                               random_state=99)\n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    print('Train_X\\t: %s\\nTrain_Y\\t: %s\\nTest_X\\t: %s\\nTest_Y\\t: %s\\n' % \n",
    "          (train_X.shape, train_Y.shape, test_X.shape, test_Y.shape))\n",
    "                \n",
    "    return (input_token_dict, target_token_dict,\n",
    "            seq_X, decoder_input_Y, decoder_target_Y,\n",
    "            train_X, test_X,\n",
    "            train_Y, test_Y,\n",
    "            train_decoder_input_Y, test_decoder_input_Y,\n",
    "            train_decoder_target_Y, test_decoder_target_Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "input_token_dict = dict(\n",
    "    [(i, char) for i, char in enumerate(input_characters)])\n",
    "target_token_dict = dict(\n",
    "    [(i, char) for i, char in enumerate(target_characters)])\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "max_len = max(encoder_input_data.shape[1], decoder_target_data.shape[1])\n",
    "print('Max Length :', max_len)\n",
    "\n",
    "seq_X = encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "decoder_input_Y = decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "decoder_target_Y = decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_len, dtype='float32', padding='post')\n",
    "\n",
    "print(seq_X.shape, decoder_input_Y.shape, decoder_target_Y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unembedded_X = np.array([''.join([input_token_dict[idx] for idx in np.argmax(item, axis=1)]) for item in seq_X])\n",
    "unembedded_Y = np.array([''.join([target_token_dict[idx] for idx in np.argmax(item, axis=1)]) for item in decoder_input_data])\n",
    "print(unembedded_X[:5])\n",
    "print(unembedded_Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting (Train & Test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "test_size = .2\n",
    "\n",
    "(train_X, test_X,\n",
    " train_Y, test_Y) = train_test_split(seq_X, decoder_input_Y,\n",
    "                                    test_size=test_size,\n",
    "                                    shuffle=False,\n",
    "                                    random_state=99)\n",
    "\n",
    "(train_decoder_input_Y,\n",
    " test_decoder_input_Y,\n",
    " train_decoder_target_Y,\n",
    " test_decoder_target_Y) = train_test_split(decoder_input_Y,\n",
    "                                           decoder_target_Y,\n",
    "                                           test_size=test_size,\n",
    "                                           shuffle=False,\n",
    "                                           random_state=99)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print('Train_X\\t: %s\\nTrain_Y\\t: %s\\nTest_X\\t: %s\\nTest_Y\\t: %s\\n' % \n",
    "      (train_X.shape, train_Y.shape, test_X.shape, test_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder : the Embeddings to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_decoder(input_embedded):\n",
    "    res = np.array([''.join([input_token_dict[idx] for idx in np.argmax(item, axis=1)]) for item in input_embedded])\n",
    "    return res\n",
    "\n",
    "def output_decoder(output_embedded):\n",
    "    res = np.array([''.join([target_token_dict[idx] for idx in np.argmax(item, axis=1)]) for item in output_embedded])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data with Function for Memory Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 34, 80) (100000, 79, 110) (100000, 79, 110)\n",
      "Max Length : 79\n",
      "(100000, 79, 80) (100000, 79, 110) (100000, 79, 110)\n",
      "(80000, 79, 80) (80000, 79, 110)\n",
      "Train_X\t: (80000, 79, 80)\n",
      "Train_Y\t: (80000, 79, 110)\n",
      "Test_X\t: (20000, 79, 80)\n",
      "Test_Y\t: (20000, 79, 110)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(input_token_dict, target_token_dict,\n",
    " _, _, _,  #seq_X, decoder_input_Y, decoder_target_Y,\n",
    " train_X, test_X,\n",
    " train_Y, test_Y,\n",
    " _, _,  #train_decoder_input_Y, test_decoder_input_Y,\n",
    " train_decoder_target_Y, test_decoder_target_Y) = generate_data(test_size=.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# apt-get install -y graphviz libgraphviz-dev\n",
    "keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder-Decoder (Seq2Seq) - It can be used for Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)\n",
    "!['lstm_seq_to_seq_1'](lstm_encdec_2.png)\n",
    "!['lstm_seq_to_seq_1'](seq2seq_1.png)\n",
    "!['Use of a summary state in the encoder-decoder architecture'](lstm_attention_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "[<tf.Variable 'Encoder_LSTM/kernel:0' shape=(80, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/recurrent_kernel:0' shape=(64, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/bias:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/kernel:0' shape=(110, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/recurrent_kernel:0' shape=(64, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/bias:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/kernel:0' shape=(64, 110) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/bias:0' shape=(110,) dtype=float32_ref>]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Input (InputLayer)      (None, None, 80)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Input (InputLayer)      (None, None, 110)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (LSTM)             [(None, 64), (None,  37120       Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)             [(None, None, 64), ( 44800       Decoder_Input[0][0]              \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Output (Dense)          (None, None, 110)    7150        Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 89,070\n",
      "Trainable params: 89,070\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_decoder_target_Y.shape\n",
    "xlen, ylen = ndimX, ndimY\n",
    "\n",
    "num_encoder_tokens = ndimX\n",
    "num_decoder_tokens = ndimY\n",
    "\n",
    "\n",
    "train_decoder_input_Y = train_Y\n",
    "test_decoder_input_Y = test_Y\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, ndimX), name='Encoder_Input')\n",
    "encoder = LSTM(LATENT_DIM, return_state=True, name='Encoder_LSTM')\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='Decoder_Input')\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Decoder_Output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "pprint(model.weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_NUM: 100, BATCH_SIZE 256\n",
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      " - 27s - loss: 1.0564 - acc: 0.0627 - val_loss: 1.2048 - val_acc: 0.1096\n",
      "Epoch 2/100\n",
      " - 27s - loss: 0.8413 - acc: 0.1065 - val_loss: 1.0168 - val_acc: 0.1587\n",
      "Epoch 3/100\n",
      " - 27s - loss: 0.7374 - acc: 0.1314 - val_loss: 0.9339 - val_acc: 0.1691\n",
      "Epoch 4/100\n",
      " - 27s - loss: 0.6896 - acc: 0.1387 - val_loss: 0.8909 - val_acc: 0.1764\n",
      "Epoch 5/100\n",
      " - 27s - loss: 0.6612 - acc: 0.1443 - val_loss: 0.8618 - val_acc: 0.1825\n",
      "Epoch 6/100\n",
      " - 27s - loss: 0.6402 - acc: 0.1496 - val_loss: 0.8386 - val_acc: 0.1883\n",
      "Epoch 7/100\n",
      " - 27s - loss: 0.6227 - acc: 0.1538 - val_loss: 0.8184 - val_acc: 0.1935\n",
      "Epoch 8/100\n",
      " - 27s - loss: 0.6073 - acc: 0.1577 - val_loss: 0.8009 - val_acc: 0.1984\n",
      "Epoch 9/100\n",
      " - 27s - loss: 0.5936 - acc: 0.1613 - val_loss: 0.7831 - val_acc: 0.2033\n",
      "Epoch 10/100\n",
      " - 27s - loss: 0.5808 - acc: 0.1656 - val_loss: 0.7684 - val_acc: 0.2085\n",
      "Epoch 11/100\n",
      " - 27s - loss: 0.5682 - acc: 0.1699 - val_loss: 0.7546 - val_acc: 0.2131\n",
      "Epoch 12/100\n",
      " - 27s - loss: 0.5562 - acc: 0.1738 - val_loss: 0.7389 - val_acc: 0.2180\n",
      "Epoch 13/100\n",
      " - 27s - loss: 0.5451 - acc: 0.1773 - val_loss: 0.7267 - val_acc: 0.2215\n",
      "Epoch 14/100\n",
      " - 27s - loss: 0.5359 - acc: 0.1799 - val_loss: 0.7152 - val_acc: 0.2242\n",
      "Epoch 15/100\n",
      " - 27s - loss: 0.5272 - acc: 0.1822 - val_loss: 0.7063 - val_acc: 0.2269\n",
      "Epoch 16/100\n",
      " - 27s - loss: 0.5188 - acc: 0.1845 - val_loss: 0.6956 - val_acc: 0.2296\n",
      "Epoch 17/100\n",
      " - 27s - loss: 0.5088 - acc: 0.1876 - val_loss: 0.6845 - val_acc: 0.2328\n",
      "Epoch 18/100\n",
      " - 27s - loss: 0.4979 - acc: 0.1906 - val_loss: 0.6728 - val_acc: 0.2362\n",
      "Epoch 19/100\n",
      " - 27s - loss: 0.4885 - acc: 0.1931 - val_loss: 0.6618 - val_acc: 0.2394\n",
      "Epoch 20/100\n",
      " - 27s - loss: 0.4802 - acc: 0.1958 - val_loss: 0.6536 - val_acc: 0.2419\n",
      "Epoch 21/100\n",
      " - 27s - loss: 0.4735 - acc: 0.1978 - val_loss: 0.6477 - val_acc: 0.2435\n",
      "Epoch 22/100\n",
      " - 27s - loss: 0.4671 - acc: 0.1999 - val_loss: 0.6408 - val_acc: 0.2461\n",
      "Epoch 23/100\n",
      " - 27s - loss: 0.4616 - acc: 0.2017 - val_loss: 0.6330 - val_acc: 0.2485\n",
      "Epoch 24/100\n",
      " - 27s - loss: 0.4563 - acc: 0.2033 - val_loss: 0.6277 - val_acc: 0.2501\n",
      "Epoch 25/100\n",
      " - 27s - loss: 0.4511 - acc: 0.2049 - val_loss: 0.6218 - val_acc: 0.2520\n",
      "Epoch 26/100\n",
      " - 27s - loss: 0.4464 - acc: 0.2063 - val_loss: 0.6160 - val_acc: 0.2535\n",
      "Epoch 27/100\n",
      " - 27s - loss: 0.4421 - acc: 0.2078 - val_loss: 0.6106 - val_acc: 0.2551\n",
      "Epoch 28/100\n",
      " - 27s - loss: 0.4380 - acc: 0.2089 - val_loss: 0.6065 - val_acc: 0.2567\n",
      "Epoch 29/100\n",
      " - 27s - loss: 0.4341 - acc: 0.2101 - val_loss: 0.6027 - val_acc: 0.2579\n",
      "Epoch 30/100\n",
      " - 27s - loss: 0.4304 - acc: 0.2112 - val_loss: 0.5976 - val_acc: 0.2591\n",
      "Epoch 31/100\n",
      " - 27s - loss: 0.4268 - acc: 0.2123 - val_loss: 0.5938 - val_acc: 0.2601\n",
      "Epoch 32/100\n",
      " - 27s - loss: 0.4235 - acc: 0.2132 - val_loss: 0.5905 - val_acc: 0.2614\n",
      "Epoch 33/100\n",
      " - 27s - loss: 0.4202 - acc: 0.2141 - val_loss: 0.5872 - val_acc: 0.2617\n",
      "Epoch 34/100\n",
      " - 27s - loss: 0.4173 - acc: 0.2149 - val_loss: 0.5833 - val_acc: 0.2634\n",
      "Epoch 35/100\n",
      " - 27s - loss: 0.4144 - acc: 0.2157 - val_loss: 0.5802 - val_acc: 0.2639\n",
      "Epoch 36/100\n",
      " - 27s - loss: 0.4122 - acc: 0.2163 - val_loss: 0.5769 - val_acc: 0.2651\n",
      "Epoch 37/100\n",
      " - 27s - loss: 0.4104 - acc: 0.2169 - val_loss: 0.5806 - val_acc: 0.2644\n",
      "Epoch 38/100\n",
      " - 27s - loss: 0.4088 - acc: 0.2173 - val_loss: 0.5723 - val_acc: 0.2667\n",
      "Epoch 39/100\n",
      " - 27s - loss: 0.4055 - acc: 0.2184 - val_loss: 0.5710 - val_acc: 0.2673\n",
      "Epoch 40/100\n",
      " - 27s - loss: 0.4029 - acc: 0.2190 - val_loss: 0.5667 - val_acc: 0.2684\n",
      "Epoch 41/100\n",
      " - 27s - loss: 0.4008 - acc: 0.2196 - val_loss: 0.5648 - val_acc: 0.2690\n",
      "Epoch 42/100\n",
      " - 27s - loss: 0.3989 - acc: 0.2202 - val_loss: 0.5631 - val_acc: 0.2694\n",
      "Epoch 43/100\n",
      " - 27s - loss: 0.3968 - acc: 0.2207 - val_loss: 0.5596 - val_acc: 0.2705\n",
      "Epoch 44/100\n",
      " - 27s - loss: 0.3952 - acc: 0.2212 - val_loss: 0.5587 - val_acc: 0.2708\n",
      "Epoch 45/100\n",
      " - 27s - loss: 0.3933 - acc: 0.2217 - val_loss: 0.5571 - val_acc: 0.2711\n",
      "Epoch 46/100\n",
      " - 27s - loss: 0.3915 - acc: 0.2222 - val_loss: 0.5550 - val_acc: 0.2718\n",
      "Epoch 47/100\n",
      " - 27s - loss: 0.3899 - acc: 0.2227 - val_loss: 0.5535 - val_acc: 0.2723\n",
      "Epoch 48/100\n",
      " - 27s - loss: 0.3883 - acc: 0.2233 - val_loss: 0.5509 - val_acc: 0.2730\n",
      "Epoch 49/100\n",
      " - 27s - loss: 0.3867 - acc: 0.2236 - val_loss: 0.5486 - val_acc: 0.2740\n",
      "Epoch 50/100\n",
      " - 27s - loss: 0.3852 - acc: 0.2240 - val_loss: 0.5480 - val_acc: 0.2743\n",
      "Epoch 51/100\n",
      " - 27s - loss: 0.3839 - acc: 0.2245 - val_loss: 0.5467 - val_acc: 0.2746\n",
      "Epoch 52/100\n",
      " - 27s - loss: 0.3828 - acc: 0.2248 - val_loss: 0.5443 - val_acc: 0.2751\n",
      "Epoch 53/100\n",
      " - 26s - loss: 0.3816 - acc: 0.2252 - val_loss: 0.5438 - val_acc: 0.2753\n",
      "Epoch 54/100\n",
      " - 27s - loss: 0.3800 - acc: 0.2257 - val_loss: 0.5415 - val_acc: 0.2759\n",
      "Epoch 55/100\n",
      " - 26s - loss: 0.3786 - acc: 0.2260 - val_loss: 0.5402 - val_acc: 0.2764\n",
      "Epoch 56/100\n",
      " - 26s - loss: 0.3775 - acc: 0.2264 - val_loss: 0.5402 - val_acc: 0.2765\n",
      "Epoch 57/100\n",
      " - 26s - loss: 0.3762 - acc: 0.2268 - val_loss: 0.5388 - val_acc: 0.2772\n",
      "Epoch 58/100\n",
      " - 26s - loss: 0.3751 - acc: 0.2271 - val_loss: 0.5371 - val_acc: 0.2774\n",
      "Epoch 59/100\n",
      " - 26s - loss: 0.3740 - acc: 0.2275 - val_loss: 0.5349 - val_acc: 0.2783\n",
      "Epoch 60/100\n",
      " - 26s - loss: 0.3728 - acc: 0.2278 - val_loss: 0.5345 - val_acc: 0.2785\n",
      "Epoch 61/100\n",
      " - 26s - loss: 0.3717 - acc: 0.2281 - val_loss: 0.5344 - val_acc: 0.2784\n",
      "Epoch 62/100\n",
      " - 26s - loss: 0.3708 - acc: 0.2284 - val_loss: 0.5320 - val_acc: 0.2795\n",
      "Epoch 63/100\n",
      " - 26s - loss: 0.3697 - acc: 0.2288 - val_loss: 0.5303 - val_acc: 0.2798\n",
      "Epoch 64/100\n",
      " - 26s - loss: 0.3688 - acc: 0.2290 - val_loss: 0.5298 - val_acc: 0.2800\n",
      "Epoch 65/100\n",
      " - 26s - loss: 0.3678 - acc: 0.2293 - val_loss: 0.5293 - val_acc: 0.2799\n",
      "Epoch 66/100\n",
      " - 26s - loss: 0.3669 - acc: 0.2295 - val_loss: 0.5291 - val_acc: 0.2802\n",
      "Epoch 67/100\n",
      " - 26s - loss: 0.3659 - acc: 0.2298 - val_loss: 0.5293 - val_acc: 0.2800\n",
      "Epoch 68/100\n",
      " - 26s - loss: 0.3650 - acc: 0.2300 - val_loss: 0.5259 - val_acc: 0.2814\n",
      "Epoch 69/100\n",
      " - 26s - loss: 0.3642 - acc: 0.2303 - val_loss: 0.5258 - val_acc: 0.2810\n",
      "Epoch 70/100\n",
      " - 26s - loss: 0.3634 - acc: 0.2305 - val_loss: 0.5240 - val_acc: 0.2816\n",
      "Epoch 71/100\n",
      " - 26s - loss: 0.3628 - acc: 0.2307 - val_loss: 0.5227 - val_acc: 0.2819\n",
      "Epoch 72/100\n",
      " - 26s - loss: 0.3616 - acc: 0.2310 - val_loss: 0.5236 - val_acc: 0.2818\n",
      "Epoch 73/100\n",
      " - 26s - loss: 0.3607 - acc: 0.2313 - val_loss: 0.5213 - val_acc: 0.2826\n",
      "Epoch 74/100\n",
      " - 26s - loss: 0.3599 - acc: 0.2315 - val_loss: 0.5213 - val_acc: 0.2823\n",
      "Epoch 75/100\n",
      " - 26s - loss: 0.3593 - acc: 0.2317 - val_loss: 0.5204 - val_acc: 0.2825\n",
      "Epoch 76/100\n",
      " - 26s - loss: 0.3585 - acc: 0.2319 - val_loss: 0.5198 - val_acc: 0.2830\n",
      "Epoch 77/100\n",
      " - 26s - loss: 0.3577 - acc: 0.2322 - val_loss: 0.5183 - val_acc: 0.2832\n",
      "Epoch 78/100\n",
      " - 26s - loss: 0.3571 - acc: 0.2323 - val_loss: 0.5179 - val_acc: 0.2835\n",
      "Epoch 79/100\n",
      " - 26s - loss: 0.3563 - acc: 0.2325 - val_loss: 0.5167 - val_acc: 0.2840\n",
      "Epoch 80/100\n",
      " - 26s - loss: 0.3556 - acc: 0.2328 - val_loss: 0.5156 - val_acc: 0.2844\n",
      "Epoch 81/100\n",
      " - 26s - loss: 0.3549 - acc: 0.2330 - val_loss: 0.5150 - val_acc: 0.2843\n",
      "Epoch 82/100\n",
      " - 26s - loss: 0.3544 - acc: 0.2331 - val_loss: 0.5148 - val_acc: 0.2843\n",
      "Epoch 83/100\n",
      " - 26s - loss: 0.3536 - acc: 0.2334 - val_loss: 0.5144 - val_acc: 0.2845\n",
      "Epoch 84/100\n",
      " - 26s - loss: 0.3531 - acc: 0.2334 - val_loss: 0.5137 - val_acc: 0.2846\n",
      "Epoch 85/100\n",
      " - 26s - loss: 0.3524 - acc: 0.2337 - val_loss: 0.5139 - val_acc: 0.2847\n",
      "Epoch 86/100\n",
      " - 26s - loss: 0.3519 - acc: 0.2338 - val_loss: 0.5115 - val_acc: 0.2851\n",
      "Epoch 87/100\n",
      " - 26s - loss: 0.3512 - acc: 0.2340 - val_loss: 0.5121 - val_acc: 0.2853\n",
      "Epoch 88/100\n",
      " - 26s - loss: 0.3507 - acc: 0.2342 - val_loss: 0.5104 - val_acc: 0.2858\n",
      "Epoch 89/100\n",
      " - 26s - loss: 0.3503 - acc: 0.2343 - val_loss: 0.5129 - val_acc: 0.2850\n",
      "Epoch 90/100\n",
      " - 26s - loss: 0.3496 - acc: 0.2346 - val_loss: 0.5118 - val_acc: 0.2852\n",
      "Epoch 91/100\n",
      " - 26s - loss: 0.3491 - acc: 0.2347 - val_loss: 0.5087 - val_acc: 0.2864\n",
      "Epoch 92/100\n",
      " - 26s - loss: 0.3484 - acc: 0.2348 - val_loss: 0.5094 - val_acc: 0.2861\n",
      "Epoch 93/100\n",
      " - 26s - loss: 0.3482 - acc: 0.2349 - val_loss: 0.5080 - val_acc: 0.2865\n",
      "Epoch 94/100\n",
      " - 26s - loss: 0.3475 - acc: 0.2351 - val_loss: 0.5087 - val_acc: 0.2859\n",
      "Epoch 95/100\n",
      " - 26s - loss: 0.3470 - acc: 0.2353 - val_loss: 0.5064 - val_acc: 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      " - 26s - loss: 0.3467 - acc: 0.2354 - val_loss: 0.5061 - val_acc: 0.2872\n",
      "Epoch 97/100\n",
      " - 26s - loss: 0.3459 - acc: 0.2356 - val_loss: 0.5064 - val_acc: 0.2871\n",
      "Epoch 98/100\n",
      " - 26s - loss: 0.3457 - acc: 0.2356 - val_loss: 0.5057 - val_acc: 0.2872\n",
      "Epoch 99/100\n",
      " - 26s - loss: 0.3451 - acc: 0.2358 - val_loss: 0.5055 - val_acc: 0.2874\n",
      "Epoch 100/100\n",
      " - 26s - loss: 0.3446 - acc: 0.2360 - val_loss: 0.5049 - val_acc: 0.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer Decoder_LSTM was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder_LSTM/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'Encoder_LSTM/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "print('EPOCH_NUM: %s, BATCH_SIZE %s' % (EPOCH_NUM, BATCH_SIZE))\n",
    "\n",
    "fitted = model.fit([train_X, train_decoder_input_Y], train_decoder_target_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,                   \n",
    "                   verbose=2,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=True,\n",
    "                   #callbacks=[history],\n",
    "                  )\n",
    "\n",
    "# Save model\n",
    "model.save('encdec_attention_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAADFCAYAAABuHjrdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGMYQw8TZMAzDMIYYJs6GYRiGMcT4/2oysodQNiKtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f725c035668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 79, 80)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 79, 110)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decoder_input_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 24.655759\n",
      "[ 'Va !\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      " 'Cours\\u202f!\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      " 'Courez\\u202f!\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t']\n",
      "[ 'Qa a\\nt .eeeeeeeneneneneneneneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee'\n",
      " 'Poure.!\\nnn..eenenenenenenenenenennnnnnnneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee'\n",
      " 'Pourez-!\\nto...eneneneneneneneneneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee']\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = fitted.model.evaluate([train_X, train_decoder_input_Y],\n",
    "                                            train_decoder_target_Y, verbose=0)\n",
    "print('Accuracy: %f' % (train_accuracy * 100))\n",
    "\n",
    "train_Y_hat_array = fitted.model.predict([train_X, train_decoder_input_Y])\n",
    "train_Y_real = output_decoder(train_decoder_target_Y)\n",
    "train_Y_hat = output_decoder(train_Y_hat_array)\n",
    "\n",
    "print(train_Y_real[:3])\n",
    "print(train_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.384051\n",
      "[ 'Je ne pouvais simplement plus le faire.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      " 'Je ne pourrais simplement plus le faire.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      " \"J'ignorais simplement quoi faire.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\"]\n",
      "[ 'Je ne vesvais semplement paus da caire.\\n..eeeenenenenenenenenenenenennnnnnnnnee'\n",
      " 'Je ne vesvrais vemplement paus da caire.\\n..eeeenenenenenenenenenenenennnnnnnnne'\n",
      " 'Jeernorei  qe plement luei qaire \\n-teeeenenenenenenenenenenenenennnnnnnnnnnnnnn']\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = fitted.model.evaluate([test_X, test_decoder_input_Y],\n",
    "                                          test_decoder_target_Y, verbose=0)\n",
    "print('Accuracy: %f' % (test_accuracy * 100))\n",
    "\n",
    "test_Y_hat_array = fitted.model.predict([test_X, test_decoder_input_Y])\n",
    "test_Y_real = output_decoder(test_decoder_target_Y)\n",
    "test_Y_hat = output_decoder(test_Y_hat_array)\n",
    "\n",
    "print(test_Y_real[:3])\n",
    "print(test_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.        , -0.        , -0.69256145, ..., -0.33435214,\n",
       "          0.34420902,  0.        ],\n",
       "        [-0.        , -0.        , -0.69256145, ..., -0.33435214,\n",
       "          0.34420902,  0.        ],\n",
       "        [-0.        , -0.        , -0.68697131, ..., -0.33889049,\n",
       "          0.34533989,  0.        ],\n",
       "        ..., \n",
       "        [-0.35704663, -0.        , -0.73305655, ..., -0.31595737,\n",
       "          0.26176482,  0.29351097],\n",
       "        [-0.35582325, -0.        , -0.74242914, ..., -0.35222739,\n",
       "          0.24373946,  0.28449452],\n",
       "        [-0.37104198, -0.        , -0.74794906, ..., -0.35420641,\n",
       "          0.245188  ,  0.27271089]], dtype=float32),\n",
       " array([[-45.51025772, -23.10789299,  -7.62661886, ..., -37.09708786,\n",
       "          10.59297752,  34.82820511],\n",
       "        [-45.51025772, -23.10789299,  -7.62661886, ..., -37.09708786,\n",
       "          10.59297752,  34.82820511],\n",
       "        [-48.71523285, -20.92699432,  -8.40753555, ..., -37.78025055,\n",
       "           9.03160191,  35.68209457],\n",
       "        ..., \n",
       "        [ -4.04885721, -19.39731598,  -4.49346972, ..., -16.6002636 ,\n",
       "          12.76477528,  11.0684042 ],\n",
       "        [ -3.03088045, -21.68319702,  -3.31416869, ..., -16.81711388,\n",
       "           9.75216198,   8.15221882],\n",
       "        [ -3.28592634, -20.34362221,  -3.95041656, ..., -16.72426987,\n",
       "          10.33356953,   7.83437634]], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_value = encoder_model.predict(test_X)\n",
    "states_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq = np.zeros((1, timestepY, ndimY))\n",
    "target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[  8.72634021e-10,   2.51322678e-08,   1.57124487e-05, ...,\n",
       "            1.25545974e-09,   3.71764663e-08,   8.67759198e-10],\n",
       "         [  3.88121464e-08,   4.09222366e-06,   1.49357645e-03, ...,\n",
       "            3.39653461e-08,   7.26343887e-06,   3.13161372e-08],\n",
       "         [  8.63519534e-10,   8.89296984e-07,   3.67032796e-01, ...,\n",
       "            1.08796794e-09,   2.24307325e-04,   8.43676906e-10],\n",
       "         ..., \n",
       "         [  3.63428593e-10,   6.86149404e-04,   1.05993846e-03, ...,\n",
       "            4.62619248e-10,   3.85086954e-04,   3.98455297e-10],\n",
       "         [  3.63384767e-10,   6.86061394e-04,   1.05980854e-03, ...,\n",
       "            4.62572342e-10,   3.85079795e-04,   3.98407973e-10],\n",
       "         [  3.63418712e-10,   6.86048588e-04,   1.05994032e-03, ...,\n",
       "            4.62606703e-10,   3.85075720e-04,   3.98445166e-10]]], dtype=float32),\n",
       " array([[ 0.42716748, -0.83249986, -0.4557212 ,  0.77366114,  0.02977827,\n",
       "         -0.35765854, -0.16042793,  0.98233396,  0.37449574, -0.8486613 ,\n",
       "          0.19144978, -0.56916535, -0.95405126,  0.57911861, -0.26643619,\n",
       "          0.27351254, -0.77974701, -0.31792173,  0.55480838, -0.44831708,\n",
       "          0.67619562,  0.52871299, -0.29140973,  0.9946543 , -0.41400328,\n",
       "         -0.93717742, -0.76409835, -0.73195428,  0.42983523, -0.15696061,\n",
       "          0.28920758,  0.0584687 ,  0.23312642, -0.54061258, -0.16363111,\n",
       "         -0.3666068 , -0.68106705,  0.36836749,  0.5988462 , -0.78272438,\n",
       "          0.98410404,  0.71508956,  0.55493635, -0.51399326, -0.57392114,\n",
       "          0.75524753,  0.11256883,  0.45733383, -0.62257379,  0.05986131,\n",
       "         -0.51302928,  0.60864592,  0.81155652, -0.54060888,  0.64253414,\n",
       "          0.59379518,  0.68963701, -0.68971825,  0.32305467, -0.77197003,\n",
       "         -0.57830077, -0.67980886,  0.21730484,  0.20617785]], dtype=float32),\n",
       " array([[  0.7013185 , -27.70203018,  -0.95914352,   1.0293839 ,\n",
       "           1.40235066,  -1.73848808,  -0.1618259 ,   2.36019254,\n",
       "          81.7694931 ,  -1.25134826,   0.45103517,  -2.0049634 ,\n",
       "          -1.87506652,   0.66113555,  -2.3030591 ,   0.28065646,\n",
       "          -1.04472494,  -3.22392511,   0.62530142,  -7.8809514 ,\n",
       "           1.30026186,   0.58835709, -18.11767387,   9.7690239 ,\n",
       "         -53.12446213,  -1.7143364 ,  -1.00599003,  -2.92481375,\n",
       "           0.94255412,  -0.84270233,   6.2529192 ,   1.32832909,\n",
       "           1.33363247,  -0.60502064,  -0.6317842 ,  -0.88626999,\n",
       "          -1.72961807,   2.73121953,  10.29107666,  -1.05236554,\n",
       "           2.41342974,   0.89752275,   0.93119973, -12.80754852,\n",
       "          -1.62870574,   1.06585407,   0.18932344,   0.94573909,\n",
       "          -1.53282666,   0.10150737, -20.27946663,   0.70676768,\n",
       "           2.61564946,  -2.42270446,   2.97763872,   1.28423262,\n",
       "           1.47839403,  -0.84741819,   0.36534807,  -2.02214193,\n",
       "          -1.88100564,  -2.11069298,   0.22082563,   0.74631715]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "states_value = [h, c]\n",
    "output_tokens, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model within a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(LATENT_DIM, test_X, timestepY, ndimY, encoder_inputs, encoder_states):\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    states_value = encoder_model.predict(test_X)\n",
    "    \n",
    "    samples, _, _ = test_X.shape\n",
    "    target_seq = np.zeros((samples, timestepY, ndimY))\n",
    "    \n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    states_value = [h, c]\n",
    "    output_tokens, h, c\n",
    "    \n",
    "    y_hat_array = output_tokens\n",
    "    \n",
    "    return y_hat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  8.72634021e-10,   2.51322199e-08,   1.57124341e-05, ...,\n",
       "           1.25545729e-09,   3.71763917e-08,   8.67757588e-10],\n",
       "        [  3.88121393e-08,   4.09222321e-06,   1.49357412e-03, ...,\n",
       "           3.39653390e-08,   7.26343023e-06,   3.13161301e-08],\n",
       "        [  8.63519034e-10,   8.89298121e-07,   3.67031902e-01, ...,\n",
       "           1.08796938e-09,   2.24306976e-04,   8.43676573e-10],\n",
       "        ..., \n",
       "        [  3.63429092e-10,   6.86149404e-04,   1.05993904e-03, ...,\n",
       "           4.62619970e-10,   3.85087274e-04,   3.98455102e-10],\n",
       "        [  3.63384933e-10,   6.86061743e-04,   1.05980958e-03, ...,\n",
       "           4.62570759e-10,   3.85079620e-04,   3.98408168e-10],\n",
       "        [  3.63418684e-10,   6.86048879e-04,   1.05994032e-03, ...,\n",
       "           4.62606647e-10,   3.85075895e-04,   3.98445138e-10]],\n",
       "\n",
       "       [[  8.72634021e-10,   2.51322199e-08,   1.57124341e-05, ...,\n",
       "           1.25545729e-09,   3.71763917e-08,   8.67757588e-10],\n",
       "        [  3.88121393e-08,   4.09222321e-06,   1.49357412e-03, ...,\n",
       "           3.39653390e-08,   7.26343023e-06,   3.13161301e-08],\n",
       "        [  8.63519034e-10,   8.89298121e-07,   3.67031902e-01, ...,\n",
       "           1.08796938e-09,   2.24306976e-04,   8.43676573e-10],\n",
       "        ..., \n",
       "        [  3.63429092e-10,   6.86149404e-04,   1.05993904e-03, ...,\n",
       "           4.62619970e-10,   3.85087274e-04,   3.98455102e-10],\n",
       "        [  3.63384933e-10,   6.86061743e-04,   1.05980958e-03, ...,\n",
       "           4.62570759e-10,   3.85079620e-04,   3.98408168e-10],\n",
       "        [  3.63418684e-10,   6.86048879e-04,   1.05994032e-03, ...,\n",
       "           4.62606647e-10,   3.85075895e-04,   3.98445138e-10]],\n",
       "\n",
       "       [[  8.01296751e-10,   3.42078366e-08,   1.63254390e-05, ...,\n",
       "           1.13855958e-09,   3.55779193e-08,   7.84191712e-10],\n",
       "        [  4.09592182e-08,   4.04109005e-06,   1.86758186e-03, ...,\n",
       "           3.48059537e-08,   7.63835851e-06,   3.13678825e-08],\n",
       "        [  9.73621073e-10,   9.73136139e-07,   3.65898758e-01, ...,\n",
       "           1.17795584e-09,   1.99318703e-04,   9.17098952e-10],\n",
       "        ..., \n",
       "        [  3.63428454e-10,   6.86157262e-04,   1.05992984e-03, ...,\n",
       "           4.62619970e-10,   3.85086430e-04,   3.98455879e-10],\n",
       "        [  3.63385128e-10,   6.86067040e-04,   1.05980004e-03, ...,\n",
       "           4.62571009e-10,   3.85079096e-04,   3.98407640e-10],\n",
       "        [  3.63420266e-10,   6.86051906e-04,   1.05993485e-03, ...,\n",
       "           4.62606897e-10,   3.85075196e-04,   3.98445388e-10]],\n",
       "\n",
       "       ..., \n",
       "       [[  1.18841448e-09,   2.95959729e-10,   2.53511771e-05, ...,\n",
       "           1.25729083e-09,   2.49212036e-07,   1.10252407e-09],\n",
       "        [  2.73835821e-09,   1.16095107e-05,   1.69670806e-04, ...,\n",
       "           4.05258493e-09,   2.89451492e-07,   3.62324015e-09],\n",
       "        [  5.50121615e-10,   2.36507924e-09,   9.57600832e-01, ...,\n",
       "           4.53421439e-10,   1.63456221e-04,   4.63703270e-10],\n",
       "        ..., \n",
       "        [  8.58722649e-10,   1.09892506e-04,   4.05886350e-03, ...,\n",
       "           9.23213395e-10,   1.17742680e-02,   6.90562663e-10],\n",
       "        [  8.72508010e-10,   9.83610880e-05,   4.24202019e-03, ...,\n",
       "           9.28321253e-10,   1.32318689e-02,   6.91163238e-10],\n",
       "        [  8.79349371e-10,   9.05085762e-05,   4.32417681e-03, ...,\n",
       "           9.29926747e-10,   1.43369846e-02,   6.89622415e-10]],\n",
       "\n",
       "       [[  7.61313623e-10,   6.11359408e-10,   1.49102052e-05, ...,\n",
       "           8.23355328e-10,   1.17606746e-07,   7.76252174e-10],\n",
       "        [  1.11477294e-09,   1.19476636e-05,   1.24062235e-05, ...,\n",
       "           1.57576840e-09,   4.39812986e-08,   1.20053401e-09],\n",
       "        [  1.54727120e-10,   3.19148596e-09,   9.75467741e-01, ...,\n",
       "           1.31812311e-10,   8.79911167e-05,   1.39809900e-10],\n",
       "        ..., \n",
       "        [  3.63408997e-10,   6.86217158e-04,   1.05973729e-03, ...,\n",
       "           4.62601402e-10,   3.85078863e-04,   3.98433037e-10],\n",
       "        [  3.63407054e-10,   6.86135900e-04,   1.05977303e-03, ...,\n",
       "           4.62598959e-10,   3.85072548e-04,   3.98432426e-10],\n",
       "        [  3.63405195e-10,   6.86075073e-04,   1.05979224e-03, ...,\n",
       "           4.62593908e-10,   3.85069841e-04,   3.98428096e-10]],\n",
       "\n",
       "       [[  8.01121280e-10,   5.07318687e-10,   8.04167848e-06, ...,\n",
       "           8.67544647e-10,   9.57224202e-08,   8.19994184e-10],\n",
       "        [  1.38485434e-09,   1.31511624e-05,   9.36394463e-06, ...,\n",
       "           2.01583927e-09,   4.65219827e-08,   1.57760061e-09],\n",
       "        [  1.91830010e-10,   2.05052531e-09,   9.72511947e-01, ...,\n",
       "           1.63879479e-10,   8.14571977e-05,   1.74222026e-10],\n",
       "        ..., \n",
       "        [  3.63417602e-10,   6.86460116e-04,   1.05968502e-03, ...,\n",
       "           4.62615835e-10,   3.85105028e-04,   3.98444000e-10],\n",
       "        [  3.63411662e-10,   6.86314772e-04,   1.05972437e-03, ...,\n",
       "           4.62607452e-10,   3.85090854e-04,   3.98437450e-10],\n",
       "        [  3.63410246e-10,   6.86209009e-04,   1.05976220e-03, ...,\n",
       "           4.62602096e-10,   3.85080726e-04,   3.98434424e-10]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y_hat_array = inference_model(LATENT_DIM, test_X, timestepY, ndimY, encoder_inputs, encoder_states)\n",
    "test_Y_hat_array"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y in test_decoder_target_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[0][:ylen], test_Y_hat[0][:ylen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __I’m given a big sequence (e.g. Time Series) and I split it into smaller sequences to construct my input matrix `X`. Is it possible that the LSTM may find dependencies between the sequences?__\n",
    "\n",
    "    No it’s not possible unless you go for the stateful LSTM.  \n",
    "    Most of the problems can be solved with stateless LSTM so if you go for the stateful mode, make sure you really need it.  \n",
    "    In stateless mode, long term memory does not mean that the LSTM will remember the content of the previous batches.\n",
    "    \n",
    "\n",
    "* __Why do we make the difference between stateless and stateful LSTM in Keras?__\n",
    "\n",
    "    A LSTM has cells and is therefore stateful by definition (not the same stateful meaning as used in Keras). Fabien Chollet gives this definition of statefulness:\n",
    "    > stateful: Boolean (default False).\n",
    "    > If `True`, the last state for each sample at index `i` in a batch will be used as initial state for the sample of index `i` in the following batch.\n",
    "\n",
    "    Said differently, whenever you train or test your LSTM, you first have to build your input matrix X\n",
    "    of shape `(nb_samples, timesteps, input_dim)` where your batch size divides `nb_samples`.  \n",
    "    For instance, if `nb_samples=1024` and `batch_size=64`, it means that your model will receive blocks of 64 samples,  \n",
    "    compute each output (whatever the number of `timesteps` is for every sample), average the gradients and propagate it to update the parameters vector.\n",
    "\n",
    "    By default, Keras shuffles (permutes) the samples in `X` and the dependencies between `Xi` and `Xi+1` are lost. Let’s assume there’s no shuffling in our explanation.\n",
    "\n",
    "    If the model is `stateless`, the cell states are reset at each sequence.  \n",
    "    _With the `stateful` model, **all the states are propagated to the next batch.**_  \n",
    "    It means that the state of the sample located at index `i`, `Xi` will be used in the computation of the sample `Xi+bs` in the next batch, where `bs` is the batch size (no `shuffling`).\n",
    "\n",
    "\n",
    "* __Why do Keras require the batch size in stateful mode?__\n",
    "\n",
    "    When the model is `stateless`, Keras allocates an array for the states of size `output_dim` (understand number of cells in your LSTM).  \n",
    "    At each sequence processing, this state array is reset.\n",
    "\n",
    "    __In Stateful model, Keras must propagate the previous states for each sample across the batches.__  \n",
    "    Referring to the explanation above, a sample at index `i` in batch `#1 (Xi+bs)` will know the states of the sample `i` in batch `#0 (Xi)`.  \n",
    "    In this case, the structure to store the states is of the shape `(batch_size, output_dim)`.  \n",
    "    This is the reason why you have to specify the batch size at the creation of the LSTM.  \n",
    "    If you don’t do so, Keras may raise an error to remind you: If a RNN is stateful, a complete `input_shape` must be provided (including batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RNN Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyRNNAttention (Feed-Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyRNNAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyRNNAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, h, c\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.Wh_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wh_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # Output(ht -> yt), hidden state\n",
    "        num = len(['h_tilda'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.Wh_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"f\"; forget gate\n",
    "        # f = sigmoid(xt * Ur + ht-1 * Wr + br) \n",
    "        # New f = sigmoid(xt * Ur + ht-1 * Wr + br + context * Vr)\n",
    "        # calculate the proposal \"h\"; hidden state for now(tilda)\n",
    "        # h_tilda = tanh(xt * Uh + ht-1 * Wh + bh)\n",
    "        # New h_tilda = tanh(xt * Uh + ht-1 * Wh + bh + context * Vh)\n",
    "        h_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ht_before, self.U[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        ht = h_tilda\n",
    "       \n",
    "        # Output Activation\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(\n",
    "                K.dot(yt_before, self.W_o)\n",
    "                + K.dot(ht, self.U_o)\n",
    "                + K.dot(context, self.C_o)\n",
    "                + self.b_o)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht]\n",
    "        else:\n",
    "            return yt, [yt, ht]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "#input_ = Embedding(ndimX, ndimY,\n",
    "#                        input_length=timestepX,\n",
    "#                        trainable=False,\n",
    "#                        #weights=[np.eye(ndimY)],\n",
    "#                        name='OneHot')#(input_)\n",
    "enc = Bidirectional(SimpleRNN(LATENT_DIM, return_sequences=True), merge_mode='concat')(input_)\n",
    "dec = MyRNNAttentionDecoder(LATENT_DIM, ndimY, output_activation='softmax')(enc)\n",
    "model = Model(inputs=input_, outputs=dec)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('EPOCH_NUM: %s, BATCH_SIZE %s' % (EPOCH_NUM, BATCH_SIZE))\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=2,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=True,\n",
    "                   #callbacks=[history],\n",
    "                  )\n",
    "\n",
    "# Save model\n",
    "model.save('rnn_attention_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = fitted.model.evaluate(train_X, train_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (train_accuracy * 100))\n",
    "\n",
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = output_decoder(train_Y)\n",
    "train_Y_hat = output_decoder(train_Y_hat_array)\n",
    "train_X_real = input_decoder(train_X)\n",
    "\n",
    "print(train_X_real[:3])\n",
    "print(train_Y_real[:3])\n",
    "print(train_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = fitted.model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (test_accuracy * 100))\n",
    "\n",
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = output_decoder(test_Y)\n",
    "test_Y_hat = output_decoder(test_Y_hat_array)\n",
    "test_X_real = input_decoder(test_X)\n",
    "\n",
    "print(test_X_real[:3])\n",
    "print(test_Y_real[:3])\n",
    "print(test_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LSTM Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyLSTMAttention (Feed-Forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lstm](keras_stateful_lstm_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyLSTMAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='sigmoid',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyLSTMAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None, None]  # y, h, c\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.Wh_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wh_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.Wc_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='Wc_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # Cell State : 'input gate'(sigmoid) + 'cell state'(tanh)\n",
    "        num = len(['forget_date', 'input_gate', 'output_gate', 'h_tilda'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.WW = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='WW',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.Wh_s))\n",
    "        c0 = activations.tanh(K.dot(inputs[:, 0], self.Wc_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0, c0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before, ct_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"f\"; forget gate\n",
    "        # f = sigmoid(xt * Uf + ht-1 * Wf + bf) \n",
    "        # New f = sigmoid(xt * Uf + ht-1 * Wf + bf + context * Vf)\n",
    "        ft = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ht_before, self.U[0])\n",
    "            + K.dot(ct_before, self.WW[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # now calculate the \"i\"; input gate\n",
    "        # i = sigmoid(xt * Ui + ht-1 * Wi + bi)\n",
    "        # New i = sigmoid(xt * Ui + ht-1 * Wi + bi + context * Vi)\n",
    "        it = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[1])\n",
    "            + K.dot(ht_before, self.U[1])\n",
    "            + K.dot(ct_before, self.WW[1])\n",
    "            + K.dot(context, self.V[1])\n",
    "            + self.b[1])\n",
    "\n",
    "        # calculate the proposal \"c\"; cell state for now(tilda)\n",
    "        # c_tilda = tanh(xt * Wh + (ht-1 * ft) * Uh + bh)\n",
    "        # New c_tilda = tanh(xt * Wh + (ht-1 * ft) * Uh + bh + context * Vh)\n",
    "        c_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[3])\n",
    "            + K.dot(ht_before, self.U[3])\n",
    "            + K.dot(context, self.V[3])\n",
    "            + self.b[3])\n",
    "        \n",
    "        # calculate the proposal \"c\"; cell state\n",
    "        # ct = ft * ct-1 + it * ct-tilda\n",
    "        ct = ft * ct_before + it * c_tilda\n",
    "\n",
    "        # now calculate the \"o\"; output gate\n",
    "        # o = sigmoid(xt * Wo + ht-1 * Uo + bo)\n",
    "        # New o = sigmoid(xt * Wo + ht-1 * Uo + bo + context * Vo)\n",
    "        ot = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[2])\n",
    "            + K.dot(ht_before, self.U[2])\n",
    "            + K.dot(ct, self.WW[2])\n",
    "            + K.dot(context, self.V[2])\n",
    "            + self.b[2])\n",
    "\n",
    "        \n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        #ht = (1 - zt) * h_tilda + zt * ht_before\n",
    "        ht = ot * activations.tanh(ct)\n",
    "\n",
    "        \n",
    "        # Output Activation\n",
    "        y_ = (K.dot(yt_before, self.W_o)\n",
    "              + K.dot(ht_before, self.U_o)\n",
    "              + K.dot(context, self.C_o)\n",
    "              + self.b_o)\n",
    "\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(y_)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(y_)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(y_)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht, ct]\n",
    "        else:\n",
    "            return yt, [yt, ht, ct]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NewLSTMAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMCell(Layer):\n",
    "    \"\"\"Cell class for the LSTM layer.\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step\n",
    "            (see [activations](../activations.md)).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix,\n",
    "            used for the linear transformation of the recurrent state.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        unit_forget_bias: Boolean.\n",
    "            If True, add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        recurrent_regularizer: Regularizer function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        recurrent_constraint: Constraint function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "        dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the inputs.\n",
    "        recurrent_dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the recurrent state.\n",
    "        implementation: Implementation mode, either 1 or 2.\n",
    "            Mode 1 will structure its operations as a larger number of\n",
    "            smaller dot products and additions, whereas mode 2 will\n",
    "            batch them into fewer, larger operations. These modes will\n",
    "            have different performance profiles on different hardware and\n",
    "            for different applications.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 **kwargs):\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.implementation = implementation\n",
    "        self.state_size = (self.units, self.units)\n",
    "        self._dropout_mask = None\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # Cell State : 'input gate'(sigmoid) + 'cell state'(tanh)\n",
    "        num = len(['h_tilda'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(shape, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                        self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                        initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                    ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.units]\n",
    "        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
    "        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_o = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_i = self.bias[:self.units]\n",
    "            self.bias_f = self.bias[self.units: self.units * 2]\n",
    "            self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_o = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_i = None\n",
    "            self.bias_f = None\n",
    "            self.bias_c = None\n",
    "            self.bias_o = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "            self._dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n",
    "                self.dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "        if (0 < self.recurrent_dropout < 1 and\n",
    "                self._recurrent_dropout_mask is None):\n",
    "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, self.units),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        # Context Vector : START ----\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(h_tm1, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # Context Vector : END ----\n",
    "        \n",
    "        if self.implementation == 1:\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "            x_i = K.dot(inputs_i, self.kernel_i)\n",
    "            x_f = K.dot(inputs_f, self.kernel_f)\n",
    "            x_c = K.dot(inputs_c, self.kernel_c)\n",
    "            x_o = K.dot(inputs_o, self.kernel_o)\n",
    "            if self.use_bias:\n",
    "                x_i = K.bias_add(x_i, self.bias_i)\n",
    "                x_f = K.bias_add(x_f, self.bias_f)\n",
    "                x_c = K.bias_add(x_c, self.bias_c)\n",
    "                x_o = K.bias_add(x_o, self.bias_o)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n",
    "                                                      self.recurrent_kernel_i)\n",
    "                                         + K.dot(context, s))\n",
    "            f = self.recurrent_activation(x_f + K.dot(h_tm1_f,\n",
    "                                                      self.recurrent_kernel_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n",
    "                                                            self.recurrent_kernel_c))\n",
    "            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n",
    "                                                      self.recurrent_kernel_o))\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "            z = K.dot(inputs, self.kernel)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            z += K.dot(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            i = self.recurrent_activation(z0)\n",
    "            f = self.recurrent_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.recurrent_activation(z3)\n",
    "\n",
    "        # Context Vector : START ----\n",
    "        # Context Vector : END ----\n",
    "        h = o * self.activation(c)\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'implementation': self.implementation}\n",
    "        base_config = super(LSTMCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class LSTM(RNN):\n",
    "    \"\"\"Long-Short Term Memory layer - Hochreiter 1997.\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step\n",
    "            (see [activations](../activations.md)).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix,\n",
    "            used for the linear transformation of the recurrent state.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        unit_forget_bias: Boolean.\n",
    "            If True, add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        recurrent_regularizer: Regularizer function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        recurrent_constraint: Constraint function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "        dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the inputs.\n",
    "        recurrent_dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the recurrent state.\n",
    "        implementation: Implementation mode, either 1 or 2.\n",
    "            Mode 1 will structure its operations as a larger number of\n",
    "            smaller dot products and additions, whereas mode 2 will\n",
    "            batch them into fewer, larger operations. These modes will\n",
    "            have different performance profiles on different hardware and\n",
    "            for different applications.\n",
    "        return_sequences: Boolean. Whether to return the last output.\n",
    "            in the output sequence, or the full sequence.\n",
    "        return_state: Boolean. Whether to return the last state\n",
    "            in addition to the output.\n",
    "        go_backwards: Boolean (default False).\n",
    "            If True, process the input sequence backwards and return the\n",
    "            reversed sequence.\n",
    "        stateful: Boolean (default False). If True, the last state\n",
    "            for each sample at index i in a batch will be used as initial\n",
    "            state for the sample of index i in the following batch.\n",
    "        unroll: Boolean (default False).\n",
    "            If True, the network will be unrolled,\n",
    "            else a symbolic loop will be used.\n",
    "            Unrolling can speed-up a RNN,\n",
    "            although it tends to be more memory-intensive.\n",
    "            Unrolling is only suitable for short sequences.\n",
    "    # References\n",
    "        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_recurrent_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 return_sequences=False,\n",
    "                 return_state=False,\n",
    "                 go_backwards=False,\n",
    "                 stateful=False,\n",
    "                 unroll=False,\n",
    "                 **kwargs):\n",
    "        if implementation == 0:\n",
    "            warnings.warn('`implementation=0` has been deprecated, '\n",
    "                          'and now defaults to `implementation=1`.'\n",
    "                          'Please update your layer call.')\n",
    "        if K.backend() == 'theano':\n",
    "            warnings.warn(\n",
    "                'RNN dropout is no longer supported with the Theano backend '\n",
    "                'due to technical limitations. '\n",
    "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
    "                'or use the TensorFlow backend.')\n",
    "            dropout = 0.\n",
    "            recurrent_dropout = 0.\n",
    "\n",
    "        cell = MyLSTMCell(units,\n",
    "                        activation=activation,\n",
    "                        recurrent_activation=recurrent_activation,\n",
    "                        use_bias=use_bias,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        recurrent_initializer=recurrent_initializer,\n",
    "                        unit_forget_bias=unit_forget_bias,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        recurrent_regularizer=recurrent_regularizer,\n",
    "                        bias_regularizer=bias_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        recurrent_constraint=recurrent_constraint,\n",
    "                        bias_constraint=bias_constraint,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout,\n",
    "                        implementation=implementation)\n",
    "        super(LSTM, self).__init__(cell,\n",
    "                                   return_sequences=return_sequences,\n",
    "                                   return_state=return_state,\n",
    "                                   go_backwards=go_backwards,\n",
    "                                   stateful=stateful,\n",
    "                                   unroll=unroll,\n",
    "                                   **kwargs)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        return super(LSTM, self).call(inputs,\n",
    "                                      mask=mask,\n",
    "                                      training=training,\n",
    "                                      initial_state=initial_state)\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self.cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.cell.activation\n",
    "\n",
    "    @property\n",
    "    def recurrent_activation(self):\n",
    "        return self.cell.recurrent_activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self.cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self.cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self.cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self.cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def unit_forget_bias(self):\n",
    "        return self.cell.unit_forget_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self.cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self.cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self.cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self.cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self.cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self.cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self.cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self.cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'implementation': self.implementation}\n",
    "        base_config = super(LSTM, self).get_config()\n",
    "        del base_config['cell']\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if 'implementation' in config and config['implementation'] == 0:\n",
    "            config['implementation'] = 1\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "def _generate_dropout_ones(inputs, dims):\n",
    "    # Currently, CTNK can't instantiate `ones` with symbolic shapes.\n",
    "    # Will update workaround once CTNK supports it.\n",
    "    if K.backend() == 'cntk':\n",
    "        ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n",
    "        return K.tile(ones, (1, dims))\n",
    "    else:\n",
    "        return K.ones((K.shape(inputs)[0], dims))\n",
    "\n",
    "\n",
    "def _generate_dropout_mask(ones, rate, training=None, count=1):\n",
    "    def dropped_inputs():\n",
    "        return K.dropout(ones, rate)\n",
    "\n",
    "    if count > 1:\n",
    "        return [K.in_train_phase(\n",
    "            dropped_inputs,\n",
    "            ones,\n",
    "            training=training) for _ in range(count)]\n",
    "    return K.in_train_phase(\n",
    "        dropped_inputs,\n",
    "        ones,\n",
    "training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "inputs shape: (?, ?, 128)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 79, 80)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 79, 128)           74240     \n",
      "_________________________________________________________________\n",
      "MyLSTMAttentionDecoder (MyLS (None, 79, 110)           139698    \n",
      "=================================================================\n",
      "Total params: 213,938\n",
      "Trainable params: 213,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = Bidirectional(LSTM(LATENT_DIM, return_sequences=True), merge_mode='concat')(input_)\n",
    "dec = MyLSTMAttentionDecoder(LATENT_DIM, ndimY, output_activation='softmax')(enc)\n",
    "model = Model(inputs=input_, outputs=dec)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_NUM: 100, BATCH_SIZE 256\n",
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      " - 60s - loss: 1.0239 - acc: 0.0695 - val_loss: 1.2517 - val_acc: 0.0949\n",
      "Epoch 2/100\n",
      " - 59s - loss: 0.9392 - acc: 0.0818 - val_loss: 1.2276 - val_acc: 0.0998\n",
      "Epoch 3/100\n",
      " - 59s - loss: 0.9158 - acc: 0.0884 - val_loss: 1.2060 - val_acc: 0.1064\n",
      "Epoch 4/100\n",
      " - 59s - loss: 0.8981 - acc: 0.0926 - val_loss: 1.1929 - val_acc: 0.1094\n",
      "Epoch 5/100\n",
      " - 59s - loss: 0.8854 - acc: 0.0958 - val_loss: 1.1824 - val_acc: 0.1115\n",
      "Epoch 6/100\n",
      " - 59s - loss: 0.8745 - acc: 0.0988 - val_loss: 1.1730 - val_acc: 0.1143\n",
      "Epoch 7/100\n",
      " - 59s - loss: 0.8643 - acc: 0.1018 - val_loss: 1.1646 - val_acc: 0.1160\n",
      "Epoch 8/100\n",
      " - 59s - loss: 0.8552 - acc: 0.1045 - val_loss: 1.1568 - val_acc: 0.1173\n",
      "Epoch 9/100\n",
      " - 59s - loss: 0.8467 - acc: 0.1069 - val_loss: 1.1496 - val_acc: 0.1201\n",
      "Epoch 10/100\n",
      " - 59s - loss: 0.8397 - acc: 0.1088 - val_loss: 1.1446 - val_acc: 0.1214\n",
      "Epoch 11/100\n",
      " - 59s - loss: 0.8329 - acc: 0.1108 - val_loss: 1.1414 - val_acc: 0.1232\n",
      "Epoch 12/100\n",
      " - 59s - loss: 0.8266 - acc: 0.1124 - val_loss: 1.1349 - val_acc: 0.1244\n",
      "Epoch 13/100\n",
      " - 59s - loss: 0.8214 - acc: 0.1137 - val_loss: 1.1331 - val_acc: 0.1244\n",
      "Epoch 14/100\n",
      " - 59s - loss: 0.8165 - acc: 0.1148 - val_loss: 1.1278 - val_acc: 0.1254\n",
      "Epoch 15/100\n",
      " - 59s - loss: 0.8122 - acc: 0.1160 - val_loss: 1.1222 - val_acc: 0.1267\n",
      "Epoch 16/100\n",
      " - 59s - loss: 0.8083 - acc: 0.1169 - val_loss: 1.1207 - val_acc: 0.1273\n",
      "Epoch 17/100\n",
      " - 59s - loss: 0.8045 - acc: 0.1179 - val_loss: 1.1187 - val_acc: 0.1284\n",
      "Epoch 18/100\n",
      " - 59s - loss: 0.8010 - acc: 0.1187 - val_loss: 1.1171 - val_acc: 0.1282\n",
      "Epoch 19/100\n",
      " - 59s - loss: 0.7979 - acc: 0.1195 - val_loss: 1.1153 - val_acc: 0.1295\n",
      "Epoch 20/100\n",
      " - 59s - loss: 0.7948 - acc: 0.1203 - val_loss: 1.1139 - val_acc: 0.1295\n",
      "Epoch 21/100\n",
      " - 59s - loss: 0.7917 - acc: 0.1209 - val_loss: 1.1121 - val_acc: 0.1293\n",
      "Epoch 22/100\n",
      " - 59s - loss: 0.7890 - acc: 0.1216 - val_loss: 1.1117 - val_acc: 0.1298\n",
      "Epoch 23/100\n",
      " - 59s - loss: 0.7867 - acc: 0.1221 - val_loss: 1.1074 - val_acc: 0.1303\n",
      "Epoch 24/100\n",
      " - 59s - loss: 0.7841 - acc: 0.1227 - val_loss: 1.1074 - val_acc: 0.1309\n",
      "Epoch 25/100\n",
      " - 59s - loss: 0.7819 - acc: 0.1231 - val_loss: 1.1067 - val_acc: 0.1305\n",
      "Epoch 26/100\n",
      " - 59s - loss: 0.7797 - acc: 0.1237 - val_loss: 1.1051 - val_acc: 0.1309\n",
      "Epoch 27/100\n",
      " - 59s - loss: 0.7776 - acc: 0.1241 - val_loss: 1.1053 - val_acc: 0.1310\n",
      "Epoch 28/100\n",
      " - 59s - loss: 0.7756 - acc: 0.1247 - val_loss: 1.1032 - val_acc: 0.1314\n",
      "Epoch 29/100\n",
      " - 59s - loss: 0.7735 - acc: 0.1251 - val_loss: 1.1026 - val_acc: 0.1315\n",
      "Epoch 30/100\n",
      " - 59s - loss: 0.7720 - acc: 0.1255 - val_loss: 1.1009 - val_acc: 0.1314\n",
      "Epoch 31/100\n",
      " - 59s - loss: 0.7705 - acc: 0.1260 - val_loss: 1.1020 - val_acc: 0.1319\n",
      "Epoch 32/100\n",
      " - 59s - loss: 0.7683 - acc: 0.1265 - val_loss: 1.0998 - val_acc: 0.1319\n",
      "Epoch 33/100\n",
      " - 59s - loss: 0.7670 - acc: 0.1267 - val_loss: 1.1025 - val_acc: 0.1321\n",
      "Epoch 34/100\n",
      " - 59s - loss: 0.7650 - acc: 0.1273 - val_loss: 1.1001 - val_acc: 0.1324\n",
      "Epoch 35/100\n",
      " - 59s - loss: 0.7641 - acc: 0.1275 - val_loss: 1.0982 - val_acc: 0.1330\n",
      "Epoch 36/100\n",
      " - 59s - loss: 0.7625 - acc: 0.1279 - val_loss: 1.0992 - val_acc: 0.1327\n",
      "Epoch 37/100\n",
      " - 59s - loss: 0.7606 - acc: 0.1285 - val_loss: 1.0996 - val_acc: 0.1327\n",
      "Epoch 38/100\n",
      " - 59s - loss: 0.7594 - acc: 0.1287 - val_loss: 1.1015 - val_acc: 0.1330\n",
      "Epoch 39/100\n",
      " - 59s - loss: 0.7581 - acc: 0.1290 - val_loss: 1.1003 - val_acc: 0.1329\n",
      "Epoch 40/100\n",
      " - 59s - loss: 0.7568 - acc: 0.1293 - val_loss: 1.0990 - val_acc: 0.1324\n",
      "Epoch 41/100\n",
      " - 59s - loss: 0.7557 - acc: 0.1296 - val_loss: 1.0994 - val_acc: 0.1336\n",
      "Epoch 42/100\n",
      " - 59s - loss: 0.7542 - acc: 0.1299 - val_loss: 1.0987 - val_acc: 0.1331\n",
      "Epoch 43/100\n",
      " - 59s - loss: 0.7532 - acc: 0.1303 - val_loss: 1.0982 - val_acc: 0.1330\n",
      "Epoch 44/100\n",
      " - 59s - loss: 0.7518 - acc: 0.1306 - val_loss: 1.0978 - val_acc: 0.1332\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c25eedef71b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# 1: progress bar, 2: one line per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    \u001b[0;31m#validation_data=(testX, testY),  # Validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m#callbacks=[history],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   )\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LOAD_MODEL = True\n",
    "print('EPOCH_NUM: %s, BATCH_SIZE %s' % (EPOCH_NUM, BATCH_SIZE))\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = load_model('lstm_attention_embedding_model.h5')\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=2,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=True,\n",
    "                   #callbacks=[history],\n",
    "                  )\n",
    "\n",
    "# Save model\n",
    "model.save('lstm_attention_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = fitted.model.evaluate(train_X, train_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (train_accuracy * 100))\n",
    "\n",
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = output_decoder(train_Y)\n",
    "train_Y_hat = output_decoder(train_Y_hat_array)\n",
    "train_X_real = input_decoder(train_X)\n",
    "\n",
    "print(train_X_real[:3])\n",
    "print(train_Y_real[:3])\n",
    "print(train_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = fitted.model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (test_accuracy * 100))\n",
    "\n",
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = output_decoder(test_Y)\n",
    "test_Y_hat = output_decoder(test_Y_hat_array)\n",
    "test_X_real = input_decoder(test_X)\n",
    "\n",
    "print(test_X_real[:3])\n",
    "print(test_Y_real[:3])\n",
    "print(test_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GRU Decoder with Attention (encoder: `return_sequence=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_attention'](rnn_with_att.jpg)\n",
    "!['Overview of the Attention mechanism in an Encoder-Decoder setup'](lstm_attention_3.png)\n",
    "!['detail_lstm_attention'](detail_attentionmodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Attention Structure 1](https://blog.heuritech.com/2016/01/20/attention-mechanism/)  \n",
    "[Attention Structure 2](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  \n",
    "[Attention Structure 3](https://medium.com/datalogue/attention-in-keras-1892773a4f22)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyGRUAttention (Feed-Forward, Not Recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import _object_list_uid\n",
    "from keras.utils.generic_utils import has_arg\n",
    "\n",
    "# Legacy support.\n",
    "from keras.legacy.layers import Recurrent\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.layers import RNN, GRUCell, GRU, SimpleRNNCell\n",
    "#from keras.layers.recurrent import (_generate_dropout_ones,\n",
    "#                                    _generate_dropout_mask)\n",
    "\n",
    "\n",
    "def _generate_dropout_ones(inputs, dims):\n",
    "    # Currently, CTNK can't instantiate `ones` with symbolic shapes.\n",
    "    # Will update workaround once CTNK supports it.\n",
    "    if K.backend() == 'cntk':\n",
    "        ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n",
    "        return K.tile(ones, (1, dims))\n",
    "    else:\n",
    "        return K.ones((K.shape(inputs)[0], dims))\n",
    "\n",
    "\n",
    "def _generate_dropout_mask(ones, rate, training=None, count=1):\n",
    "    def dropped_inputs():\n",
    "        return K.dropout(ones, rate)\n",
    "\n",
    "    if count > 1:\n",
    "        return [K.in_train_phase(\n",
    "            dropped_inputs,\n",
    "            ones,\n",
    "            training=training) for _ in range(count)]\n",
    "    return K.in_train_phase(\n",
    "        dropped_inputs,\n",
    "        ones,\n",
    "        training=training)\n",
    "\n",
    "\n",
    "class GRUAttentionCell(GRUCell):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        #self.state_size = self.units :for RNN, GRU\n",
    "        #self.state_size = (self.units, self.units) : for LSTM\n",
    "        #self.attn_length = 79\n",
    "        #self.attn_size = 79\n",
    "        #self.attn_vec_size = self.attn_size\n",
    "        #self.input_size = None\n",
    "        self.state_size = (self.units, 79*self.units)\n",
    "        #self.state_size = (self.units, self.units)\n",
    "        self.full_inputs = None\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print('cell.build Shape:', input_shape)\n",
    "        #self.timesteps = input_shape[1]\n",
    "        input_dim = input_shape[-1]\n",
    "        self.states = [None, None]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.context_kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 3),\n",
    "            name='context_kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_z = self.kernel[:,               : self.units * 1]\n",
    "        self.kernel_r = self.kernel[:, self.units * 1: self.units * 2]\n",
    "        self.kernel_h = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_c = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_z = self.recurrent_kernel[:,               : self.units * 1]\n",
    "        self.recurrent_kernel_r = self.recurrent_kernel[:, self.units * 1: self.units * 2]\n",
    "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        self.context_kernel_z = self.context_kernel[:,               : self.units * 1]\n",
    "        self.context_kernel_r = self.context_kernel[:, self.units * 1: self.units * 2]\n",
    "        self.context_kernel_h = self.context_kernel[:, self.units * 2:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_z = self.bias[              : self.units * 1]\n",
    "            self.bias_r = self.bias[self.units * 1: self.units * 2]\n",
    "            self.bias_h = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_c = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_z = None\n",
    "            self.bias_r = None\n",
    "            self.bias_h = None\n",
    "            self.bias_c = None\n",
    "        self.built = True\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameter Shapes\n",
    "        kernel : (input_dim, units)\n",
    "        recurrent_kernel : (units, units)\n",
    "        bias : (units, )\n",
    "\n",
    "        # Inherited Parameters\n",
    "        kernel : z, r, h\n",
    "        recurrent_kernel : z, r, h\n",
    "        bias : z, r, h\n",
    "\n",
    "        # New Parameters\n",
    "        input_dim\n",
    "        kernel_c\n",
    "        recurrent_kernel_c\n",
    "        bias_c\n",
    "        \"\"\"\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        \n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        if self.full_inputs is None:\n",
    "            self.full_inputs = states[-1]\n",
    "        full_inputs = self.full_inputs\n",
    "        timesteps = K.int_shape(full_inputs)[1]\n",
    "        print('cell.call Input Shape:', K.int_shape(inputs),\n",
    "              'Full State len:', len(states),\n",
    "              'State Shape:', K.int_shape(states[0]),\n",
    "              'Full h Shape:', K.int_shape(states[-1]))\n",
    "\n",
    "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "            self._dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n",
    "                self.dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "        if (0 < self.recurrent_dropout < 1 and\n",
    "                self._recurrent_dropout_mask is None):\n",
    "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, self.units),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "                inputs_c = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "                inputs_c = inputs\n",
    "            x_z = K.dot(inputs_z, self.kernel_z)\n",
    "            x_r = K.dot(inputs_r, self.kernel_r)\n",
    "            x_h = K.dot(inputs_h, self.kernel_h)\n",
    "            x_c = K.dot(inputs_c, self.kernel_c)\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.bias_z)\n",
    "                x_r = K.bias_add(x_r, self.bias_r)\n",
    "                x_h = K.bias_add(x_h, self.bias_h)\n",
    "                x_c = K.bias_add(x_c, self.bias_c)\n",
    "\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "\n",
    "            # calculate the context vector\n",
    "            #context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "\n",
    "            # Attention Context Part\n",
    "            h_tm1_c = K.repeat(h_tm1_c, timesteps)\n",
    "            #h_tm1_c = full_inputs\n",
    "            print('h:', K.int_shape(full_inputs), 's0:', K.int_shape(h_tm1_c))\n",
    "            e = self.activation(h_tm1_c + K.dot(full_inputs, self.recurrent_kernel_c))\n",
    "            a = K.softmax(e)\n",
    "            print('A:', K.int_shape(a), 'inputs_c:', K.int_shape(inputs_c), 'full input:', K.int_shape(full_inputs))\n",
    "            c_t = K.sum(a * full_inputs, axis=1, keepdims=False)\n",
    "            #c_t = K.batch_dot(a, inputs_c, axes=1)\n",
    "\n",
    "            print('Done c_t:', K.int_shape(c_t))\n",
    "            print('Done context_kernel:', K.int_shape(self.context_kernel_z))\n",
    "            print('Done recurrent_kernel:', self.recurrent_kernel_z)\n",
    "            print('Done h_tm_z * rec:', K.dot(h_tm1_z, self.recurrent_kernel_z))\n",
    "            \n",
    "            # GRU Part\n",
    "            z = self.recurrent_activation(x_z +\n",
    "                                          K.dot(h_tm1_z,\n",
    "                                                self.recurrent_kernel_z) +\n",
    "                                          K.dot(c_t, self.context_kernel_z))\n",
    "            r = self.recurrent_activation(x_r +\n",
    "                                          K.dot(h_tm1_r,\n",
    "                                                self.recurrent_kernel_r) +\n",
    "                                          K.dot(c_t, self.context_kernel_r))\n",
    "\n",
    "            hh = self.activation(x_h +\n",
    "                                 K.dot(r * h_tm1_h,\n",
    "                                       self.recurrent_kernel_h) +\n",
    "                                 K.dot(c_t, self.context_kernel_h))\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "            matrix_x = K.dot(inputs, self.kernel)\n",
    "            if self.use_bias:\n",
    "                matrix_x = K.bias_add(matrix_x, self.bias)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            matrix_inner = K.dot(h_tm1,\n",
    "                                 self.recurrent_kernel[:, :2 * self.units])\n",
    "\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            recurrent_z = matrix_inner[:, :self.units]\n",
    "            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            recurrent_h = K.dot(r * h_tm1,\n",
    "                                self.recurrent_kernel[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "                \n",
    "        print(K.int_shape(states[-1]))\n",
    "        states = [h] + [states[-1]]\n",
    "        return h, [h, self.full_inputs]\n",
    "\n",
    "\n",
    "class MyRNNAttention(RNN):\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # build an all-zero tensor of shape (samples, output_dim)\n",
    "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
    "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
    "        if hasattr(self.cell.state_size, '__len__'):\n",
    "            state = [K.tile(initial_state, [1, dim])\n",
    "                     for dim in self.cell.state_size[:-1]]\n",
    "        else:\n",
    "            state = [K.tile(initial_state, [1, self.cell.state_size])]\n",
    "\n",
    "        full_inputs = [inputs]\n",
    "        return state + full_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "79 80\n",
      "cell.build Shape: (None, 80)\n",
      "cell.call Input Shape: (None, 80) Full State len: 2 State Shape: (None, 110) Full h Shape: (None, 79, 80)\n",
      "h: (None, 79, 80) s0: (None, 79, 110)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 80 and 110 for 'my_rnn_attention_6/MatMul_4' (op: 'MatMul') with input shapes: [?,80], [110,110].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 80 and 110 for 'my_rnn_attention_6/MatMul_4' (op: 'MatMul') with input shapes: [?,80], [110,110].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-049b7e6246f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mGRUAttention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyRNNAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRUAttentionCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRUAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    587\u001b[0m                                              \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                                              \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                                              input_length=timesteps)\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m         output_ta = tensor_array_ops.TensorArray(\n\u001b[1;32m   2563\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         last_output, outputs, states = K.rnn(step,\n",
      "\u001b[0;32m<ipython-input-435-f19f6a0e6532>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m#h_tm1_c = full_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's0:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_kernel_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inputs_c:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full input:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_permute_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         return tf.reshape(tf.matmul(xt, yt),\n\u001b[0m\u001b[1;32m   1053\u001b[0m                           x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1891\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2436\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2438\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 80 and 110 for 'my_rnn_attention_6/MatMul_4' (op: 'MatMul') with input shapes: [?,80], [110,110]."
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "print(timestepX, ndimX)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = LSTM(ndimX, return_sequences=True)(input_)\n",
    "GRUAttention = MyRNNAttention(GRUAttentionCell(ndimY), return_sequences=True)\n",
    "dec = GRUAttention(enc)\n",
    "act = Dense(ndimY, activation='softmax')(dec)\n",
    "model = Model(inputs=input_, outputs=act)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MinimalRNNCell(Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        #self.state_size = units\n",
    "        self.state_size = (units, units * 79)\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer has never been called and thus has no defined input shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-456-aa30eef606a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinimalRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36minput_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \"\"\"\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             raise AttributeError('The layer has never been called '\n\u001b[0m\u001b[1;32m   1029\u001b[0m                                  'and thus has no defined input shape.')\n\u001b[1;32m   1030\u001b[0m         \u001b[0mall_input_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The layer has never been called and thus has no defined input shape."
     ]
    }
   ],
   "source": [
    "a = MinimalRNNCell(ndimY)\n",
    "a.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "79 80\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An initial_state was passed that is not compatible with `cell.state_size`. Received `state_spec`=[<keras.engine.topology.InputSpec object at 0x7f70d9075a90>]; However `cell.state_size` is (110, 8690)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-453-bee2953dda3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Cel = MinimalRNNCell(ndimY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMinimalRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    457\u001b[0m                     \u001b[0;34m'`cell.state_size`. Received `state_spec`={}; '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;34m'However `cell.state_size` is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                     '{}'.format(self.state_spec, self.cell.state_size))\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             self.state_spec = [InputSpec(shape=(None, dim))\n",
      "\u001b[0;31mValueError\u001b[0m: An initial_state was passed that is not compatible with `cell.state_size`. Received `state_spec`=[<keras.engine.topology.InputSpec object at 0x7f70d9075a90>]; However `cell.state_size` is (110, 8690)"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "print(timestepX, ndimX)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = LSTM(ndimX, return_sequences=True)(input_)\n",
    "#Cel = MinimalRNNCell(ndimY)\n",
    "dec = RNN(MinimalRNNCell(ndimY), return_sequences=True)(enc, enc)\n",
    "act = Dense(ndimY, activation='softmax')(dec)\n",
    "model = Model(inputs=input_, outputs=act)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### class CellWrapper(Layer):\n",
    "    def __init__(self, cell, **kwargs):\n",
    "        self._cell = cell\n",
    "        super(CellWrapper, self).__init__(**kwargs)\n",
    "        #self._cell.__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self._cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self._cell.activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self._cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self._cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self._cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self._cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self._cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self._cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self._cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self._cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self._cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self._cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self._cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self._cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        return self._cell.output_shape\n",
    "\n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self._cell.weights, self.weights]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [self._cell.get_weights(), self.get_weights()]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._cell.build(input_shape)\n",
    "        \n",
    "        if self.built is None:\n",
    "            self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "\n",
    "        cell_output, new_state = self._cell.call(inputs, states, training=None)\n",
    "\n",
    "        output = cell_output\n",
    "        state = new_state\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "79 80\n",
      "cell.build Shape: (None, 80)\n",
      "cell.call Input Shape: (None, 80) Full State len: 2 State Shape: (None, 80) Full h Shape: (None, 6320)\n",
      "h: (None, 6320) s0: (None, 6320, 80)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 6320 and 80 for 'rnn_26/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 6320 and 80 for 'rnn_26/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-441-0e340b8850aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mCel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRUAttentionCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    587\u001b[0m                                              \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                                              \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                                              input_length=timesteps)\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m         output_ta = tensor_array_ops.TensorArray(\n\u001b[1;32m   2563\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         last_output, outputs, states = K.rnn(step,\n",
      "\u001b[0;32m<ipython-input-435-f19f6a0e6532>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m#h_tm1_c = full_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's0:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_kernel_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inputs_c:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full input:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1891\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2436\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2438\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 6320 and 80 for 'rnn_26/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80]."
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "print(timestepX, ndimX)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = LSTM(ndimX, return_sequences=True)(input_)\n",
    "Cel = GRUAttentionCell(ndimX)\n",
    "dec = RNN(Cel, return_sequences=True)(enc)\n",
    "act = Dense(ndimY, activation='softmax')(dec)\n",
    "model = Model(inputs=input_, outputs=act)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CellWrapper(Layer):\n",
    "    def __init__(self, cell, **kwargs):\n",
    "        self._cell = cell\n",
    "        super(CellWrapper, self).__init__(**kwargs)\n",
    "        #self._cell.__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self._cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self._cell.activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self._cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self._cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self._cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self._cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self._cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self._cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self._cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self._cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self._cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self._cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self._cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self._cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        return self._cell.output_shape\n",
    "\n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        inner_weights = self._cell.weights\n",
    "        wrapper_weights = super(CellWrapper, self).weights\n",
    "        return [inner_weights, inner_weights]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [self._cell.get_weights(), super(CellWrapper, self).get_weights()]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._cell.build(input_shape)\n",
    "        \n",
    "        if self.built is None:\n",
    "            self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "\n",
    "        cell_output, new_state = self._cell.call(inputs, states, training=None)\n",
    "\n",
    "        output = cell_output\n",
    "        state = new_state\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CellWrapper(SimpleRNNCell):\n",
    "    def __init__(self, cell, *args, **kwargs):\n",
    "        self._cell = cell\n",
    "        super(CellWrapper, self).__init__(cell.units, **kwargs)\n",
    "        #self._cell.__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._cell.build(input_shape)\n",
    "        if self.built is None:\n",
    "            self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "\n",
    "        cell_output, new_state = self._cell.call(inputs, states, training=None)\n",
    "\n",
    "        output = cell_output\n",
    "        state = new_state\n",
    "        \n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from keras import activations\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine import Layer\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import has_arg\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "\n",
    "\n",
    "class _RNNAttentionCell(Layer):\n",
    "    \"\"\"Base class for recurrent attention mechanisms.\n",
    "\n",
    "    This base class implements the RNN cell interface and defines a standard\n",
    "    way for attention mechanisms to interact with a (wrapped) \"core\" RNN cell\n",
    "    (such as the `SimpleRNNCell`, `GRUCell` or `LSTMCell`).\n",
    "\n",
    "    The main idea is that the attention mechanism, implemented by\n",
    "    `attention_call` in extensions of this class, computes an \"attention\n",
    "    encoding\", based on the attended input as well as the input and the core\n",
    "    cell state(s) at the current time step, which will be used as modified\n",
    "    input for the core cell.\n",
    "\n",
    "    # Arguments\n",
    "        cell: A RNN cell instance. The cell to wrap by the attention mechanism.\n",
    "            A RNN cell is a class that has:\n",
    "            - a `call(input_at_t, states_at_t)` method, returning\n",
    "                `(output_at_t, states_at_t_plus_1)`.\n",
    "            - a `state_size` attribute. This can be a single integer\n",
    "                (single state) in which case it is the size of the recurrent\n",
    "                state (which should be the same as the size of the cell\n",
    "                output). This can also be a list/tuple of integers (one size\n",
    "                per state). In this case, the first entry (`state_size[0]`)\n",
    "                should be the same as the size of the cell output.\n",
    "        attend_after: Boolean (default False). If True, the attention\n",
    "            transformation defined by `attention_call` will be applied after\n",
    "            the core cell transformation (and the attention encoding will be\n",
    "            used as input for core cell transformation next time step).\n",
    "        concatenate_input: Boolean (default True). If True the concatenation of\n",
    "            the attention encoding and the original input will be used as input\n",
    "            for the core cell transformation. If set to False, only the\n",
    "            attention encoding will be used as input for the core cell\n",
    "            transformation.\n",
    "\n",
    "    # Abstract Methods and Properties\n",
    "        Extension of this class must implement:\n",
    "            - `attention_build` (method): Builds the attention transformation\n",
    "              based on input shapes.\n",
    "            - `attention_call` (method): Defines the attention transformation\n",
    "              returning the attention encoding.\n",
    "            - `attention_size` (property): After `attention_build` has been\n",
    "              called, this property should return the size (int) of the\n",
    "              attention encoding. Do this by setting `_attention_size` in scope\n",
    "              of `attention_build` or by implementing `attention_size`\n",
    "              property.\n",
    "        Extension of this class can optionally implement:\n",
    "            - `attention_state_size` (property): Default [`attention_size`].\n",
    "              If the attention mechanism has it own internal states (besides\n",
    "              the attention encoding which is by default the only part of\n",
    "              `attention_states`) override this property accordingly.\n",
    "        See docs of the respective method/property for further details.\n",
    "\n",
    "    # Details of interaction between attention and cell transformations\n",
    "        Let \"cell\" denote core (wrapped) RNN cell and \"att(cell)\" the complete\n",
    "        attentive RNN cell defined by this class. We write the core cell\n",
    "        transformation as:\n",
    "\n",
    "            y{t}, s_cell{t+1} = cell.call(x{t}, s_cell{t})\n",
    "\n",
    "        where y{t} denotes the output, x{t} the input at and s_cell{t} the core\n",
    "        cell state(s) at time t and s_cell{t+1} the updated state(s).\n",
    "\n",
    "        We can then write the complete \"attentive\" cell transformation as:\n",
    "\n",
    "            y{t}, s_att(cell){t+1} = att(cell).call(x{t}, s_att(cell){t},\n",
    "                                                    constants=attended)\n",
    "\n",
    "        where s_att(cell) denotes the complete states of the attentive cell,\n",
    "        which consists of the core cell state(s) followed but the attention\n",
    "        state(s), and attended denotes the tensor attended to (note: no time\n",
    "        indexing as this is the same constant input at each time step).\n",
    "\n",
    "        Internally, this is how the attention transformation, implemented by\n",
    "        `attention_call`, interacts with the core cell transformation\n",
    "        `cell.call`:\n",
    "\n",
    "        - with `attend_after=False` (default):\n",
    "            a{t}, s_att{t+1} = att(cell).attention_call(x_t, s_cell{t},\n",
    "                                                        attended, s_att{t})\n",
    "            with `concatenate_input=True` (default):\n",
    "                x'{t} = [x{t}, a{t}]\n",
    "            else:\n",
    "                x'{t} = a{t}\n",
    "            y{t}, s_cell{t+1} = cell.call(x'{t}, s_cell{t})\n",
    "\n",
    "        - with `attend_after=True`:\n",
    "            with `concatenate_input=True` (default):\n",
    "                x'{t} = [x{t}, a{t-1}]\n",
    "            else:\n",
    "                x'{t} = a{t-1}\n",
    "            y{t}, s_cell{t+1} = cell.call(x'{t}, s_cell{t})\n",
    "            a{t}, s_att{t+1} = att(cell).attention_call(x_t, s_cell{t+1},\n",
    "                                                        attended, s_att{t})\n",
    "\n",
    "        where a{t} denotes the attention encoding, s_att{t} the attention\n",
    "        state(s), x'{t} the modified core cell input and [x{.}, a{.}] the\n",
    "        (tensor) concatenation of the input and attention encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cell,\n",
    "                 attend_after=False,\n",
    "                 concatenate_input=False,\n",
    "                 **kwargs):\n",
    "        self.cell = cell  # must be set before calling super\n",
    "        super(_RNNAttentionCell, self).__init__(**kwargs)\n",
    "        self.attend_after = attend_after\n",
    "        self.concatenate_input = concatenate_input\n",
    "        self.attended_spec = None\n",
    "        self._attention_size = None\n",
    "\n",
    "    def attention_call(self,\n",
    "                       inputs,\n",
    "                       cell_states,\n",
    "                       attended,\n",
    "                       attention_states,\n",
    "                       training=None):\n",
    "        \"\"\"The main logic for computing the attention encoding.\n",
    "\n",
    "        # Arguments\n",
    "            inputs: The input at current time step.\n",
    "            cell_states: States for the core RNN cell.\n",
    "            attended: The same tensor(s) to attend at each time step.\n",
    "            attention_states: States dedicated for the attention mechanism.\n",
    "            training: whether run in training mode or not\n",
    "\n",
    "        # Returns\n",
    "            attention_h: The computed attention encoding at current time step.\n",
    "            attention_states: States to be passed to next `attention_call`. By\n",
    "                default this should be [`attention_h`].\n",
    "                NOTE: if additional states are used, these should be appended\n",
    "                after `attention_h`, i.e. `attention_states[0]` should always\n",
    "                be `attention_h`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            '`attention_call` must be implemented by extensions of `{}`'.format(\n",
    "                self.__class__.__name__))\n",
    "\n",
    "    def attention_build(self, input_shape, cell_state_size, attended_shape):\n",
    "        \"\"\"Build the attention mechanism.\n",
    "\n",
    "        NOTE: `self._attention_size` should be set in this method to the size\n",
    "        of the attention encoding (i.e. size of first `attention_states`)\n",
    "        unless `attention_size` property is implemented in another way.\n",
    "\n",
    "        # Arguments\n",
    "            input_shape: Tuple of integers. Shape of the input at a single time\n",
    "                step.\n",
    "            cell_state_size: List of tuple of integers.\n",
    "            attended_shape: List of tuple of integers.\n",
    "\n",
    "            NOTE: both `cell_state_size` and `attended_shape` will always be\n",
    "            lists - for simplicity. For example: even if (wrapped)\n",
    "            `cell.state_size` is an integer, `cell_state_size` will be a list\n",
    "            of this one element.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            '`attention_build` must be implemented by extensions of `{}`'.format(\n",
    "                self.__class__.__name__))\n",
    "\n",
    "    @property\n",
    "    def attention_size(self):\n",
    "        \"\"\"Size off attention encoding, an integer.\n",
    "        \"\"\"\n",
    "        if self._attention_size is None and self.built:\n",
    "            raise NotImplementedError(\n",
    "                'extensions of `{}` must either set property `_attention_size`'\n",
    "                ' in `attention_build` or implement the or implement'\n",
    "                ' `attention_size` in some other way'.format(\n",
    "                    self.__class__.__name__))\n",
    "\n",
    "        return self._attention_size\n",
    "\n",
    "    @property\n",
    "    def attention_state_size(self):\n",
    "        \"\"\"Size of attention states, defaults to `attention_size`, an integer.\n",
    "\n",
    "        Modify this property to return list of integers if the attention\n",
    "        mechanism has several internal states. Note that the first size should\n",
    "        always be the size of the attention encoding, i.e.:\n",
    "            `attention_state_size[0]` = `attention_size`\n",
    "        \"\"\"\n",
    "        return self.attention_size\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        \"\"\"Size of states of the complete attentive cell, a tuple of integers.\n",
    "\n",
    "        The attentive cell's states consists of the core RNN cell state size(s)\n",
    "        followed by attention state size(s). NOTE it is important that the core\n",
    "        cell states are first as the first state of any RNN cell should be same\n",
    "        as the cell's output.\n",
    "        \"\"\"\n",
    "        state_size_s = []\n",
    "        for state_size in [self.cell.state_size, self.attention_state_size]:\n",
    "            if hasattr(state_size, '__len__'):\n",
    "                state_size_s += list(state_size)\n",
    "            else:\n",
    "                state_size_s.append(state_size)\n",
    "\n",
    "        return tuple(state_size_s)\n",
    "\n",
    "    def call(self, inputs, states, constants, training=None):\n",
    "        \"\"\"Complete attentive cell transformation.\n",
    "        \"\"\"\n",
    "        attended = constants\n",
    "        cell_states = states[:self._num_wrapped_states]\n",
    "        attention_states = states[self._num_wrapped_states:]\n",
    "\n",
    "        if self.attend_after:\n",
    "            attention_call = self.call_attend_after\n",
    "        else:\n",
    "            attention_call = self.call_attend_before\n",
    "\n",
    "        return attention_call(inputs=inputs,\n",
    "                              cell_states=cell_states,\n",
    "                              attended=attended,\n",
    "                              attention_states=attention_states,\n",
    "                              training=training)\n",
    "\n",
    "    def call_attend_before(self,\n",
    "                           inputs,\n",
    "                           cell_states,\n",
    "                           attended,\n",
    "                           attention_states,\n",
    "                           training=None):\n",
    "        \"\"\"Complete attentive cell transformation, if `attend_after=False`.\n",
    "        \"\"\"\n",
    "        attention_h, new_attention_states = self.attention_call(\n",
    "            inputs=inputs,\n",
    "            cell_states=cell_states,\n",
    "            attended=attended,\n",
    "            attention_states=attention_states,\n",
    "            training=training)\n",
    "\n",
    "        if self.concatenate_input:\n",
    "            cell_input = concatenate([attention_h, inputs])\n",
    "        else:\n",
    "            cell_input = attention_h\n",
    "\n",
    "        if has_arg(self.cell.call, 'training'):\n",
    "            output, new_cell_states = self.cell.call(cell_input, cell_states,\n",
    "                                                     training=training)\n",
    "        else:\n",
    "            output, new_cell_states = self.cell.call(cell_input, cell_states)\n",
    "\n",
    "        return output, new_cell_states + new_attention_states\n",
    "\n",
    "    def call_attend_after(self,\n",
    "                          inputs,\n",
    "                          cell_states,\n",
    "                          attended,\n",
    "                          attention_states,\n",
    "                          training=None):\n",
    "        \"\"\"Complete attentive cell transformation, if `attend_after=True`.\n",
    "        \"\"\"\n",
    "        attention_h_previous = attention_states[0]\n",
    "\n",
    "        if self.concatenate_input:\n",
    "            cell_input = concatenate([attention_h_previous, inputs])\n",
    "        else:\n",
    "            cell_input = attention_h_previous\n",
    "\n",
    "        if has_arg(self.cell.call, 'training'):\n",
    "            output, new_cell_states = self.cell.call(cell_input, cell_states,\n",
    "                                                     training=training)\n",
    "        else:\n",
    "            output, new_cell_states = self.cell.call(cell_input, cell_states)\n",
    "\n",
    "        attention_h, new_attention_states = self.attention_call(\n",
    "            inputs=inputs,\n",
    "            cell_states=new_cell_states,\n",
    "            attended=attended,\n",
    "            attention_states=attention_states,\n",
    "            training=training)\n",
    "\n",
    "        return output, new_cell_states, new_attention_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _num_elements(x):\n",
    "        if hasattr(x, '__len__'):\n",
    "            return len(x)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    @property\n",
    "    def _num_wrapped_states(self):\n",
    "        return self._num_elements(self.cell.state_size)\n",
    "\n",
    "    @property\n",
    "    def _num_attention_states(self):\n",
    "        return self._num_elements(self.attention_state_size)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Builds attention mechanism and wrapped cell (if keras layer).\n",
    "\n",
    "        Arguments:\n",
    "            input_shape: list of tuples of integers, the input feature shape\n",
    "                (inputs sequence shape without time dimension) followed by\n",
    "                constants (i.e. attended) shapes.\n",
    "        \"\"\"\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('input shape should contain shape of both cell '\n",
    "                             'inputs and constants (attended)')\n",
    "\n",
    "        attended_shape = input_shape[1:]\n",
    "        input_shape = input_shape[0]\n",
    "        self.attended_spec = [InputSpec(shape=shape) for shape in attended_shape]\n",
    "        if isinstance(self.cell.state_size, int):\n",
    "            cell_state_size = [self.cell.state_size]\n",
    "        else:\n",
    "            cell_state_size = list(self.cell.state_size)\n",
    "        self.attention_build(\n",
    "            input_shape=input_shape,\n",
    "            cell_state_size=cell_state_size,\n",
    "            attended_shape=attended_shape,\n",
    "        )\n",
    "\n",
    "        if isinstance(self.cell, Layer):\n",
    "            cell_input_shape = (input_shape[0],\n",
    "                                self.attention_size +\n",
    "                                input_shape[-1] if self.concatenate_input\n",
    "                                else self._attention_size)\n",
    "            self.cell.build(cell_input_shape)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if hasattr(self.cell.state_size, '__len__'):\n",
    "            cell_output_dim = self.cell.state_size[0]\n",
    "        else:\n",
    "            cell_output_dim = self.cell.state_size\n",
    "\n",
    "        return input_shape[0], cell_output_dim\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return super(_RNNAttentionCell, self).trainable_weights + \\\n",
    "               self.cell.trainable_weights\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return super(_RNNAttentionCell, self).non_trainable_weights + \\\n",
    "               self.cell.non_trainable_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'attend_after': self.attend_after,\n",
    "                  'concatenate_input': self.concatenate_input}\n",
    "\n",
    "        cell_config = self.cell.get_config()\n",
    "        config['cell'] = {'class_name': self.cell.__class__.__name__,\n",
    "                          'config': cell_config}\n",
    "        base_config = super(_RNNAttentionCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class MixtureOfGaussian1DAttention(_RNNAttentionCell):\n",
    "    \"\"\"RNN attention mechanism for attending sequences.\n",
    "\n",
    "    The attention encoding (passed to the wrapped core RNN cell) is obtained by\n",
    "    letting the attention mechanism predict a Mixture of Gaussian distribution\n",
    "    (MoG) over the time dimension of the attended feature sequence. The\n",
    "    attention encoding is taken as the weighted sum of all features - where the\n",
    "    weight is given by the probability density function (evaluated in the\n",
    "    respective time step) according the predicted MoG distribution.\n",
    "\n",
    "    # Arguments\n",
    "        components: Positive integer, the number of mixture components to use\n",
    "            (for each head, see below).\n",
    "        heads: Positive integer (Default 1), the number of independent \"read\n",
    "            heads\" to use. Each head produces an independent (sub) attention\n",
    "            encoding, by predicting an independent MoG each. The (full)\n",
    "            attention encoding passed to the wrapped core RNN cell is the\n",
    "            concatenation of the attention encodings from each head. See \"Notes\n",
    "            on multiple heads vs multiple components\" below.\n",
    "        mu_activation: The activation function applied (after learnt linear\n",
    "            transformation) for mu:s (expectation value/location) of each\n",
    "            Gaussian component.\n",
    "        sigma_activation: The activation function applied (after learnt linear\n",
    "            transformation) for sigma:s (standard deviation) of each Gaussian\n",
    "            component. *NOTE* that this function should only return values > 0.\n",
    "        sigma_epsilon: Positive Float, this value is added to sigma to force it\n",
    "            to be at least this value.\n",
    "        predict_delta_mu: Boolean (Default True), whether or not to let the\n",
    "            attention mechanism to predict the _change_ in location (mu) of\n",
    "            each mixture component. This is recommended as it usually leads to\n",
    "            more stable convergence. By passing a `mu_activation` that always\n",
    "            returns a value > 0 and having `predict_delta_mu=True` it is\n",
    "            enforced that the attention mechanism \"parses\" the attended\n",
    "            sequence \"from start to end\" as the attention can not be moved\n",
    "            backwards.\n",
    "        For initializers, regularizers & constraints: See docs of Dense layer.\n",
    "\n",
    "    # Notes on multiple heads vs multiple components\n",
    "        A single head can \"attend to multiple parts of the sequence\" by\n",
    "        using multiple components. However, the features from the location of\n",
    "        the components are averaged together by a weighted sum (no\n",
    "        information is kept on their internal ordering for example). With\n",
    "        multiple heads, on the other side, the attention mechanism can \"pick\n",
    "        out\" features from multiple locations without averaging them, and\n",
    "        passing them \"intact\" to the core RNN cell. This is done at the cost of\n",
    "        a larger input vector to, and thereby more parameters of, the core RNN\n",
    "        cell.\n",
    "\n",
    "    # Example - Machine Translation with Attention and \"teacher forcing\"\n",
    "        # NOTE that this is a minimal naive example, this setup will not\n",
    "        # perform well for machine translation in general.\n",
    "        # TODO add `examples/machine_translation_with_attention.py`\n",
    "        # with performing setup\n",
    "\n",
    "        input_english = Input((None, tokens_english))\n",
    "        target_french_tm1 = Input((None, tokens_french))\n",
    "\n",
    "        cell = MixtureOfGaussian1DAttention(LSTMCell(64), components=3, heads=3)\n",
    "        attention_lstm = RNN(cell, return_sequences=True)\n",
    "        h_sequence = attention_lstm(target_french_tm1, constants=input_english)\n",
    "        output_layer = TimeDistributed(Dense(tokens_french, activation='softmax'))\n",
    "        predicted_french = output_layer(h_sequence)\n",
    "\n",
    "        train_model = Model(\n",
    "            inputs=[target_french_tm1, input_english],\n",
    "            outputs=predicted_french\n",
    "        )\n",
    "        model.compile(optimizer='Adam', loss='categorical_crossentropy')\n",
    "        model.fit(\n",
    "            x=[french_text[:, :-1], english_text],\n",
    "            y=french_text[:, 1:],\n",
    "            epochs=10\n",
    "        )\n",
    "    \"\"\"\n",
    "    def __init__(self, cell,\n",
    "                 components,\n",
    "                 heads=1,\n",
    "                 mu_activation=None,\n",
    "                 sigma_activation='exponential',\n",
    "                 sigma_epsilon=1e-3,\n",
    "                 predict_delta_mu=True,  # TODO alternative name `cumulative_mu`?\n",
    "                 kernel_initializer='glorot_uniform',  # FIXME most likely not optimal\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(MixtureOfGaussian1DAttention, self).__init__(cell, **kwargs)\n",
    "        self.components = components\n",
    "        self.heads = heads\n",
    "        self.mu_activation = activations.get(mu_activation)\n",
    "        self.sigma_activation = activations.get(sigma_activation)\n",
    "        self.sigma_epsilon = sigma_epsilon\n",
    "        self.predict_delta_mu = predict_delta_mu\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    @property\n",
    "    def attention_state_size(self):\n",
    "        \"\"\"Size of states dedicated for the attention mechanism.\n",
    "\n",
    "        If self.predict_delta_mu is True, mu (the \"location\") for all heads'\n",
    "        components needs to be forwarded to next time step and is therefore\n",
    "        added to the attention states.\n",
    "        \"\"\"\n",
    "        attention_state_size = [self.attention_size]\n",
    "        if self.predict_delta_mu:\n",
    "            mu_size = self.components * self.heads\n",
    "            attention_state_size.append(mu_size)\n",
    "\n",
    "        return attention_state_size\n",
    "\n",
    "    def attention_call(self,\n",
    "                       inputs,\n",
    "                       cell_states,\n",
    "                       attended,\n",
    "                       attention_states,\n",
    "                       training=None):\n",
    "        # only one attended sequence for now (verified in build)\n",
    "        [attended] = attended\n",
    "        mu_tm1 = attention_states[1] if self.predict_delta_mu else None\n",
    "\n",
    "        mog_input = concatenate([inputs, cell_states[0]])\n",
    "        params = K.bias_add(K.dot(mog_input, self.kernel), self.bias)\n",
    "\n",
    "        # dynamic creation of time index\n",
    "        # TODO check support by all backends\n",
    "        # TODO faster with non-dynamic if size of time dimension is fixed?\n",
    "        time_idx = K.arange(K.shape(attended)[1], dtype='float32')\n",
    "        time_idx = K.expand_dims(K.expand_dims(time_idx, 0), -1)\n",
    "\n",
    "        if self.heads == 1:\n",
    "            attention_h, mu = self._get_attention_h_and_mu(params, attended,\n",
    "                                                           mu_tm1, time_idx)\n",
    "        else:\n",
    "            c = self.components\n",
    "            attention_h_s, mu_s = zip(*[\n",
    "                self._get_attention_h_and_mu(\n",
    "                    params=params[..., c * i * 3:c * (i+1) * 3],\n",
    "                    attended=attended,\n",
    "                    mu_tm1=(mu_tm1[..., c * i:c * (i+1)]\n",
    "                            if self.predict_delta_mu else None),\n",
    "                    time_idx=time_idx\n",
    "                ) for i in range(self.heads)\n",
    "            ])\n",
    "            attention_h = concatenate(list(attention_h_s))\n",
    "            mu = concatenate(list(mu_s))\n",
    "\n",
    "        new_attention_states = [attention_h]\n",
    "        if self.predict_delta_mu:\n",
    "            new_attention_states.append(mu)\n",
    "\n",
    "        return attention_h, new_attention_states\n",
    "\n",
    "    def _get_attention_h_and_mu(self, params, attended, mu_tm1, time_idx):\n",
    "        \"\"\"Computes the attention encoding for \"one head\".\n",
    "\n",
    "        # Arguments\n",
    "            params: The MoG params (before activation) for one head.\n",
    "            attended: The attended sequence (tensor).\n",
    "            mu_tm1: mu from previous time step (tensor) if self.use_delta is\n",
    "                True otherwise None.\n",
    "            time_idx: Time index of the attended (tensor).\n",
    "\n",
    "        # Returns\n",
    "            attention_h: The attention encoding for the attention of one head.\n",
    "            mu: the location(s) of each mixture component for one head.\n",
    "        \"\"\"\n",
    "        def sigma_activation(x):\n",
    "            return self.sigma_activation(x) + self.sigma_epsilon\n",
    "\n",
    "        mixture_weights, mu, sigma = [\n",
    "            activation(params[..., i * self.components:(i + 1) * self.components])\n",
    "            for i, activation in enumerate(\n",
    "                [K.softmax, self.mu_activation, sigma_activation])]\n",
    "\n",
    "        if self.predict_delta_mu:\n",
    "            mu += mu_tm1\n",
    "\n",
    "        mixture_weights_, mu_, sigma_ = [\n",
    "            K.expand_dims(p, 1) for p in [mixture_weights, mu, sigma]]\n",
    "\n",
    "        attention_w = K.sum(\n",
    "            mixture_weights_ * K.exp(- sigma_ * K.square(mu_ - time_idx)),\n",
    "            # NOTE no normalisation was carried out in original paper by A. Graves\n",
    "            axis=-1,\n",
    "            keepdims=True\n",
    "        )\n",
    "        attention_h = K.sum(attention_w * attended, axis=1)\n",
    "\n",
    "        return attention_h, mu\n",
    "\n",
    "    def attention_build(self, input_shape, cell_state_size, attended_shape):\n",
    "        if not len(attended_shape) == 1:\n",
    "            raise ValueError('only a single attended supported')\n",
    "        attended_shape = attended_shape[0]\n",
    "        if not len(attended_shape) == 3:\n",
    "            raise ValueError('only support attending tensors with dim=3')\n",
    "\n",
    "        # NOTE _attention_size must always be set in `attention_build`\n",
    "        self._attention_size = attended_shape[-1] * self.heads\n",
    "        mog_in_dim = (input_shape[-1] + cell_state_size[0])\n",
    "        mog_out_dim = self.heads * self.components * 3\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(mog_in_dim, mog_out_dim),\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='kernel',\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint)\n",
    "        self.bias = self.add_weight(shape=(mog_out_dim,),\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    regularizer=self.bias_regularizer,\n",
    "                                    constraint=self.bias_constraint)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'components': self.components,\n",
    "            'heads': self.heads,\n",
    "            'mu_activation': activations.serialize(self.mu_activation),\n",
    "            'sigma_activation': activations.serialize(self.sigma_activation),\n",
    "            'sigma_epsilon': self.sigma_epsilon,\n",
    "            'predict_delta_mu': self.predict_delta_mu,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(MixtureOfGaussian1DAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRUAttentionCell(GRUCell):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        #self.state_size = self.units :for RNN, GRU\n",
    "        #self.state_size = (self.units, self.units) : for LSTM\n",
    "        self.attn_length = 79\n",
    "        self.attn_size = 79\n",
    "        self.attn_vec_size = self.attn_size\n",
    "        self.input_size = None\n",
    "        self.state_size = (self.units, self.units)\n",
    "\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_size(self):\n",
    "    size = (self._cell.state_size, self._attn_size,\n",
    "            self._attn_size * self._attn_length)\n",
    "    if self._state_is_tuple:\n",
    "      return size\n",
    "    else:\n",
    "      return sum(list(size))\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # build an all-zero tensor of shape (samples, output_dim)\n",
    "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
    "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
    "        if hasattr(self.cell.state_size, '__len__'):\n",
    "            state = [K.tile(initial_state, [1, dim])\n",
    "                     for dim in self.cell.state_size[:-1]]\n",
    "        else:\n",
    "            state = [K.tile(initial_state, [1, self.cell.state_size])]\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #self.timesteps = input_shape[0]\n",
    "        print('build cell:', input_shape)\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.context_kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 3),\n",
    "            name='context_kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_z = self.kernel[:,               : self.units * 1]\n",
    "        self.kernel_r = self.kernel[:, self.units * 1: self.units * 2]\n",
    "        self.kernel_h = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_c = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_z = self.recurrent_kernel[:,               : self.units * 1]\n",
    "        self.recurrent_kernel_r = self.recurrent_kernel[:, self.units * 1: self.units * 2]\n",
    "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        self.context_kernel_z = self.context_kernel[:,               : self.units * 1]\n",
    "        self.context_kernel_r = self.context_kernel[:, self.units * 1: self.units * 2]\n",
    "        self.context_kernel_h = self.context_kernel[:, self.units * 2:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_z = self.bias[              : self.units * 1]\n",
    "            self.bias_r = self.bias[self.units * 1: self.units * 2]\n",
    "            self.bias_h = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_c = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_z = None\n",
    "            self.bias_r = None\n",
    "            self.bias_h = None\n",
    "            self.bias_c = None\n",
    "        self.built = True\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameter Shapes\n",
    "        kernel : (input_dim, units)\n",
    "        recurrent_kernel : (units, units)\n",
    "        bias : (units, )\n",
    "\n",
    "        # Inherited Parameters\n",
    "        kernel : z, r, h\n",
    "        recurrent_kernel : z, r, h\n",
    "        bias : z, r, h\n",
    "\n",
    "        # New Parameters\n",
    "        input_dim\n",
    "        kernel_c\n",
    "        recurrent_kernel_c\n",
    "        bias_c\n",
    "        \"\"\"\n",
    "\n",
    "    def call(self, inputs, states, training=None,\n",
    "             constants=None):\n",
    "        print('cell.call Input Shape:', K.int_shape(inputs), 'State Shape:', K.int_shape(states[0]))\n",
    "        h_tm1 = states[0]  # previous memory\n",
    "        full_inputs = constants\n",
    "        timesteps = K.int_shape(full_inputs)[1]\n",
    "\n",
    "\n",
    "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "            self._dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n",
    "                self.dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "        if (0 < self.recurrent_dropout < 1 and\n",
    "                self._recurrent_dropout_mask is None):\n",
    "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
    "                _generate_dropout_ones(inputs, self.units),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = inputs * dp_mask[0]\n",
    "                inputs_r = inputs * dp_mask[1]\n",
    "                inputs_h = inputs * dp_mask[2]\n",
    "                inputs_c = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_z = inputs\n",
    "                inputs_r = inputs\n",
    "                inputs_h = inputs\n",
    "                inputs_c = inputs\n",
    "            x_z = K.dot(inputs_z, self.kernel_z)\n",
    "            x_r = K.dot(inputs_r, self.kernel_r)\n",
    "            x_h = K.dot(inputs_h, self.kernel_h)\n",
    "            x_c = K.dot(inputs_c, self.kernel_c)\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.bias_z)\n",
    "                x_r = K.bias_add(x_r, self.bias_r)\n",
    "                x_h = K.bias_add(x_h, self.bias_h)\n",
    "                x_c = K.bias_add(x_c, self.bias_c)\n",
    "\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "\n",
    "            # calculate the context vector\n",
    "            #context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "\n",
    "            # Attention Context Part\n",
    "            #h_tm1_c = K.repeat(h_tm1_c, timesteps)\n",
    "            h_tm1_c = full_inputs\n",
    "            e = self.activation(x_c + K.dot(h_tm1_c, self.recurrent_kernel_c))\n",
    "            a = K.softmax(e)\n",
    "            print('A:', K.int_shape(a), 'inputs_c:', K.int_shape(inputs_c), 'full input:', K.int_shape(full_inputs))\n",
    "            c_t = K.dot(a, inputs_c)\n",
    "\n",
    "            # GRU Part\n",
    "            z = self.recurrent_activation(x_z +\n",
    "                                          K.dot(h_tm1_z,\n",
    "                                                self.recurrent_kernel_z) +\n",
    "                                          K.dot(c_t, self.context_kernel_z))\n",
    "            r = self.recurrent_activation(x_r +\n",
    "                                          K.dot(h_tm1_r,\n",
    "                                                self.recurrent_kernel_r) +\n",
    "                                          K.dot(c_t, self.context_kernel_r))\n",
    "\n",
    "            hh = self.activation(x_h +\n",
    "                                 K.dot(r * h_tm1_h,\n",
    "                                       self.recurrent_kernel_h) +\n",
    "                                 K.dot(c_t, self.context_kernel_h))\n",
    "\n",
    "        else:\n",
    "            \"\"\"\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "            matrix_x = K.dot(inputs, self.kernel)\n",
    "            if self.use_bias:\n",
    "                matrix_x = K.bias_add(matrix_x, self.bias)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            matrix_inner = K.dot(h_tm1,\n",
    "                                 self.recurrent_kernel[:, :2 * self.units])\n",
    "\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            recurrent_z = matrix_inner[:, :self.units]\n",
    "            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            recurrent_h = K.dot(r * h_tm1,\n",
    "                                self.recurrent_kernel[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        return h, [h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        input_english = Input((None, tokens_english))\n",
    "        target_french_tm1 = Input((None, tokens_french))\n",
    "\n",
    "        cell = MixtureOfGaussian1DAttention(LSTMCell(64), components=3, heads=3)\n",
    "        h_sequence = RNN(cell, return_sequences=True)(target_french_tm1, constants=input_english)\n",
    "        output_layer = TimeDistributed(Dense(tokens_french, activation='softmax'))\n",
    "        predicted_french = output_layer(h_sequence)\n",
    "\n",
    "        train_model = Model(\n",
    "            inputs=[target_french_tm1, input_english],\n",
    "            outputs=predicted_french\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "79 80\n",
      "cell.build Shape: (None, 80)\n",
      "cell.call Input Shape: (None, 80) Full State len: 2 State Shape: (None, 80) Full h Shape: (None, 6320)\n",
      "h: (None, 6320) s0: (None, 6320, 80)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 6320 and 80 for 'rnn_27/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 6320 and 80 for 'rnn_27/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-0e340b8850aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mCel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRUAttentionCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    587\u001b[0m                                              \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                                              \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                                              input_length=timesteps)\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m         output_ta = tensor_array_ops.TensorArray(\n\u001b[1;32m   2563\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         last_output, outputs, states = K.rnn(step,\n",
      "\u001b[0;32m<ipython-input-435-f19f6a0e6532>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m#h_tm1_c = full_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's0:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_c\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_kernel_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inputs_c:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full input:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1891\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2436\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2438\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 6320 and 80 for 'rnn_27/MatMul_4' (op: 'MatMul') with input shapes: [?,6320], [80,80]."
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "print(timestepX, ndimX)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = LSTM(ndimX, return_sequences=True)(input_)\n",
    "Cel = GRUAttentionCell(ndimX)\n",
    "dec = RNN(Cel, return_sequences=True)(enc)\n",
    "act = Dense(ndimY, activation='softmax')(dec)\n",
    "model = Model(inputs=input_, outputs=act)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATENT_DIM: 64\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_172 (InputLayer)       (None, 79, 80)            0         \n",
      "_________________________________________________________________\n",
      "lstm_167 (LSTM)              (None, 79, 80)            51520     \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 79, 1)             81        \n",
      "=================================================================\n",
      "Total params: 51,601\n",
      "Trainable params: 51,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "print('LATENT_DIM: %s' % LATENT_DIM)\n",
    "\n",
    "input_ = Input(shape=(timestepX, ndimX), dtype='float32')\n",
    "enc = LSTM(ndimX, return_sequences=True)(input_)\n",
    "tim = TimeDistributed(Dense(1))(enc)\n",
    "#dec = GRUAttention(4, return_sequences=True)(enc)\n",
    "act = Dense(ndimY, activation='softmax')(tim)\n",
    "model = Model(inputs=input_, outputs=tim)\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=GPU_NUM)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent  # _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyGRUAttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 output_activation='sigmoid',\n",
    "                 return_probabilities=False,\n",
    "                 name='MyGRUAttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='ones',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super().reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, h\n",
    "\n",
    "        \n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for creating the context vector\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # Matrices for Gates\n",
    "        # st = h_tilda\n",
    "        num = len(['reset_gate', 'update_gate', 'h_tilda(proposal)'])\n",
    "\n",
    "        self.W = self.add_weight(shape=(num, self.output_dim, self.units),\n",
    "                                   name='W',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U = self.add_weight(shape=(num, self.units, self.units),\n",
    "                                   name='U',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.V = self.add_weight(shape=(num, self.input_dim, self.units),\n",
    "                                   name='V',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b = self.add_weight(shape=(num, self.units, ),\n",
    "                                   name='b',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "\n",
    "        # Matrices for making the final prediction vector\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "\n",
    "        return super().call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        h0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, h0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        yt_before, ht_before = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        repeated_ht_before = K.repeat(ht_before, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        weighted_ht_before = K.dot(repeated_ht_before, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(weighted_ht_before + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.softmax(et)  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        \n",
    "        # At timestep `t`:\n",
    "        \n",
    "        # first calculate the \"r\"; reset gate\n",
    "        # r = sigmoid(xt * Ur + ht-1 * Wr + br) \n",
    "        # New r = sigmoid(xt * Ur + ht-1 * Wr + br + context * Vr)\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[0])\n",
    "            + K.dot(ht_before, self.U[0])\n",
    "            + K.dot(context, self.V[0])\n",
    "            + self.b[0])\n",
    "\n",
    "        # now calculate the \"z\"; update gate\n",
    "        # z = sigmoid(xt * Uz + ht-1 * Wz + bz)\n",
    "        # New z = sigmoid(xt * Uz + ht-1 * Wz + bz + context * Vz)\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(yt_before, self.W[1])\n",
    "            + K.dot(ht_before, self.U[1])\n",
    "            + K.dot(context, self.V[1])\n",
    "            + self.b[1])\n",
    "\n",
    "        # calculate the proposal \"g\"; hidden state for now(tilda)\n",
    "        # h_tilda = tanh(xt * Wh + (ht-1 * rt) * Uh + bh)\n",
    "        # New h_tilda = tanh(xt * Wh + (ht-1 * rt) * Uh + bh + context * Vh)\n",
    "        h_tilda = activations.tanh(\n",
    "            K.dot(yt_before, self.W[2])\n",
    "            + K.dot((rt * ht_before), self.U[2])\n",
    "            + K.dot(context, self.V[2])\n",
    "            + self.b[2])\n",
    "\n",
    "        # new hidden state 'ht' from 'h_tilda'\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        # ht = (1-zt) * h_tilda + zt * ht-1\n",
    "        ht = (1 - zt) * h_tilda + zt * ht_before\n",
    "\n",
    "        \n",
    "        # Output Activation\n",
    "        y_ = (K.dot(yt_before, self.W_o)\n",
    "              + K.dot(ht_before, self.U_o)\n",
    "              + K.dot(context, self.C_o)\n",
    "              + self.b_o)\n",
    "\n",
    "        if self.output_activation == 'softmax':\n",
    "            yt = activations.softmax(y_)\n",
    "            \n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            yt = activations.sigmoid(y_)\n",
    "\n",
    "        elif self.output_activation == 'tanh':\n",
    "            yt = activations.tanh(y_)\n",
    "\n",
    "            \n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, ht]\n",
    "        else:\n",
    "            return yt, [yt, ht]\n",
    "\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_NUM: 100, BATCH_SIZE 256\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_10 to have shape (None, 79, 80) but got array with shape (80000, 79, 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-d5e98611bd84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# 1: progress bar, 2: one line per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    \u001b[0;31m#validation_data=(testX, testY),  # Validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m#callbacks=[history],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   )\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1419\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1420\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_10 to have shape (None, 79, 80) but got array with shape (80000, 79, 110)"
     ]
    }
   ],
   "source": [
    "print('EPOCH_NUM: %s, BATCH_SIZE %s' % (EPOCH_NUM, BATCH_SIZE))\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=10,     # How many times to run back_propagation\n",
    "                   batch_size=2,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=2,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=True,\n",
    "                   #callbacks=[history],\n",
    "                  )\n",
    "\n",
    "# Save model\n",
    "model.save('gru_attention_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = fitted.model.evaluate(train_X, train_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (train_accuracy * 100))\n",
    "\n",
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = output_decoder(train_Y)\n",
    "train_Y_hat = output_decoder(train_Y_hat_array)\n",
    "train_X_real = input_decoder(train_X)\n",
    "\n",
    "print(train_X_real[:3])\n",
    "print(train_Y_real[:3])\n",
    "print(train_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = fitted.model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Accuracy: %.3f' % (test_accuracy * 100))\n",
    "\n",
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = output_decoder(test_Y)\n",
    "test_Y_hat = output_decoder(test_Y_hat_array)\n",
    "test_X_real = input_decoder(test_X)\n",
    "\n",
    "print(test_X_real[:3])\n",
    "print(test_Y_real[:3])\n",
    "print(test_Y_hat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "gist": {
   "data": {
    "description": "tensorflow/konlpy.ipynb",
    "public": false
   },
   "id": "5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "kernelspec": {
   "display_name": "Tensorflow: Python3.6 (conda env)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "906px",
    "left": "0px",
    "right": "865.4px",
    "top": "135px",
    "width": "157px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
