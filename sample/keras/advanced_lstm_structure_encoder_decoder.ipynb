{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#LSTM-Lecture\" data-toc-modified-id=\"LSTM-Lecture-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>LSTM Lecture</a></div><div class=\"lev1 toc-item\"><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Reshaping-the-Data-2-(X-:-Multiple,-Y-:-Multiple)\" data-toc-modified-id=\"Reshaping-the-Data-2-(X-:-Multiple,-Y-:-Multiple)-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reshaping the Data 2 (X : Multiple, Y : Multiple)</a></div><div class=\"lev3 toc-item\"><a href=\"#Dimension\" data-toc-modified-id=\"Dimension-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Dimension</a></div><div class=\"lev3 toc-item\"><a href=\"#Assignment\" data-toc-modified-id=\"Assignment-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Assignment</a></div><div class=\"lev3 toc-item\"><a href=\"#Reshaping-:-1D-to-2D-(for-MinMaxScaler)\" data-toc-modified-id=\"Reshaping-:-1D-to-2D-(for-MinMaxScaler)-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Reshaping : 1D to 2D (for <code>MinMaxScaler</code>)</a></div><div class=\"lev3 toc-item\"><a href=\"#Scaling-:-MinMax,-0-~-1\" data-toc-modified-id=\"Scaling-:-MinMax,-0-~-1-214\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Scaling : <code>MinMax</code>, 0 ~ 1</a></div><div class=\"lev3 toc-item\"><a href=\"#Reshaping-X:-2D-to-3D,-(Samples,-Timestep-Sequence,-Features)\" data-toc-modified-id=\"Reshaping-X:-2D-to-3D,-(Samples,-Timestep-Sequence,-Features)-215\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Reshaping <code>X</code>: 2D to 3D, (Samples, Timestep-Sequence, Features)</a></div><div class=\"lev3 toc-item\"><a href=\"#Padding\" data-toc-modified-id=\"Padding-216\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Padding</a></div><div class=\"lev3 toc-item\"><a href=\"#Start-&amp;-End-Marking\" data-toc-modified-id=\"Start-&amp;-End-Marking-217\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span>Start &amp; End Marking</a></div><div class=\"lev3 toc-item\"><a href=\"#Splitting-(Train-&amp;-Test)\" data-toc-modified-id=\"Splitting-(Train-&amp;-Test)-218\"><span class=\"toc-item-num\">2.1.8&nbsp;&nbsp;</span>Splitting (Train &amp; Test)</a></div><div class=\"lev1 toc-item\"><a href=\"#Encoder-Decoder-1-(RepeatVector)\" data-toc-modified-id=\"Encoder-Decoder-1-(RepeatVector)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Encoder-Decoder 1 (RepeatVector)</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-303\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-304\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev1 toc-item\"><a href=\"#Encoder-Decoder-2-(Seq2Seq)---It-can-be-used-for-Text-Summarization\" data-toc-modified-id=\"Encoder-Decoder-2-(Seq2Seq)---It-can-be-used-for-Text-Summarization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Encoder-Decoder 2 (Seq2Seq) - It can be used for Text Summarization</a></div><div class=\"lev3 toc-item\"><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-401\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Modeling</a></div><div class=\"lev3 toc-item\"><a href=\"#Training\" data-toc-modified-id=\"Training-402\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Training</a></div><div class=\"lev3 toc-item\"><a href=\"#Scoring\" data-toc-modified-id=\"Scoring-403\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>Scoring</a></div><div class=\"lev3 toc-item\"><a href=\"#Testing\" data-toc-modified-id=\"Testing-404\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>Testing</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model\" data-toc-modified-id=\"Inference-Model-4041\"><span class=\"toc-item-num\">4.0.4.1&nbsp;&nbsp;</span>Inference Model</a></div><div class=\"lev4 toc-item\"><a href=\"#Inference-Model-within-a-Function\" data-toc-modified-id=\"Inference-Model-within-a-Function-4042\"><span class=\"toc-item-num\">4.0.4.2&nbsp;&nbsp;</span>Inference Model within a Function</a></div><div class=\"lev1 toc-item\"><a href=\"#Stateful\" data-toc-modified-id=\"Stateful-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stateful</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Recurrent Neural Networks, we are quickly confronted to the so-called __gradient vanishing problem__:\n",
    "\n",
    "In machine learning, __the vanishing gradient problem__ is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation.  \n",
    "In such methods, each of the neural network’s weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training.   \n",
    "_Traditional activation functions such as the hyperbolic tangent function have gradients in the range `(−1,1)` or `(0,1)`_, and backpropagation computes gradients by the chain rule.  \n",
    "This has the effect of multiplying n of these small numbers to compute gradients of the “front” layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.\n",
    "\n",
    "One solution is __to consider *adding the updates* instead of multiplying them__, and this is exactly what the LSTM does. The state of every cell is updated in an additive way (Equation 9) such that the gradient hardly vanishes.\n",
    "\n",
    "* Input  Gate  \n",
    "* Forget Gate  \n",
    "* Cell   State\n",
    "* Output Gate \n",
    "* Hidden State \n",
    "\n",
    "![lstm](keras_stateful_lstm_2.png)\n",
    "![lstm](lstm_basic.png)\n",
    "![lstm](lstm_module.jpg)\n",
    "![](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm'](lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime as dt\n",
    "import itertools as it\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten\n",
    "from keras.callbacks import Callback, LambdaCallback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (TimeDistributed, Embedding, RepeatVector,\n",
    "                          Permute, Lambda, Bidirectional)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Reshape, dot, multiply, concatenate, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.weights = []\n",
    "        self.states = []\n",
    "\n",
    "#    def on_batch_begin(self, batch, logs={}):\n",
    "#        self.weights.append([{'begin_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.weights.append([{'end_' + layer.name: layer.get_weights()} for layer in model.layers])\n",
    "        \n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[0].get_weights()))\n",
    "#print_outputs = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].output))\n",
    "#print_states = LambdaCallback(on_epoch_end=lambda batch, logs: pprint(model.layers[2].states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>22.5</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>25.5</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>31.5</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>34.5</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>43.5</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>106.5</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.87</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>109.5</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.08</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>112.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.36</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>115.5</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.47</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>118.5</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.61</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>121.5</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>124.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>126.0</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>127.5</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>130.5</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>133.5</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>136.5</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>139.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.07</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>144.0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>145.5</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>147.0</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.34</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>148.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    col1   col2  col3  col4  col5\n",
       "0      0    0.0  1.00  1.00   0.0\n",
       "1      1    1.5  0.54 -0.42   0.2\n",
       "2      2    3.0 -0.42 -0.65   0.4\n",
       "3      3    4.5 -0.99  0.96   0.6\n",
       "4      4    6.0 -0.65 -0.15   0.8\n",
       "5      5    7.5  0.28 -0.84   1.0\n",
       "6      6    9.0  0.96  0.84   1.2\n",
       "7      7   10.5  0.75  0.14   1.4\n",
       "8      8   12.0 -0.15 -0.96   1.6\n",
       "9      9   13.5 -0.91  0.66   1.8\n",
       "10    10   15.0 -0.84  0.41   2.0\n",
       "11    11   16.5  0.00 -1.00   2.2\n",
       "12    12   18.0  0.84  0.42   2.4\n",
       "13    13   19.5  0.91  0.65   2.6\n",
       "14    14   21.0  0.14 -0.96   2.8\n",
       "15    15   22.5 -0.76  0.15   3.0\n",
       "16    16   24.0 -0.96  0.83   3.2\n",
       "17    17   25.5 -0.28 -0.85   3.4\n",
       "18    18   27.0  0.66 -0.13   3.6\n",
       "19    19   28.5  0.99  0.96   3.8\n",
       "20    20   30.0  0.41 -0.67   4.0\n",
       "21    21   31.5 -0.55 -0.40   4.2\n",
       "22    22   33.0 -1.00  1.00   4.4\n",
       "23    23   34.5 -0.53 -0.43   4.6\n",
       "24    24   36.0  0.42 -0.64   4.8\n",
       "25    25   37.5  0.99  0.96   5.0\n",
       "26    26   39.0  0.65 -0.16   5.2\n",
       "27    27   40.5 -0.29 -0.83   5.4\n",
       "28    28   42.0 -0.96  0.85   5.6\n",
       "29    29   43.5 -0.75  0.12   5.8\n",
       "..   ...    ...   ...   ...   ...\n",
       "70    70  105.0  0.63 -0.20  14.0\n",
       "71    71  106.5 -0.31 -0.81  14.2\n",
       "72    72  108.0 -0.97  0.87  14.4\n",
       "73    73  109.5 -0.74  0.08  14.6\n",
       "74    74  111.0  0.17 -0.94  14.8\n",
       "75    75  112.5  0.92  0.70  15.0\n",
       "76    76  114.0  0.82  0.36  15.2\n",
       "77    77  115.5 -0.03 -1.00  15.4\n",
       "78    78  117.0 -0.86  0.47  15.6\n",
       "79    79  118.5 -0.90  0.61  15.8\n",
       "80    80  120.0 -0.11 -0.98  16.0\n",
       "81    81  121.5  0.78  0.21  16.2\n",
       "82    82  123.0  0.95  0.80  16.4\n",
       "83    83  124.5  0.25 -0.88  16.6\n",
       "84    84  126.0 -0.68 -0.08  16.8\n",
       "85    85  127.5 -0.98  0.94  17.0\n",
       "86    86  129.0 -0.38 -0.71  17.2\n",
       "87    87  130.5  0.57 -0.35  17.4\n",
       "88    88  132.0  1.00  1.00  17.6\n",
       "89    89  133.5  0.51 -0.48  17.8\n",
       "90    90  135.0 -0.45 -0.60  18.0\n",
       "91    91  136.5 -0.99  0.98  18.2\n",
       "92    92  138.0 -0.63 -0.22  18.4\n",
       "93    93  139.5  0.32 -0.80  18.6\n",
       "94    94  141.0  0.97  0.88  18.8\n",
       "95    95  142.5  0.73  0.07  19.0\n",
       "96    96  144.0 -0.18 -0.93  19.2\n",
       "97    97  145.5 -0.93  0.71  19.4\n",
       "98    98  147.0 -0.82  0.34  19.6\n",
       "99    99  148.5  0.04 -1.00  19.8\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = 100\n",
    "data = pd.DataFrame({'col1': np.arange(data_len),\n",
    "                     'col2': np.arange(data_len) * 1.5,\n",
    "                     'col3': np.round(np.cos(np.arange(data_len)), 2),\n",
    "                     'col4': np.round(np.cos(np.arange(data_len) * 2), 2),\n",
    "                     'col5': np.arange(0, data_len*.2, .2)})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fefe4a5b860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd81eXZ+PHPnZyTvcgkZBD2CjtBRGTvIUURUGvVttL2\nsfZ5bBVXVZxVa9unP7VaW61i+5iAoqCIbEQUTMLeysxJCGTvcdb9++MbMEBYWeckud6vF68k3/P9\nnnOdGK/cub/3dV9Ka40QQoi2y8PVAQghhGhekuiFEKKNk0QvhBBtnCR6IYRo4yTRCyFEGyeJXggh\n2jhJ9EII0cZJohdCiDZOEr0QQrRxJlcHABAeHq4TEhJcHYYQQrQq27dvz9daR1zpPLdI9AkJCWRk\nZLg6DCGEaFWUUiev5jyZuhFCiDZOEr0QQrRxkuiFEKKNc4s5+vrYbDaysrKorq52dSjNwsfHh9jY\nWMxms6tDEUK0cW6b6LOysggMDCQhIQGllKvDaVJaawoKCsjKyqJLly6uDkcI0ca57dRNdXU1YWFh\nbS7JAyilCAsLa7N/rQgh3IvbJnqgTSb5s9ryexNCuBe3nboRQghxCVrDgU+u+nS3HtG3JosWLeKV\nV14BYOnSpfTr1w8PDw8pBBNCNK0z++G9mbD07qu+RBJ9M0hMTGTZsmWMGjXK1aEIIdqKykJY+SC8\nORLO7IPpf77qSyXRX8HixYsZMGAAAwcO5M477+TEiROMGzeOAQMGMH78eDIzMy+6pk+fPvTq1csF\n0Qoh2hynA9LfhleHQsbbkPQzuH8HJP/sqp+iVczRP/3pfg6cKm3S5+zbKYinZva77Dn79+/nueee\n45tvviE8PJzCwkLuuuuuc//eeecdfvOb3/DJJ1c/VyaEEFft5FZY9RCc3gudR8LUl6Bj4jU/jYzo\nL2PDhg3ceuuthIeHAxAaGsrWrVu5/fbbAbjzzjvZsmWLK0MUQrRFpafgo5/Dv6YYUzZz3oG7P2tQ\nkodWMqK/0shbCCHaBHsNbH0dNr8CTjuMWggj/we8/Bv1tDKiv4xx48axdOlSCgoKACgsLGTEiBGk\npKQA8J///Icbb7zRlSEKIdoCreHwF/D6dbD+aeg2Fn6dBuMeb3SSh1YyoneVfv368fjjjzN69Gg8\nPT0ZPHgwr776Kvfccw9//OMfiYiI4F//+tdF13388cfcf//95OXlMX36dAYNGsTq1atd8A6EEG4v\n/wh88QgcWQvhPeHHy6D7+CZ9CaW1vvwJSr0DzABytdaJFzz2O+AVIEJrna+Mcs+/AtOASuBurfWO\nKwWRlJSkL1xvfvDgQfr06XMt76XVaQ/vUQhxCdWlsPmPsO0NMPvCmEdg2ALwvPqNDpVS27XWSVc6\n72pG9O8CrwGLL3iBOGASUHd94VSgR+2/64A3aj8KIYQAcDphTyqsewrKz8DgH8P4pyAgstle8oqJ\nXmu9WSmVUM9DfwEWAsvrHJsFLNbGnwnblFIhSqlorXVOUwQrhBCtWvYOWLUQstIhZijM/wBihzb7\nyzZojl4pNQvI1lrvvmBzrhjAUufrrNpjFyV6pdQCYAFAfHx8Q8IQQojWoTzPuMm689/gHwGz/gYD\nbwOPllkPc82JXinlBzyGMW3TYFrrt4C3wJijb8xzCSGEW3LYIP2fsPEPYKuA6++D0QvBJ7hFw2jI\niL4b0AU4O5qPBXYopYYB2UBcnXNja48JIUT7cuxLWPUw5B2EbuNgyksQ0dMloVxzotda7wXO3TVQ\nSp0AkmpX3awAfq2USsG4CVsi8/NCiHal6CSs+T0cXAEhnY15+F5TwYU9KK44QaSU+gDYCvRSSmUp\npS63k87nwDHgCPAP4L+aJMpWoO42xQ899BC9e/dmwIABzJ49m+LiYhdHJ4RodtZKY4rm9WFwZB2M\n+z3clwa9p7k0ycNVJHqt9W1a62ittVlrHau1fvuCxxO01vm1n2ut9X1a625a6/5a63a5GfvEiRPZ\nt28fe/bsoWfPnvzhD39wdUhCiOaiNez/xEjwX74IvabBr9Nh1ENg9nF1dIBsgXBFDdmmeNKkSZhM\nxqzY8OHDycrKaumwhRAt4VwTkLuMG6x3fw63/guCY10d2XlaxxYIqx4xtulsSh37w9QXL3tKU2xT\n/M477zBv3rymjV0I4VpVRcY0Tfo/wTsQpr0CQ+8BT/dMqe4ZlZu41DbFy5YtA4xtihcuXHjJ659/\n/nlMJhN33HFHi8QrhGhmTgfsfB/WP2Mk+6SfwtjHwS/U1ZFdVutI9FcYebujd999l88++4z169ej\nXHwjRgjRBDK/NZqA5OyG+BFGE5DoAa6O6qrIHP1lNHSb4i+++IKXX36ZFStW4Ofn16IxCyGaWGkO\nLFsA70wyKlxveRvu+bzVJHloLSN6F2noNsW//vWvqampYeLEiYBxQ/bNN99s6fCFEI1hrzF2ltz8\nR3BY4cbfGf+aYH/4lnbFbYpbgmxTLIRwK9+tMfaILzwKvabD5OcgtKuro7pIU25TLIQQ7UPBUfji\nUfh+NYT1gDs+gh4TXB1Vo0miF0KImjKjT+vW18HkA5Oeg2G/AJOXqyNrEpLohRDtl9awZwmsfRLK\nT8PA22HCIgiMcnVkTUoSvRCifTq1y2gCYvkWOg2B+f+B2CtOd7dKkuiFEO1LRT5seBa2vwf+4XDT\nazDojhZrAuIKkuiFEO2Dww4Z78DG58BaAcP/C8Y83OJNQFxBEn0TWbRoEQEBATz44IM88cQTLF++\nHA8PDyIjI3n33Xfp1KmTq0MUov06vtloApJ7ALqOMZqARPZ2dVQtpu3+reJCDz30EHv27GHXrl3M\nmDGDZ555xtUhCdE+FWfCkruMHSat5TDvP3DnJ+0qyYMk+itqyDbFQUFB5z6vqKiQvW6EaGm2Ktj0\nErw2DL5bDWMeM5qA9Jnh8iYgrtAqpm5eSnuJQ4WHmvQ5e4f25uFhD1/2nMZsU/z444+zePFigoOD\n2bhxY5PGLoS4BK2NFn6rfw8lmdBvNkx8FkLirnxtG3Y1rQTfUUrlKqX21Tn2R6XUIaXUHqXUx0qp\nkDqPPaqUOqKUOqyUmtxcgbeES21TfPvttwPGNsVbtmyp99rnn38ei8XCHXfcwWuvvdZiMQvRbuUe\nhMWzYMlPjD3i714Jt77b7pM8XN2I/l3gNWBxnWNrgUe11nal1EvAo8DDSqm+wHygH9AJWKeU6qm1\ndjQmyCuNvN3ZHXfcwbRp03j66addHYoQbVNVMWx6EdLeahVNQFzhanrGbgYKLzi2Rmttr/1yG3C2\nb9YsIEVrXaO1Po7RJHxYE8bbohq6TfH3339/7vPly5fTu3f7uvEjRItwOmHHYnh1KHz7Jgz5Cdy/\nA4bdK0n+Ak3x3fgpkFr7eQxG4j8rq/ZYq9TQbYofeeQRDh8+jIeHB507d5YtioVoapY0o6r11E6I\nGw53LoPoga6Oym01KtErpR4H7MB/GnDtAmABQHx8fGPCaFZnb7zWtWHDhovOW7Ro0bnPP/roo+YO\nS4j2qew0rFsEuz+AwGi4+R/Q/9Z2uZLmWjQ40Sul7gZmAOP1D5vaZwN173zE1h67iNb6LeAtMPaj\nb2gcQoh2wG6Fb9+AL182moCM/K3RBMQ7wNWRtQoNSvRKqSnAQmC01rqyzkMrgP9TSv0Z42ZsDyCt\n0VEKIdqv79caTUAKjkDPKTD5BQjr5uqoWpUrJnql1AfAGCBcKZUFPIWxysYbWFtbDLRNa/1LrfV+\npdQS4ADGlM59jV1xI4RopwqOwurH4LsvIKw73PEh9Jjo6qhapSsmeq31bfUcfvsy5z8PPN+YoIQQ\n7VhNOXxV2wTE0wsmPgPX/arNNAFxBVmDJIRwD1rD3g9h7RNQlgMDb6ttAtLR1ZG1epLohRCul7PH\nWC6ZudVYJnnrexB/naujajNkU7MmsmjRIl555ZXzjv3pT39CKUV+fr6LohLCzVUUwGcPwFujIf87\nmPn/4N6NkuSbmIzom4nFYmHNmjVuXSMghMs47LD9X7DhOaMx97AFMOZR8A258rXimsmI/goask0x\nwAMPPMDLL78sWxQLcaHjX8HfR8HnD0L0APjV1zD1JUnyzahVjOhPv/ACNQebdpti7z696fjYY5c9\np6HbFC9fvpyYmBgGDpSSbCHOKbYYN1r3fwzB8TB3MfS5SapaW0CrSPSucqltipctWwYY2xQvXLjw\nvGsqKyt54YUXWLNmTYvHK4RbslXDN6/CV38CtNEE5IbfgNnX1ZG1G60i0V9p5O1Ojh49yvHjx8+N\n5rOyshgyZAhpaWl07CjLxEQ7ojUcWmkUPRWfhL6zYNJzECL3rVqazNFfRkO2Ke7fvz+5ubmcOHGC\nEydOEBsby44dOyTJi/Yl7zC8PxtS7wCzH/xkhTFVI0neJVrFiN5VGrpNsRDtVnWJ0as17e9g9ocp\nL0Hyz2V/eBdTP2w86TpJSUk6IyPjvGMHDx6kT58+LoqoZbSH9yjaCacTdv0H1j8NFflGE5DxT4J/\nuKsja9OUUtu11klXOk9+zQohGicrAz5/CE7tgNhhcMdS6DTY1VGJOiTRCyEapuyMMYLf9R8I6ChN\nQNyYJHohxLWxW405+E0vgb0abvgfGPWg0ZhbuCVJ9EKIq3dkHax6BAq+hx6TjSYg4d1dHZW4Akn0\nQogrKzwOqx+HwyshtCvcvgR6TnZ1VOIqSaIXQlyatQK++rNR2ephgvFPwfX3gcnb1ZGJayAFU02k\n7jbFixYtIiYmhkGDBjFo0CA+//xzF0cnxDXSGvZ9BK8lG92e+s6C+7fDjb+VJN8KXU3P2HeAGUCu\n1jqx9lgokAokACeAuVrrImVs1fhXYBpQCdyttd7RPKG7twceeIAHH3zQ1WEIce1O74VVD8PJr6Hj\nAJjzDsQPd3VUohGuZkT/LjDlgmOPAOu11j2A9bVfA0wFetT+WwC80TRhuk5DtykWotWpLISVvzO2\nEM47BDP/Cgs2SZJvA66mOfhmpVTCBYdnAWNqP38P2AQ8XHt8sTbKbbcppUKUUtFa65zGBPnVku/I\nt5Q35ikuEh4XwI1ze172nIZuUwzw2muvsXjxYpKSkvjTn/5Ehw4dmjR+IZqM0/FDE5DqUki+F8Y+\nCr7yM9tWNHSOPqpO8j4NRNV+HgNY6pyXVXvsIkqpBUqpDKVURl5eXgPDaF6X2qb49ttvB4xtirds\n2XLRdb/61a84evQou3btIjo6mt/97nctGrcQV+3E1/D30cZIPioRfvkVTHtZknwrYHM4r/rcRq+6\n0VprpdQ1b5ijtX4LeAuMvW4ud+6VRt7uJioq6tzn9957LzNmzHBhNELUoyTbaAKy7yMIjoNb34W+\nP5Kq1lbgeH4FqekWPtyeddXXNHREf0YpFQ1Q+zG39ng2EFfnvNjaY61SQ7YpBsjJ+WGm6uOPPyYx\nMbFlAhbiSmzVsPkVeC3J2Ct+9MNwXxr0my1J3o1V2xx8sjOb+W9tZewrm/jHV8cYFBd81dc3dES/\nArgLeLH24/I6x3+tlEoBrgNKGjs/70oN3aZ44cKF7Nq1C6UUCQkJ/P3vf3dB9ELUoTUcXgWrH4Wi\nE9BnJkx6Hjp0dnVk4jIO5pSSmm5h2Y4sSqvtxIf68dDkXswZGktUkA9v3311z3PFbYqVUh9g3HgN\nB84ATwGfAEuAeOAkxvLKwtrlla9hrNKpBO7RWmfU97x1yTbFQjSjvO/gi0fg6HoI72U04u421tVR\niUsoq7bx6e4cUtMz2Z1VgpfJg6mJHZmXHMfwLmF4ePzwl1eTbVOstb7tEg+Nr+dcDdx3pecUQrSA\n6lL48iX49s3aJiAv1jYBMbs6MnEBrTU7MotISbPw2Z4cqmwOekUF8tTMvsweHEOIn1ejnl+2QBCi\nrXE6YfcHsG4RVOTB4B8bWxcERLg6MnGBgvIaPt6ZTUq6hSO55fh7efKjwZ2YmxTHoLgQVBPdN3Hr\nRK+1brI36m7cobOXaIOyt8PnCyE7A2KT4fZUiBni6qhEHU6nZsuRfFLTLaw5cBqbQzM4PoSXbxnA\n9AHR+Hs3fVp220Tv4+NDQUEBYWFhbS7Za60pKCjAx8fH1aGItqI812gCsvPfEBAFP3oTBswDD9nO\nyl3klFSxNCOL1HQL2cVVdPAz85PrE5iXHEfPqObdy99tE31sbCxZWVm4azFVY/n4+BAbG+vqMERr\n57BB2j9g0x/AVgUjfgOjF0oTEDdhczhZfzCX1PRMvvwuD6eGkd3DeWRqbyb1i8Lb5Nkicbhtojeb\nzXTp0sXVYQjhvo5uNDYfyz8M3ScYN1vDe7g6KgEcyysnNcPCR9uzyS+vISrIm/8a0515yXHEhfq1\neDxum+iFEJdQdMJoAnLoM+iQALelQM8pUvDkYlVWB6v25ZCSbiHteCGeHopxvSO5bVgco3pEYPJ0\n3TSaJHohWgtrJWz5C3z9V/DwhPFPwvD7wCz3elxpX3YJqekWPtmVTVm1nYQwPx6e0ptbhsQQGeQe\n/20k0Qvh7rSG/R/DmiegNAv63woTnobgevcLFC2gtNrGil2nSEnPZF92KV4mD6YldmT+sHiu6xLq\ndgtIJNEL4c5O76ttArIFovrDLf+AziNcHVW7pLUm46RR1LRy7ymqbU56dwxk0cy+zB4cS7Cf+xai\nSaIXwh1VFsLGFyDjbfAJgRl/gSF3GVM2okXll9ewbEcWKekWjuVVEOBtYvbgWG4bFkf/mGC3G73X\nRxK9EO7E6YAd78H6Z6G6GJJ+BmMfA79QV0fWrjhqi5pS0jJZe+AMdqdmSHwIL88ZwIwB0fh5ta7U\n2bqiFaItO7kVVj1k9GztPNLYfKyjbHHdkrKLq1iaYWFpRhbZxVWE+ntx9wijqKlHMxc1NSdJ9EK4\nWukpWPsU7F0CQTFGM+5+N8tyyRZitTtZf/AMKekWNn9vFGiO7B7Oo9N6M7FvyxU1NSdJ9EK4ir0G\ntr5uNAJx2mHUQzDyAfDyd3Vk7cLRvHJS0y18tD2Lggor0cE+3D+2O7cmuaaoqTlJoheipWkN3602\n9ogvOg69Z8Ck5yBUKsGbW5XVwed7c0hNt5B2ohCTh2JCnyjm1RY1eXq0zb+iJNEL0ZLyjxgJ/sha\nCO8JP14G3S9q7SCa2L7sEj5Iy2TFrlOU1djpGu7Po1N7c/OQWCICvV0dXrOTRC9ES6gpgy9fhm1v\ngNnXaOM3bAGYGtdQQlxaSZWNFbuMvd73nyrF2+TB9P7RzE2Oc8uipubUqESvlHoA+Dmggb3APUA0\nkAKEAduBO7XW1kbGKUTr5HQaN1nXPgnlZ+o0AYl0dWRtktaatOOFpKZbWLk3hxq7k77RQTwzqx+z\nBsUQ7Ou+RU3NqcGJXikVA/wG6Ku1rlJKLQHmA9OAv2itU5RSbwI/A95okmiFaE2ydxhVrVlpEDMU\n5n8AsUNdHVWblFdmFDWlpls4ll9BoLeJOUNjuW1YPIkxwa4Oz+UaO3VjAnyVUjbAD8gBxgG31z7+\nHrAISfSiPSnPgw3PwI73wT8cZv0NBt4mTUCamMOp2fxdHqnpFtYdNIqahiWEct/Y7kzrH42vV+tf\nFtlUGpzotdbZSqlXgEygCliDMVVTrLW2156WBcjOS6J9cNgg/W1j6wJbBVx/n9EExEdGlE3JUljJ\n0u1ZLM2wkFNSTZi/Fz8d2YW5SXF0jwxwdXhuqTFTNx2AWUAXoBhYCky5husXAAsA4uPjGxqGEO7h\n2CZY9QjkHYRu42DKSxDR09VRtRk1dgfrDuSSkp7JliP5AIzqEcETM/oyoU8UXib5a+lyGjN1MwE4\nrrXOA1BKLQNuAEKUUqbaUX0skF3fxVrrt4C3AJKSkqRTtmidik7Cmt/DwRUQ0hnm/x/0miZVrU3k\nSG4ZKWkWlu3MprDCSqdgH/57fA9uTYojJsTX1eG1Go1J9JnAcKWUH8bUzXggA9gIzMFYeXMXsLyx\nQQrhdqyVRgOQr/8XlAeM+z1cf780AWkClVY7K/cYnZq2nyzC5KGY1C+KecnxjOwe3maLmppTY+bo\nv1VKfQjsAOzATowR+kogRSn1XO2xt5siUCHcgtZwYLkxii+xQOItMPEZCJZG742htWZvdgkp6RZW\n7DpFeY2drhH+PDbNKGoKD2j7RU3NqVGrbrTWTwFPXXD4GDCsMc8rhFs6cwBWLYQTX0FUIsz+OyTc\n4OqoWrWSShuf1BY1HcwpxcfswbT+0dw2LJ6kzh3aVVFTc5LKWCGupKoINv4B0v8JPkEw/U8w5G7w\nlP99GkJrzbfHC0lJy+Tzfaex2p0kxgTx7I8SmTWoE0E+7bOoqTnJT6oQl+J0wM73Yf0zRrIfeo8x\nFy9NQBokt7SaD3dksSTdwomCSgJ9TMxPjmNuUpwUNTUzSfRC1CfzW6MJSM5uiB9hNAGJHuDqqFod\nu8PJ5u/zSEmzsP5QLg6nZliXUH4zvgdTE6WoqaVIoheirtIcWPcU7EmFwE5wy9vGDVeZK74mlsJK\nltR2ajpdWk14gBc/H9mFeclxdI2QoqaWJoleCDCagGz7G3z5R3Da4MYH4cbfShOQa1Bjd7Bm/xlS\n0y1sOZKPh4JRPSNYdFM/xveJxOwpRU2uIoleiO/WGHvEFx6FXtNh8nMQ2tXVUbUa350xipo+3plF\nUaWNmBBfHpjQk1uTYukkRU1uQRK9aL8KjsIXj8L3qyGsB/z4I+g+wdVRtQoVNWeLmjLZkVmM2VMx\nqW9H5ibHSVGTG5JEL9qfmjKjT+vW18HkY7TxG/YLaQJyBVprdmeVkJpudGqqsDroHhnA49P6cPOQ\nGMKkqMltSaIX7YfWsHcprHkCyk/DoDuMJiCBUa6OzK0VV1r5eGc2qekWDp0uw9fsyfQB0dw2LI4h\n8VLU1BpIohftw6ldRlWr5VvoNBjm/Rvikl0dldtyOjXbjhWQkm7hi/1GUdPA2GBemN2fmQOjCZSi\nplZFEr1o2yryYcOzsP098AuDm14zRvLSBKReZ0qr+XC70akps7CSIB8TtyXHMTc5jn6dpKiptZJE\nL9omhx0y3oaNz0NNOQz/L6MJiG+IqyNzO3aHk42H80hNz2TDoVycGoZ3DeWBiUZRk49ZippaO0n0\nou05vtno1Zp7ALqOMZqARPZ2dVRu52RBxbmiptyyGiICvVkwqhvzkuPoEi71A22JJHrRdhRnGjda\nD3wCIfHGPHzvGVLVWke1zcHq/adJTbfwzdECPBSM7RXJvOQ4xvaWoqa2ShK9aP1sVfD1/4MtfzG+\nHvt7GPFrMEuxzlmHTpfWFjVlU1JlI7aDL7+b2JM5SbFEB8v3qa2TRC9aL63h4Kew+nEoyYR+s2Hi\nsxAS5+rI3EJ5jZ3Pdp/ig3QLuy3FeHl61HZqiuOGbuF4SFFTuyGJXrROuYeM5ZLHv4TIfnDXZ9Dl\nRldH5XJaa3ZaiklNs/DpnlNUWh30iAzgiRl9mT04hlB/KQprjyTRi9alqhi+fAm+/Tt4B8DUP0LS\nT9t9E5CiCivLdmaTmp7Jd2fK8fPyZOaATswbFsfguBApamrnGvV/h1IqBPgnkAho4KfAYSAVSABO\nAHO11kWNilIIpwN2/ttoAlJZAEPvhnFPgH+YqyNzGadT883RAlLSM1mz/wxWh5NBcSG8eHN/Zgzs\nRIB3+/7lJ37Q2J+EvwJfaK3nKKW8AD/gMWC91vpFpdQjwCPAw418HdGeWdLg84cgZxfEXw9Tl0H0\nQFdH5TI5JVV8mJHFku0WLIVVBPuauf26eOYlx9EnOsjV4Qk31OBEr5QKBkYBdwNora2AVSk1CxhT\ne9p7wCYk0YuGKDsN6xbB7g8gMBpu/if0n9Mul0vaHE42HsolNd3CxsNGUdP1XcN4cFIvJvfrKEVN\n4rIaM6LvAuQB/1JKDQS2A/8NRGmtc2rPOQ3Uu2OUUmoBsAAgPj6+EWGINsduhW/fhC9fBkcNjHzA\naATi3f46E53IryA1w8KH27PIqy1q+uVoo6ipc5gUNYmr05hEbwKGAPdrrb9VSv0VY5rmHK21Vkrp\n+i7WWr8FvAWQlJRU7zmiHfp+HXzxMBQcgZ5TYPILENbN1VG1qGqbgy/2nSYlPZNtxwrx9FCM7RXB\nvOR4xvaKwCRFTeIaNSbRZwFZWutva7/+ECPRn1FKRWutc5RS0UBuY4MU7UDhMWM9/OHPIbQb3L4U\nek5ydVQt6sCpUpZk/FDUFB/qx0OTezFnaCxRQT6uDk+0Yg1O9Frr00opi1Kql9b6MDAeOFD77y7g\nxdqPy5skUtE21ZTDlj/DN6+CpxdMfAau+1W7aQJSVm3j091Gp6Y9WSV4eXowObEjtyXHMbxrmBQ1\niSbR2FU39wP/qV1xcwy4B/AAliilfgacBOY28jVEW6Q17PvI2Jum7BQMvA0mLILAjq6OrNlprdmR\nWURKmoXP9uRQZXPQKyqQp2b25UeDYuggRU2iiTUq0WutdwFJ9Tw0vjHPK9q4nD1GVWvmVogeBHPf\ng7hhro6q2RWU1/DxzmxS0i0cyS3H38uTHw3uxLzkeAbGBktRk2g2UlEhWk5lIWx4Drb/C3xD4aZX\nYdCP23QTEKdTs+VIPqnpFtYcOI3NoRkSH8JLt/RnxoBO+EtRk2gB8lMmmp/DbiT3Dc8ZjbmH/QLG\nPNKmm4DklFSxNMPo1JRdXEWIn5kfD+/M/OR4enUMdHV4op2RRC+a1/GvapuA7Icuo2DqyxDZx9VR\nNQubw8n6g7mkpmfy5Xd5ODWM7B7OI1N7M6lfFN4mKWoSriGJXjSPYgusfQL2fwzB8TD3fegzs01W\ntR7LKyc1w8JH27PIL7cSFeTNfWO7MzcpjrhQP1eHJ4QketHEbNXGUsmv/gRoGPMojPgNeLWthFdl\ndbBqXw4p6RbSjhtFTeN6RzI/OY7RPaWoSbgXSfSiaWgNh1bC6seg+CT0uQkmP2+09GtD9mWXkJpu\n4ZNd2ZRV20kI82PhlF7MGRJLpBQ1CTcliV40Xt5h+OIROLoBIvrAT1ZA19GujqrJlFbbWLHrFCnp\nmezLLsXL5MG0xI7MS45neNdQWRYp3J4ketFw1SWw6SVI+zt4+Rs3WpN+1iaagGityThpFDWt3HuK\napuTPtHc/1sOAAAgAElEQVRBPH1TP340KIZgP7OrQxTiqrX+/yNFy3M6Yff/GVsIV+TDkJ/A+CfB\nP9zVkTVafnkNy3ZkkZJu4VheBQHeJm4eEsv85Dj6x0hRk2idJNGLa5OVYVS1Zm+HuOvgjg+h0yBX\nR9UoDqfmq+/zWJJhYe2BM9gcmqGdO/DyLd2YPiBaippEqyc/weLqlJ2B9U/Drv9AQEeY/RYMmNuq\nl0tmF1exJN3C0gwLp0qqCfX34q7rE5iXHEePKClqEm2HJHpxeXarMQe/6SWwV8MN/wOjHgTv1pkI\nrXYn6w+eISXdwubv8wCjqOnx6X2Z0DdSippEmySJXlzakXWw6hEo+B56TIIpL7baJiBHcstZkmFh\n2Q6jqCk62If7x/Xg1qGxUtQk2jxJ9OJi5zUB6Qq3L4Gek10d1TWrsjpYuTeH1PRM0k8UYfJQTOgT\nxbzkOEb1jMBT9noX7YQkevEDa4VR0frNq+BhhvFPwfX3gcnb1ZFdk33ZJaSkZ7J85ynKaux0Cffn\nkam9uWVILBGBreu9CNEUJNGLi5uA9J8LE5+GoE6ujuyqlVTZWLHL2Ot9/6lSvE0eTO8fzbzkOIZ1\nkaIm0b5Jom/vTu+FzxdC5jfQcQDc+i+IH+7qqK6K1pq044WkpltYuTeHGruTvtFBPDurHzcNiiHY\nV4qahIAmSPRKKU8gA8jWWs9QSnUBUoAwYDtwp9ba2tjXEU2sshA2Pg8Z74BvB5j5Vxh8J3i4/6qT\nvDKjqCk13cKx/AoCvU3MGRrL/OR4+scGuzo8IdxOU4zo/xs4CATVfv0S8BetdYpS6k3gZ8AbTfA6\noik4HT80AakuheSfw9jHjGTvxhxOzebv80hNs7Du4BnsTk1yQgfuG9udaf2j8fVy/19QQrhKoxK9\nUioWmA48D/xWGROh44Dba095D1iEJHr3cPIbY5rmzF5IuBGmvgRR/Vwd1WVlFVWyJCOLpRkWckqq\nCfP34qcjuzA3KY7ukQGuDk+IVqGxI/r/BRYCZ6tnwoBirbW99ussIKaRryEaqyQL1j5p3HANioVb\n34O+s9y2qrXG7mDdgVxS0jPZciQfgFE9InhyRl/G94nCyyR7vQtxLRqc6JVSM4BcrfV2pdSYBly/\nAFgAEB/ftvYsdxu2atj6Knz1Z2PKZvTDRmWrmzYBOZJbRkqahWU7symssNIp2IffjOvB3OQ4YkJ8\nXR2eEK1WY0b0NwA3KaWmAT4Yc/R/BUKUUqbaUX0skF3fxVrrt4C3AJKSknQj4hAX0hoOr4LVj0LR\nCeg9w2gC0iHB1ZFdpNJqZ+WeHFLTLWSc/KGoaf6wOG7sIUVNQjSFBid6rfWjwKMAtSP6B7XWdyil\nlgJzMFbe3AUsb4I4xdXK/95oxn10PUT0hjs/gW5jXR3VebTW7M0uISXdwopdpyivsdMtwp/Hp/Vh\n9pAYwgOkqEmIptQc6+gfBlKUUs8BO4G3m+E1xIWqS2Hzy7DtDTD7w+Q/wLB7wdN91pKXVNr4pLao\n6WBOKT5mD6b378S85DiSEzpIUZMQzaRJEr3WehOwqfbzY8CwpnhecRWcTtiTAmufgoo8GHInjHsS\nAiJcHRlgjN63HStkSYaFz2uLmhJjgnj2R4nMGtSJIB/3+UUkRFsllbGtWfZ2Y7lkdgbEJMHtKRAz\n1NVRAZBbWs2HO7JYkm7hREElgT4m5ibFMS85jsQYKWoSoiVJom+NynONJiA7/w0BUfCjN2HAPPBw\n7bJDh1Pz5Xe5pKRZWH8oF4dTM6xLKL8Z34OpiVLUJISrSKJvTRw2SHsLNr0ItioYcT+MWgg+QVe+\nthlZCitZkmFhaUYWp0urCQ/w4uc3GkVN3SKkqEkIV5NE31oc3WA0Ack/DN3GG1Wt4T1cFk6N3cGa\n/WdITbew5Ug+HgpG94xg0U1GUZPZU4qahHAXkujdXdEJownIoc+MdfDzP4BeU11W1frdmTJS041O\nTUWVNmJCfPmfCT2YmxRHJylqEsItSaJ3V9ZK2PIX+Pqvxo6S45+E4feB2afFQ6moMYqaUtIz2ZFZ\njNlTMalvR+Ylx3FD93ApahLCzUmidzdaw4FPYPXvoTQLEufAxGcguGW3DNJaszurhNT0TFbsOkWF\n1UH3yAB+P70PswfHECZFTUK0GpLo3cmZ/UZV64mvoGN/uOUf0HlEi4ZQXGnlk51GUdOh02X4mj2Z\nMcDo1DS0sxQ1CdEaSaJ3B5WFsOkPkP5P8AmG6X+GoXe3WBMQp1Oz7XgBqekWVu07jdXuZEBsMC/M\n7s/MgdEESlGTEK2aJHpXcjpgx3uw/lmoLoaknxlNQPxCW+Tlc0urWbo9iyUZFk4WVBLkY+K25Djm\nJcfTt5Nrl2wKIZqOJHpXObkVVj1k9GztfANMfRk6Jjb7y9odTjYdziMl3cLGw0ZR0/CuoTwwoSdT\nEjviY5aiJiHaGkn0La30lNEEZO9SCIqBOf+CfrObfbnkyYKKc0VNuWU1hAd4s2BUV+YmxdEl3L9Z\nX1sI4VqS6FuKvQa2vg6bXwGnHUY9BCMfAK/mS7LVNgdrDpwhNT2Tr48U4KFgTK9I5iXHMa53pBQ1\nCdFOSKJvCYe/MJqAFB5rkSYgh06Xkppu4eOd2RRX2ogL9eXBST2ZMzSOjsEtvw5fCOFakuibU/4R\n+OIROLIWwnvCj5dB9/HN8lLlNXY+232KlHQLuyzFeHl6MKlfFPOT4xnRLQwPKWoSot2SRN8caspg\n8x9h69/A7AuTX4BhC5q8CYjWmp2WYlLTLHy65xSVVgc9IgN4YkZfZg+OIdTfq0lfTwjROkmib0pO\nJ+xJhXVPQfkZGPRjmPAUBEQ26csUVVhZtjOb1PRMvjtTjp+XUdQ0f1g8g+NCpKhJCHGeBid6pVQc\nsBiIAjTwltb6r0qpUCAVSABOAHO11kWND9XNZe8wqlqz0ozmH/M/gNimawLidGq2HisgJd3C6n2n\nsTqcDIwN5g8392fmwE4EeMvvbCFE/RqTHezA77TWO5RSgcB2pdRa4G5gvdb6RaXUI8AjGH1k26by\nPNjwDOx4H/zDYdbrMPD2JmsCcrqkmg+3W0jNsGAprCLY18zt18UzLzmOPtFS1CSEuLIGJ3qtdQ6Q\nU/t5mVLqIBADzALG1J72HkYv2baX6B02SH8bNr4Atgq4/j4YvdDYwqCRbA4nGw/lklpb1OTUMKJb\nGA9O6sXkflLUJIS4Nk3y975SKgEYDHwLRNX+EgA4jTG107Yc22Q0Ack7CN3GwZSXIKJno5/2eL5R\n1PTh9izyymqIDPTml6O7MS85js5hUtQkRHumnZqaKjvV5TaqK2xUlduu+tpGJ3qlVADwEfA/WuvS\nujcCtdZaKaUvcd0CYAFAfHx8Y8NoGUUnYc3v4eCKJmsCUm1z8MW+06SkZ7LtWCGeHoqxvSKYnxzP\nmF4RmKSoSYg2R2uNrcZBdbmRsKvLbVSXW899XlVx9tjZx61UV9jRznrT6RUprRt2IYBSygx8BqzW\nWv+59thhYIzWOkcpFQ1s0lr3utzzJCUl6YyMjAbH0eyslUYDkK//F5QHjPyt0a+1EU1ADpwqJTU9\nk493ZlNabSc+1I95yXHMGRpLVJAUNQnRmtitjjoJ20ZVhfWCJH5BQq+w4bTXn3uVh8LH34RPgBe+\nAWZ8AsznPvr4m/EN9Dr3eccuwdu11klXiq8xq24U8DZw8GySr7UCuAt4sfbj8oa+hstpDQeWw5on\noCQTEm+pbQIS26CnK6u28eluo1PTnqwSvDw9mJJodGq6vqsUNQnhDhwO57nkXHdEXVU7ZXJh4q4q\nt2K3Out/MgU+fmeTtInAMB8iOwcaXweY8a2TzM8mb29fE6qJc0Fjpm5uAO4E9iqldtUeewwjwS9R\nSv0MOAnMbVyILnLmAKxaaDQBiUqE2SshYeQ1P43Wmh2ZRaSkWfhsTw5VNge9ogJ5sraoqYMUNQnR\nbJxOTU3lhSNqIzmfO1Zx/uPWKvsln8/LxxOfQC98/M34BXkR2sm/dpRdO9oOMEbbvoFG4vb2M7vF\nAK4xq262AJd6B81T598SqopgY20TEO9AmPYKDL0HPK/tW1VQXsPHtZ2ajuQaRU2zBnViXnIcg6So\nSYhrprXGWu2gqsx68XRIhbX+aZJKm1HlUw+Tl8d5UyFB4b7nTZV41z7mWzvS9gkw42lqnffMpMrm\nLKcDdr4P658xkv3Qu2Hs78E/7OqfwqnZciSf1HQLaw6cxubQDI4P4aVb+jN9gBQ1CXFW3ZuRF46o\nz4626464q8pt1JTbcF7iZqSHpzo3FeITYCYsJuDcqPrsiNvX3+uHKZIAM2av9rNMWTIPQOa3xjRN\nzi6IHwFTX4LoAVd9+aniKj7cnkVquoXs4ipC/MzcOTyBeclx9OoY2IyBC+Ee7LYLV5D8kKAvmiqp\n/eew1z+vrRTn3XgMifSjYzczvv51b0x6nRtl+waaMXt7yl/Jl9G+E31pDqxbBHtSILAT3PK2ccP1\nKn5gbA4n6w/mkpqeyZff5eHUMLJ7OA9P7c3kflF4m9rPaEG0LQ6Hk5oK+8Uj6wtWlJw9XlVuw17j\nuOTzefuZziXuwFAfIuIDjaQdeH7SPjtt0hw3I9u79pno7TWw7Q1jh0mHFW58EG787VU1ATmWV05q\nhoWPtmeRX24lKsib+8Z259ahccSH+bVA8EJcPe3U1FTaL3Pz8YKRdoWNmspL34w0+3jWScpedOho\n3Iz8IWnXfjw7TeJvwkNqQVyu/SX679YYe8QXHoVe04wmIKFdL3tJldXBqn05pKRbSDtuFDWN6x3J\n/OQ4RveUoibRMs7ejKxbWFNvwU2dRF5TYeNSpTKeZo/zknNQmA8+F9x8vHDE7WmWn/XWqP0k+oKj\n8MWj8P1qCOsBd3wEPSZc9pJ92SWkplv4ZFc2ZdV2Oof5sXBKL+YMiSVSippEI9msjh+SdZn1vFF1\n3dUkdZO503GJm5Ee6odRtb+Z0Gj/85L22RuTvgFeePub8A30alc3I9u7tp/oa8qMPq1bXweTD0x8\nFq77JZjqX79eWm1jxa5TpKRnsi+7FC+TB9MSOzI3OY7hXaSoSdTPYXPWSdAXJO2y+gtu7LYr34z0\nCTATHOFHVILph5F1oPmighuzj9yMFJfWdhO91rBnCax9EspPG1sHT3gKAjvWc6om46RR1LRy7ymq\nbU56dwxk0cy+zB4cS7Bf03aGEu7N6XBSXWE/N6KuumiK5OKEbrvSzcjapB0Q4k14XOB5Nx/rTpP4\nBnjh5WeSAYVoUm0z0Z/aZSyXtHwLnQbDvPchbthFp+WX17BsRxYp6RaO5VXg7+XJ7MGxzE+OY0Bs\nsIyQ2oC6O/6dW5Ndt+Cm4oL57fIr3Iz09jxvVB3S0e+8isjzRtxni2zkHo5wsbaV6CsKYMOzsP1d\n8AuDm16DQXec1wTEca6oKZO1B85gc2iGdu7Ay3O6Mb1/NP5S1OS2tNbYqh3nr82uu/dIhY3qsjoF\nN7WPXfJmpMnjh3J1fzMR8T71Juy6NydN0gtAtEJtI6s57JDxDmx8DqwVMPxXMPph8A05d0p2cRVL\nMywszcgiu7iKUH8v7rreKGrqESVFTa5Qd8e/+gpqzrspeTU7/tVJyh2i/Y2pkECvC1aQ/DC3bfLy\nkL/aRLvQ+hP98c1Gr9bcA9B1jNEEJLI3AFa7k/UHz5CSbmHz93mAUdT02LQ+TOgbKUVNTchhd15U\nyn7e/trnLfszkvrV7fhnJijch8iEwPM3jTovaZvx8jVJ0hbiElpvoi+2GE1ADnwCIfEw79/QewYo\nxZHccpbUFjUVVFiJDvbh/nE9uHVoLHGhUtR0JU6npqbiwpuP1gtG2OcX3FirL30z0sv3h8pIv2Av\nwjr5413n5uOFBTfusuOfEG1F60v0tir4+v/Blr8YX499HEbcT5X2YuWObFLTM0k/UYTJQzGhTxTz\nhsUxqkcEnu00cWitsVbZL1g5comCmwojoddU2i+945+3Jz7+pnPL+oIjfI0No+opZT+b3Fvrjn9C\ntBWtJ9FrDQc/hdWPG01A+s2Gic+yryKIlJVHWL7zFGU1drqE+/PI1N7cPCSGyMC2VdR04Y5/F464\n6xtt11TYL73jn0mdNxUS3iHA2J71Ejv++QaYMUmRjRCtTutI9LkHjXn4419CZF/K53/Mx0VdSVl8\ngv2nSvE2eTCtfzTzk+MY1iW01czVntd+rL79R+oeK7v69mM+/iY6dPTDNyD4hxUkdUfctatJZMc/\nIdoH9070VcXw5Uvw7d/R3gFkDnuKV0tH8em/86ix76dvdBDPzurHTYNiCPZ1bVFTve3HKi49TVJV\ncYUd/2qnR3z8TQSG+hAZH3jeXHbdZYCy458Q4nLcM9E7nbDr37DuaXRlAQc73cxjJT9i12ZPAr0L\nuDUplnlJ8STGBDXLiPRK7cfO32f7KtuP1Y6o62s/5nNh4pYd/4QQTajZEr1SagrwV8AT+KfW+sWr\nutCSjl71EOrUTo74JPI722/ZfSyB5IQOvDIunun9o/G9hnniS7Yfu7Ai8uw0Sdm1tx+ru3777PG2\n0H5MCNE2NEuiV0p5Aq8DE4EsIF0ptUJrfaC+80vPWPj28Epidy4l5tDH5BPKs9b72OIxhltGxPKn\n5HgSVBUV23eiyoIoxVxvJ/aqcit5hYWUni7EqX3QNZ44qkBfYrn2D+3HjIQc2sn/ospI7wATOfYs\nyj1L0N527B6VBHoFktSxPz6mS9/stRcV4SjKwzOky0WPaa0pqC4gqyyL0ycO4Nh7gMjxU0iMHYqv\nybfe53OUllK2fgP+w5Ixx8TUe47NYeNwwUEy1yxHRUcRmTiU2IBYIvwi8FAX/7KxZmWD04FXfPyl\n34fTzt68PeSUnsLbyxcvTy8CvQJJDE/E5NGwHx+b08bR4qMUVRdRUllIzfHjxPQaysCYJMye9U/B\naZuNiq1bMcfF4d3l4u8pQElNCUeLjnB642rwUMSMnUqPDj3xM9e/pLb6u++wZWUTcONIlLn+161x\n1LDnZBqn96fj3bs3oYGRhPmG0Tmoc73fU6fVijKbL/uXptVhZXfebsqt5di1HbvTTqeATvQP71/v\nc55lO3MG5emJKTz8osfsTjunK05zqvwUeQd3ovMK6Dp+Fj1De13yv5M1K4vK9AwCJ07AMyCg3nPK\nreXsy8qgcP0azIMG0Klrf+ID4wn0urjIUGtN9YEDmMIjMEdFXvJ9VNoqSc/aRoWtAh8ff3xMPkT6\nRtItpFuD/0KvsFVwtPgopdZSysoKcJzJpWf/0fTo0OOSz+kor6AyLQ3fwYMwdehQ7/vJrczlaN5h\nitatxhQfR7fkCSQEJ1zye1qx7VsA/K4bdsnXLakpYceBdZRnZ+Kf2J8w/wg6+nUkyj+q3hh0VRUe\nfo1bFq70perDG/OkSl0PLNJaT679+lEArfUf6ju/j3+QfumGcWwaEEh+aBBBjj70C+pHgm841ko7\nFVm5VBZWYPP0w+lZ/66TWmlsHhX4VZXhX1mByV5OhVcF+UEVZEdZierWlSGdBzIgth/+/maqP/2I\n4jdeRQGmiAg8I8LxGzKUsJ/9FBUUyO683aw5sYadO1dx4/pcYvM1IRUQXAEl/vB/k3zwGjuSMbFj\nGB8/nmDvYKp27qR8wwYqvtlK9cGDoDXBs2cT9cjDeAYHk1mayafHPuXTIysIOJTN1O1Ohh3WmJxw\nOgTenmrGNrQPo2NHM7PbTOIC43BarRR/8AH5f3sDR0kJysuL0LvuIuwXC/AMCCC/Kp+Vx1ayIXMD\npQf2cvfKKnqeMr4nuxMUnycrMvuGMbXbdGZ2m0mf0D5om438v/2Ngn/8ExwOzJ3jCbhxFAFjx+B/\n/fVUOarZnLWZTVmbOLVtI7d9Wkp8HhQFQEEgnOmg+PKGYLoPn8TEzhO5ruN1eNqdFC5eTPmmL3EU\nFeEoKkLX1BBy23wi7rsP5evL7rzdrDy2krQ9q+i3q4h+JzV9LBpfK2SFwTs3+RGaNJxRMaOYnDCZ\nEJ8QbGfOULxkKcVLlmDPywOzmfB77yXsl7/Aw8uLY8XHWH50OatPrCbwYBZ3bHTQK9t4//s6KxaP\n98SzZzemdJnCTd1uolNAJ5wVFeS99jqFixeDw4GpY0dC77yTkLm34hkYSG5lLquOr2JL9hZs2zK4\nZ2U1kSVQ6QV7uii2d1ecHNyRsX2mM73LdHp26ImztJQzL79MyUfL8PDzwxzTCXOnGALGjyPk5pup\nxsbX2V+z9uRacjevZ9LXlYSWa/yqwb8GjkfBRzND6ZE0gbFxYxnRaQQmB5StXkPFN99QmZGBzWJB\neXkRfv+vCbvnHvD0ZFfeLj49+inrj3xB3z0lTNjlpK/lh/f//nQ/OvYewoTOE5icMJlg72DsBQXk\nv/l3ilJSwGbDMyyMiP/+DSG33ILy9OT7ou9ZcXQFX2V9RWj6Ee5Z6yC8FOwesKWf4tPrPPDu0YOZ\n3WYyvct0ovyjsOflcfqZZylbuxYAnwEDCBw3jsDJk/Du0oXcylzWnlzLZsuXeK/dxh3rbPhXQ34w\nnAlRHI+CXePiGJ44lUkJk+gT2gdHfj55r71O1Z49OMvLcZaXG+//1/cRcsstWLWNr7K+4vPjn5O9\nbSNDDtbQK0vT9TSYHbCjq+Kj2RH06TOScXHjuDH2RsweZqr3H6B4yRJKP/sMZ2Ulnh06EPXYowTN\nmIFGs/3MdpYfWc6mkxsYuLOEuV85iSwBJ7BpgGLZOB/iEwYys9tMJnWeRIBXALbsbE4//wLlGzYY\n779vX8IW3EvgxIkoT08OFx5m5bGVbMv+hoR1B7ltkxMfGxT7Q3oPRVovhX1wX6b2mMGUhClE+UdR\nc+wYOU88SdX27XiGhODVuTNeXboQcsvN+CUnk1+VT4RfxHatddIVc3IzJfo5wBSt9c9rv74TuE5r\n/ev6zo+P6KUfvuWN8445qADPaoKqqvAtLcQ72IfgxJ7oYwfR+3dispZRGQ57IgrYFVdKYp7ipk3l\nKC9vfO+9E0d+AdZtGXgeOQnAnu4mPh3qRAUH88svNKGZxfjceAN+Xbthy83DejqH6l27sfmZWXGj\nD6t7VDD7W5i83YkymaB/bzzCQvEMC6VmWzqmY1ns6ePLW6Ot9MvyYM5uHyIsZWAy4TN4IIEjbsBW\nXkbRu+9hDfTh01lRfF95kkHHYHimFx3yqnEG+GG6aTL+g4dQ9L+vYsrOZW9SGBs6FRNaruntiGTA\noWq8zxTjM/w6In5+L0UrPqF8xWfYg/3ZNySUg/oUxX6aAeWhXL+lAB3oR8D9v8JeWEBN6sd4FBRT\n3sGHjHgbuztrAiJjmLuqjMDsYvxnzSQgcQBlm7+kMi0NaqyURvixcqCDr3rauTXdzJiMahxhIXhP\nnYCjoBBnbh760FE8yyvZlmjm3yOd9M/z4Y5NmoCCSrwG9McruhOmDiHYSoqpWLWaijB/3p/mw3HP\nYm7KgOsPOPFwapxx0ZiSBuHdowdl776PZ14Rm0cE83nPcgadUIzJDKDj8RKUBt+RIwidM5fitaup\n+OxzKmNCWTM6kPx8CxFlisFFQcQfKsIRFkLAL3+KttmofPMdVFkFhwaHs8e/kHJf6BycwI0b8/HO\nLyXw1lsIGjWa/PcXU5OWgdPHixPdA/g6qoRDnWD2oQCS0kuwx0URePdPqNqzG+eWNDwKirF6ebKl\nL3wxGPrXRHDryhK8y2oInjsHT7M31uwsao4dw37iJMVR/iwe5eBoBxv3fOnJoMNWHJGhePTujgoM\nxMPXF9uaDajyKr4Y7sWKwXYmHjQzbTv4llSjQoLxGzoU/6RkyjK+pWr9Joq6hPHeJDMqJ5frjngw\n5Dh4VdlxdorA5+ab8PD3o+K1f4DVxroxIXznU0THUk8GWiPpvisfjxobQbN/RPCUqZz522tYd+6m\nIiGSjO6KTGce1d4ejMsKptveAuxdYuhw/39R/s03OD5di0eNlex4f7ZHV3I4zpP+XgmMX56FyeYg\n/Be/wMPTk5J167Du2w+ApVsQn/St4FhH+NVGL3oeqcLerzvew4biyDqFMzsHdfgYdk/4LEnxeRLM\nOhTIlM0VeNqd+I8YgSk4GI+AACoPHsC6aw+nu3fgtQk2/AsqmZPmSY+TNrTZhO7VFfOg/nj4+1Hz\nbgp2pUmZ4M3h0GpGHjUz8qiZwNNl4O1N4LSpBI0bx5m33sS+9wBnBsbyWWINHmcKiCsxkZTlTUhO\nGY6eCXT41S8oS0/DuWQFdpMHW5P8yTSX4vA2M9CcwIA1x/BQHoTfdx+mDh3I+8c/cJzMpCY8iP0J\nHqRFlJIX5sk93/gQe6wMx7ABBE6fRsWXX8I3O1DVNZQGmVib6OCrRE9mW6K4ce1pPHx9Cbv9DhzF\nRdScOEH1oUPoklKO9Q7mreEVrHh0n3sneqXUAmABQOfo2KGb1+3EuXUjFR/+H1iO4mG3AVDmC4vH\nefBlf0W4XwSF1YUE///27j04qvoK4Pj37CPZzea5m4QEAgEEo6E+eJiCilWpFoRRB2xLC8VXO3aq\nre3U6Whnaqcz9g871j59TCsqM/VFwfGFBa1agXaSIZKG8JQIkhBCHpDdbB77yO7pH/caiDUW/kiW\n7v4+/5B7d9k599yzJ3fPvZvbm+DaRmXuIWH6sSQOextyFy2i7OcP4i499bEx3tFJcMNf6XnhRRLd\n3QAEc4W11wk7LnDiECdDap1IrexQ1rynXHTYnvU4HBSuWE7xPd8f8VFU43FOPPss3X98DI1GAWgv\ndfHanCTbZwnRLBkewZQdHeB7mxJUdtr/OTsb3/wvkrdoEQXLlg1/JEtGo3Q//gQn1q6FISueaJZw\npFhZv9DBrukOSnNKOTF4ginHhlj1jyRVbZAdP7X/ClYsp/S++4Y/hmo8Tu+Wtwhv2UxfbR0aDgPW\nUfmTSxzsmZmFx+mhL96HO65cdlBZ2uBgZsvQJzuJotWrKbn3Xpy5p26zmAiHOfHUWk6uW4dGIgC0\nlFFcjlMAAAlySURBVDl55hplz1Rr9CAIIsL5LQnu2qxM6rZyKjk5FH31FopWrRoxMkr09dP16K/p\nef6F4XUt5S52TEvwj4sddBQJfo+f3mgvsz6K8Z3N1lEWAC4X7okTKVyxAv+ab+HwWrlPhEJ0P/Ek\nwY0bSdrbDtBSDH9e4uSjKW7ys/I5GTnJtOPKon8nubTVSWm3vf1OJ4E77qD4nrtxZGdbOVUlsmsX\nPevXE9q0CSLW/j9cJjx+g4Nj5VmoqlVTauV09ftCebd1hZXD5yPw3bvwr1kz/Jpgjfo6H3mE0MaX\nh9c1nefklcuU3VMFFcHn9hGJD1Kzb4g731LyB6x97/D7ybv2GvKXLMG3YAFi/xG/eEcnHQ89NHyU\nDRDKFfZMhvULHXQWu/F7/HQOdLBgv/L195OUBcFhl5R4vZTcczf+NWuGR1tDPT0EX1pP39atDDY1\nQdx6nx6YBE8sdRKc4ENRBocGKQorV+1WvrLLQfFJK6eOnBxK7vsxRStXDscJEDtyhK7f/4HeTZuG\n19Wf72DdNUKH3xqBuMRFIjnEl5qSrHlXyR20AnWVlRG4/TYKb7kFh+9UncZaW2n/2YMM1NYCkHQI\ne6Y6qJ2pbK8Wol4nhdmF9AycYEm9snKrdZQNID4fnhkz8N9+G3nXXz8ca/TwYTof/hV9W7daF43Y\ndswUnr7OwWDAh9PhpC/SS82H1vZXH3Pg67f3f34+Ex54gIKbbxoe7SQjEfq2biW4cSP927YPv+4/\nLxSevc5BrCCHWCJGQhO448rincryWvANJKk+sD+ljf6sRjfz5s3T+vr64WVNJkmEQiROnCAWyGNP\n5DCNnY20hlspzy2nIreCirwKZgVm4R6I0V9Xh8PjxXflFaPOxTQWo3fLFuJtx8hb+VWaIoeoO17H\nUHKILEcWbqebyXmTWThpIcnanfRv30bB8hV4qs4fdTtjra0EN27EV1ODZ/4Xaexq5FDoEMFokJ5I\nD0lNMnfCXOYFLkXe/RfOQICcefNGvME/Ld7eTrK/H9eECYjPR1t/Gwd7DnKw5yAt4RbKfGVUFVVR\n5a9ict5kiERJnDwJMOrsHkATCSJ79xH9qBn31VfQ2H+QuuN1RBNRirKLKPIUMSl3EjVlNSQOfkT4\n7b+Te9VCvJdcMnqsHR30PPc8WVOnkrNsCXWdOzjQc4B4Ik48ab1jZpfO5rLAbAY2vALJBAXLl+PM\nG/2PyA00NBD7+Ai+yy9HSvzs7t5Na7iVtr42jvUdI+ANMLt0NhflVuE52o2rtARXcfGIpvGZ2x+L\nkejtJREO0xvw0hTcS2NXI6FoiIo8q54q8yqp8leR6OxmcOcHZE2fjqeqatTXTIRChF5/A3G7ybpx\nMbWdO2jsasTpcJLtzMbj9FAdqGZO4BL6XnudWOtR/KtX4SopGX376+vp27ad/BuWwHmV1LbXcrz/\nOKFoiGA0iNflZcHEBXzBNYXBt97BW12N5+KLP3f7Iwc+RNxu3BPLSWQ5ORw6THNPM83BZjoGOphW\nMG24pko8xSQHBkiGwzh8Ppz5+aO+bjISIdLURCIUIjL/Iuq7dtLQ2YBTnAS8AQKeADOLZlJddCGD\ndXUMNDRQuHw57vLy0WPdu5fQq6+Se/XVJObO4v3W92nrayOhCRLJBG6nm8snXs6FzgpCf3ke95TJ\nFCxdOuo5FlUlvOUtdGiI3KsWMuhxsKtrF+397bT3t9M12EVlfiVzSudwfrIE2jvJqqzEWVT0uecL\nVBWNRKxcDcVp8wyyu3s3Td1NJDVJZX4lU/KmML1wOhW5FcSPHCGyfz85c+d+7v6Pt7fT+7fNZM84\nj4F5F7Dt6Daag814XV68Li857hxqymqY5iqj59l1lP7g+ylt9C7gQ2AR0AbsAL6pqns+6/mfbvSG\nYRjG/yYiZ9Tox+SqG1UdEpF7gC1Yl1c+PVqTNwzDMMbWmF1Hr6pvAm+O1esbhmEYZ8Z8k8cwDCPN\nmUZvGIaR5kyjNwzDSHOm0RuGYaQ50+gNwzDSnGn0hmEYaW5MvjB11kGIhIEDqY7jHFIMdKc6iHOE\nycVIJh8jZXo+KlV19K/a2s6VG48cOJNvd2UKEak3+bCYXIxk8jGSyceZMaMbwzCMNGcavWEYRpo7\nVxr9n1IdwDnG5OMUk4uRTD5GMvk4A+fEyVjDMAxj7JwrR/SGYRjGGEl5oxeRxSJyQESaReT+VMcz\nnkRksoi8JyJ7RWSPiNxrr/eLyNsictD+97/vXJzGRMQpIg0i8oa9PE1E6uwaeUlEPvvGwWlIRApF\nZIOI7BeRfSKyIFPrQ0R+ZL9PdovICyLiyeTaOBspbfQi4gQeA5YA1cA3RKQ6lTGNsyHgx6paDcwH\n7ra3/37gHVWdCbxjL2eSe4F9py0/DPxGVWcAPcCdKYkqNX4HbFbVC4BLsPKScfUhIpOAHwDzVPUL\nWPe5WElm18YZS/URfQ3QrKqHVDUGvAjclOKYxo2qtqvqTvvnMNabeBJWDtbZT1sH3JyaCMefiFQA\nS4Gn7GUBrgU22E/JmHyISAFwFbAWQFVjqhokc+vDBXjtO9jlAO1kaG2crVQ3+klA62nLR+11GUdE\npgKzgTpggqq22w8dByakKKxU+C3wE+CTOy8HgKCqfQf3zKqRaUAX8Iw9ynpKRHxkYH2oahvwCNCC\n1eBDwAdkbm2clVQ3egMQkVxgI/BDVe09/TG1LovKiEujRGQZ0KmqH6Q6lnOEC5gDPKGqs4F+PjWm\nyZT6sM9D3IT1y28i4AMWpzSo/yOpbvRtwOTTlivsdRlDRNxYTf45VX3ZXt0hIuX24+VAZ6riG2dX\nADeKyMdYY7xrsWbUhfbHdcisGjkKHFXVOnt5A1bjz8T6+DJwWFW7VDUOvIxVL5laG2cl1Y1+BzDT\nPnOehXVy5bUUxzRu7PnzWmCfqj562kOvAbfaP98KvDresaWCqj6gqhWqOhWrFt5V1VXAe8At9tMy\nKR/HgVYRqbJXLQL2kpn10QLMF5Ec+33zSS4ysjbOVsq/MCUiN2DNZZ3A06r6y5QGNI5E5EpgG9DE\nqZn0T7Hm9OuBKcAR4GuqejIlQaaIiFwN3Keqy0RkOtYRvh9oAFarajSV8Y0XEbkU68R0FnAIuB3r\nAC3j6kNEfgF8HetqtQbg21gz+YysjbOR8kZvGIZhjK1Uj24MwzCMMWYavWEYRpozjd4wDCPNmUZv\nGIaR5kyjNwzDSHOm0RuGYaQ50+gNwzDSnGn0hmEYae4/xjBb45I4RF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefe4a5b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Data 2 (X : Multiple, Y : Multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfeature = 1\n",
    "yfeature = 1\n",
    "look_back = timestep = xlen = 4\n",
    "foresight = 1\n",
    "ylen = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_1d = data.iloc[:, :xfeature].values.astype('float32')\n",
    "print(data_X_1d.shape)\n",
    "data_X_1d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.2       ],\n",
       "       [ 0.40000001],\n",
       "       [ 0.60000002],\n",
       "       [ 0.80000001]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y_1d = data.iloc[:, -yfeature:].values.astype('float32')\n",
    "print(data_Y_1d.shape)\n",
    "data_Y_1d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping : 1D to 2D (for `MinMaxScaler`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_2d = data_X_1d.reshape(-1, xfeature)\n",
    "print(data_X_2d.shape)\n",
    "data_X_2d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.2       ],\n",
       "       [ 0.40000001],\n",
       "       [ 0.60000002],\n",
       "       [ 0.80000001]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y_2d = data_Y_1d.reshape(-1, yfeature)\n",
    "print(data_Y_2d.shape)\n",
    "data_Y_2d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling : `MinMax`, 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.01010101]\n",
      " [ 0.02020202]\n",
      " [ 0.03030303]\n",
      " [ 0.04040404]]\n",
      "[[ 0.        ]\n",
      " [ 0.01010101]\n",
      " [ 0.02020202]\n",
      " [ 0.03030303]\n",
      " [ 0.04040404]]\n"
     ]
    }
   ],
   "source": [
    "scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "scalerY = MinMaxScaler(feature_range=(0, 1))  # sigmoid(0, 1), tanh(-1, 1)\n",
    "\n",
    "scaled_X = scalerX.fit_transform(data_X_2d)\n",
    "scaled_Y = scalerY.fit_transform(data_Y_2d)\n",
    "\n",
    "print(scaled_X[:5])\n",
    "print(scaled_Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping `X`: 2D to 3D, (Samples, Timestep-Sequence, Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scaled_X) - xlen - foresight - ylen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (94, 4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 0.01010101],\n",
       "        [ 0.02020202],\n",
       "        [ 0.03030303]],\n",
       "\n",
       "       [[ 0.01010101],\n",
       "        [ 0.02020202],\n",
       "        [ 0.03030303],\n",
       "        [ 0.04040404]],\n",
       "\n",
       "       [[ 0.02020202],\n",
       "        [ 0.03030303],\n",
       "        [ 0.04040404],\n",
       "        [ 0.05050505]],\n",
       "\n",
       "       [[ 0.03030303],\n",
       "        [ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060606]],\n",
       "\n",
       "       [[ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060606],\n",
       "        [ 0.07070707]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_X = np.array([scaled_X[i:i+xlen] for i in range(0, len(scaled_X) - xlen - (foresight - 1) - (ylen - 1))])\n",
    "print('X Shape:', seq_X.shape)\n",
    "seq_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Shape: (94, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607]],\n",
       "\n",
       "       [[ 0.05050505],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708]],\n",
       "\n",
       "       [[ 0.06060607],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809]],\n",
       "\n",
       "       [[ 0.07070708],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909]],\n",
       "\n",
       "       [[ 0.08080809],\n",
       "        [ 0.09090909],\n",
       "        [ 0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_Y = np.array([scaled_Y[i:i+ylen] for i in range(xlen + (foresight - 1), len(scaled_Y) - (ylen - 1))])\n",
    "print('Y Shape:', seq_Y.shape)\n",
    "seq_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(xlen, ylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_X = pad_sequences(seq_X, maxlen=max_len, dtype='float32', padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909],\n",
       "        [ 0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq_Y = pad_sequences(seq_Y, maxlen=max_len, dtype='float32', padding='pre')\n",
    "padded_seq_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start & End Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 3, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_Y  = pad_sequences(seq_Y, maxlen=1+ylen, dtype='float32', padding='pre', value=0.)\n",
    "decoder_input_Y  = pad_sequences(decoder_input_Y, maxlen=max_len, dtype='float32', padding='post', value=0.)\n",
    "decoder_target_Y = pad_sequences(seq_Y, maxlen=max_len, dtype='float32', padding='post', value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909],\n",
       "        [ 0.10101011]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04040404],\n",
       "        [ 0.05050505],\n",
       "        [ 0.06060607],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.05050505],\n",
       "        [ 0.06060607],\n",
       "        [ 0.07070708],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.06060607],\n",
       "        [ 0.07070708],\n",
       "        [ 0.08080809],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.07070708],\n",
       "        [ 0.08080809],\n",
       "        [ 0.09090909],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.08080809],\n",
       "        [ 0.09090909],\n",
       "        [ 0.10101011],\n",
       "        [ 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting (Train & Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 4, 1) (65, 4, 1)\n",
      "Train_X\t: (65, 4, 1)\n",
      "Train_Y\t: (65, 4, 1)\n",
      "Test_X\t: (29, 4, 1)\n",
      "Test_Y\t: (29, 4, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = .3\n",
    "\n",
    "(train_X, test_X,\n",
    " train_Y, test_Y) = train_test_split(seq_X, padded_seq_Y,\n",
    "                                    test_size=test_size,\n",
    "                                    shuffle=False,\n",
    "                                    random_state=99)\n",
    "\n",
    "(train_decoder_input_Y,\n",
    " test_decoder_input_Y,\n",
    " train_decoder_target_Y,\n",
    " test_decoder_target_Y) = train_test_split(decoder_input_Y,\n",
    "                                           decoder_target_Y,\n",
    "                                           test_size=test_size,\n",
    "                                           shuffle=False,\n",
    "                                           random_state=99)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print('Train_X\\t: %s\\nTrain_Y\\t: %s\\nTest_X\\t: %s\\nTest_Y\\t: %s\\n' % \n",
    "      (train_X.shape, train_Y.shape, test_X.shape, test_Y.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# apt-get install -y graphviz libgraphviz-dev\n",
    "keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder-Decoder 1 (RepeatVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_seq_to_seq_1'](lstm_encdec_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_seq_to_seq_1'](seq2seq_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 16)                1152      \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 4, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 4, 16)             2112      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4, 1)              17        \n",
      "=================================================================\n",
      "Total params: 3,281\n",
      "Trainable params: 3,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "# simple lstm network learning\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "2D: (batch_size, units)\n",
    "3D: (batch_size, timesteps, input_dim)\n",
    "\"\"\"\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=False,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "model.add(RepeatVector(timestepX))\n",
    "\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=True,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "# TimeDistributed to compare the predicted with the real one, sequence by sequence\n",
    "model.add(TimeDistributed(Dense(ndimY,  # Network Node\n",
    "                                input_shape=(ylen, ndimY),\n",
    "                                activation='linear'),))\n",
    "\n",
    "#model.add(Dense(ndimY,  # Network Node\n",
    "#                input_shape=(ylen, ndimY),  # Time-step, Feature Number\n",
    "#                activation='linear',\n",
    "#                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),\n",
    "#                bias_initializer=keras.initializers.Constant(value=0),))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/300\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0779 - mean_absolute_error: 0.2165 - val_loss: 0.2784 - val_mean_absolute_error: 0.4595\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0751 - mean_absolute_error: 0.2124 - val_loss: 0.2717 - val_mean_absolute_error: 0.4549\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0720 - mean_absolute_error: 0.2078 - val_loss: 0.2645 - val_mean_absolute_error: 0.4500\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0688 - mean_absolute_error: 0.2029 - val_loss: 0.2570 - val_mean_absolute_error: 0.4448\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0655 - mean_absolute_error: 0.1978 - val_loss: 0.2493 - val_mean_absolute_error: 0.4392\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0622 - mean_absolute_error: 0.1926 - val_loss: 0.2412 - val_mean_absolute_error: 0.4334\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0589 - mean_absolute_error: 0.1873 - val_loss: 0.2329 - val_mean_absolute_error: 0.4272\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0555 - mean_absolute_error: 0.1821 - val_loss: 0.2243 - val_mean_absolute_error: 0.4206\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0522 - mean_absolute_error: 0.1768 - val_loss: 0.2153 - val_mean_absolute_error: 0.4136\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0489 - mean_absolute_error: 0.1715 - val_loss: 0.2060 - val_mean_absolute_error: 0.4060\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0455 - mean_absolute_error: 0.1662 - val_loss: 0.1964 - val_mean_absolute_error: 0.3980\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0422 - mean_absolute_error: 0.1610 - val_loss: 0.1864 - val_mean_absolute_error: 0.3894\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0390 - mean_absolute_error: 0.1558 - val_loss: 0.1761 - val_mean_absolute_error: 0.3802\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0358 - mean_absolute_error: 0.1508 - val_loss: 0.1654 - val_mean_absolute_error: 0.3703\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0328 - mean_absolute_error: 0.1459 - val_loss: 0.1545 - val_mean_absolute_error: 0.3597\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0300 - mean_absolute_error: 0.1413 - val_loss: 0.1435 - val_mean_absolute_error: 0.3484\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0274 - mean_absolute_error: 0.1371 - val_loss: 0.1324 - val_mean_absolute_error: 0.3365\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0252 - mean_absolute_error: 0.1334 - val_loss: 0.1213 - val_mean_absolute_error: 0.3239\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0233 - mean_absolute_error: 0.1304 - val_loss: 0.1106 - val_mean_absolute_error: 0.3110\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0219 - mean_absolute_error: 0.1282 - val_loss: 0.1005 - val_mean_absolute_error: 0.2979\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0210 - mean_absolute_error: 0.1268 - val_loss: 0.0911 - val_mean_absolute_error: 0.2848\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0206 - mean_absolute_error: 0.1264 - val_loss: 0.0827 - val_mean_absolute_error: 0.2723\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0207 - mean_absolute_error: 0.1269 - val_loss: 0.0755 - val_mean_absolute_error: 0.2609\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0211 - mean_absolute_error: 0.1282 - val_loss: 0.0698 - val_mean_absolute_error: 0.2509\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0217 - mean_absolute_error: 0.1300 - val_loss: 0.0654 - val_mean_absolute_error: 0.2429\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0224 - mean_absolute_error: 0.1317 - val_loss: 0.0623 - val_mean_absolute_error: 0.2371\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0229 - mean_absolute_error: 0.1331 - val_loss: 0.0605 - val_mean_absolute_error: 0.2334\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0232 - mean_absolute_error: 0.1339 - val_loss: 0.0597 - val_mean_absolute_error: 0.2318\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0233 - mean_absolute_error: 0.1342 - val_loss: 0.0597 - val_mean_absolute_error: 0.2319\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0232 - mean_absolute_error: 0.1339 - val_loss: 0.0603 - val_mean_absolute_error: 0.2332\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0229 - mean_absolute_error: 0.1333 - val_loss: 0.0614 - val_mean_absolute_error: 0.2355\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0226 - mean_absolute_error: 0.1324 - val_loss: 0.0628 - val_mean_absolute_error: 0.2382\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0222 - mean_absolute_error: 0.1314 - val_loss: 0.0643 - val_mean_absolute_error: 0.2410\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0218 - mean_absolute_error: 0.1304 - val_loss: 0.0657 - val_mean_absolute_error: 0.2438\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0215 - mean_absolute_error: 0.1295 - val_loss: 0.0671 - val_mean_absolute_error: 0.2462\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 448us/step - loss: 0.0212 - mean_absolute_error: 0.1287 - val_loss: 0.0682 - val_mean_absolute_error: 0.2483\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 415us/step - loss: 0.0209 - mean_absolute_error: 0.1279 - val_loss: 0.0691 - val_mean_absolute_error: 0.2499\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 409us/step - loss: 0.0207 - mean_absolute_error: 0.1272 - val_loss: 0.0697 - val_mean_absolute_error: 0.2510\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.0205 - mean_absolute_error: 0.1266 - val_loss: 0.0700 - val_mean_absolute_error: 0.2515\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.0203 - mean_absolute_error: 0.1261 - val_loss: 0.0701 - val_mean_absolute_error: 0.2517\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0202 - mean_absolute_error: 0.1257 - val_loss: 0.0699 - val_mean_absolute_error: 0.2514\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 410us/step - loss: 0.0201 - mean_absolute_error: 0.1252 - val_loss: 0.0695 - val_mean_absolute_error: 0.2507\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0200 - mean_absolute_error: 0.1249 - val_loss: 0.0690 - val_mean_absolute_error: 0.2497\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0199 - mean_absolute_error: 0.1246 - val_loss: 0.0683 - val_mean_absolute_error: 0.2485\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0198 - mean_absolute_error: 0.1243 - val_loss: 0.0675 - val_mean_absolute_error: 0.2471\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0197 - mean_absolute_error: 0.1240 - val_loss: 0.0667 - val_mean_absolute_error: 0.2455\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0196 - mean_absolute_error: 0.1238 - val_loss: 0.0658 - val_mean_absolute_error: 0.2439\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0196 - mean_absolute_error: 0.1235 - val_loss: 0.0650 - val_mean_absolute_error: 0.2423\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0195 - mean_absolute_error: 0.1233 - val_loss: 0.0641 - val_mean_absolute_error: 0.2407\n",
      "Epoch 50/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 398us/step - loss: 0.0194 - mean_absolute_error: 0.1231 - val_loss: 0.0633 - val_mean_absolute_error: 0.2391\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0194 - mean_absolute_error: 0.1228 - val_loss: 0.0626 - val_mean_absolute_error: 0.2377\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 410us/step - loss: 0.0193 - mean_absolute_error: 0.1226 - val_loss: 0.0619 - val_mean_absolute_error: 0.2363\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0192 - mean_absolute_error: 0.1223 - val_loss: 0.0613 - val_mean_absolute_error: 0.2351\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0191 - mean_absolute_error: 0.1220 - val_loss: 0.0607 - val_mean_absolute_error: 0.2340\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0190 - mean_absolute_error: 0.1217 - val_loss: 0.0602 - val_mean_absolute_error: 0.2330\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0189 - mean_absolute_error: 0.1214 - val_loss: 0.0598 - val_mean_absolute_error: 0.2321\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0188 - mean_absolute_error: 0.1210 - val_loss: 0.0594 - val_mean_absolute_error: 0.2312\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0187 - mean_absolute_error: 0.1207 - val_loss: 0.0590 - val_mean_absolute_error: 0.2304\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0186 - mean_absolute_error: 0.1203 - val_loss: 0.0586 - val_mean_absolute_error: 0.2297\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0185 - mean_absolute_error: 0.1199 - val_loss: 0.0583 - val_mean_absolute_error: 0.2289\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0184 - mean_absolute_error: 0.1195 - val_loss: 0.0579 - val_mean_absolute_error: 0.2282\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 402us/step - loss: 0.0183 - mean_absolute_error: 0.1191 - val_loss: 0.0576 - val_mean_absolute_error: 0.2274\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0181 - mean_absolute_error: 0.1186 - val_loss: 0.0572 - val_mean_absolute_error: 0.2266\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0180 - mean_absolute_error: 0.1182 - val_loss: 0.0568 - val_mean_absolute_error: 0.2258\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0179 - mean_absolute_error: 0.1178 - val_loss: 0.0564 - val_mean_absolute_error: 0.2249\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0178 - mean_absolute_error: 0.1174 - val_loss: 0.0560 - val_mean_absolute_error: 0.2240\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0177 - mean_absolute_error: 0.1169 - val_loss: 0.0555 - val_mean_absolute_error: 0.2231\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0175 - mean_absolute_error: 0.1165 - val_loss: 0.0551 - val_mean_absolute_error: 0.2220\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0174 - mean_absolute_error: 0.1161 - val_loss: 0.0546 - val_mean_absolute_error: 0.2210\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0173 - mean_absolute_error: 0.1157 - val_loss: 0.0541 - val_mean_absolute_error: 0.2199\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0172 - mean_absolute_error: 0.1152 - val_loss: 0.0536 - val_mean_absolute_error: 0.2188\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0171 - mean_absolute_error: 0.1148 - val_loss: 0.0531 - val_mean_absolute_error: 0.2176\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0170 - mean_absolute_error: 0.1144 - val_loss: 0.0526 - val_mean_absolute_error: 0.2165\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0168 - mean_absolute_error: 0.1140 - val_loss: 0.0521 - val_mean_absolute_error: 0.2153\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.0167 - mean_absolute_error: 0.1135 - val_loss: 0.0516 - val_mean_absolute_error: 0.2141\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0166 - mean_absolute_error: 0.1131 - val_loss: 0.0510 - val_mean_absolute_error: 0.2129\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 417us/step - loss: 0.0165 - mean_absolute_error: 0.1126 - val_loss: 0.0505 - val_mean_absolute_error: 0.2117\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0164 - mean_absolute_error: 0.1122 - val_loss: 0.0500 - val_mean_absolute_error: 0.2105\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0162 - mean_absolute_error: 0.1117 - val_loss: 0.0495 - val_mean_absolute_error: 0.2093\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0161 - mean_absolute_error: 0.1112 - val_loss: 0.0491 - val_mean_absolute_error: 0.2081\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0160 - mean_absolute_error: 0.1108 - val_loss: 0.0486 - val_mean_absolute_error: 0.2069\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0159 - mean_absolute_error: 0.1103 - val_loss: 0.0481 - val_mean_absolute_error: 0.2057\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 407us/step - loss: 0.0157 - mean_absolute_error: 0.1098 - val_loss: 0.0476 - val_mean_absolute_error: 0.2044\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0156 - mean_absolute_error: 0.1093 - val_loss: 0.0471 - val_mean_absolute_error: 0.2032\n",
      "Epoch 85/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0155 - mean_absolute_error: 0.1088 - val_loss: 0.0466 - val_mean_absolute_error: 0.2020\n",
      "Epoch 86/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0153 - mean_absolute_error: 0.1083 - val_loss: 0.0462 - val_mean_absolute_error: 0.2007\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0152 - mean_absolute_error: 0.1078 - val_loss: 0.0457 - val_mean_absolute_error: 0.1995\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0151 - mean_absolute_error: 0.1073 - val_loss: 0.0452 - val_mean_absolute_error: 0.1982\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0149 - mean_absolute_error: 0.1067 - val_loss: 0.0447 - val_mean_absolute_error: 0.1969\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0148 - mean_absolute_error: 0.1062 - val_loss: 0.0442 - val_mean_absolute_error: 0.1956\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0147 - mean_absolute_error: 0.1057 - val_loss: 0.0438 - val_mean_absolute_error: 0.1942\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0146 - mean_absolute_error: 0.1052 - val_loss: 0.0433 - val_mean_absolute_error: 0.1929\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0144 - mean_absolute_error: 0.1047 - val_loss: 0.0428 - val_mean_absolute_error: 0.1915\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0143 - mean_absolute_error: 0.1041 - val_loss: 0.0423 - val_mean_absolute_error: 0.1901\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0142 - mean_absolute_error: 0.1036 - val_loss: 0.0418 - val_mean_absolute_error: 0.1888\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0140 - mean_absolute_error: 0.1030 - val_loss: 0.0414 - val_mean_absolute_error: 0.1873\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0139 - mean_absolute_error: 0.1025 - val_loss: 0.0409 - val_mean_absolute_error: 0.1859\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0138 - mean_absolute_error: 0.1019 - val_loss: 0.0404 - val_mean_absolute_error: 0.1845\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 394us/step - loss: 0.0136 - mean_absolute_error: 0.1014 - val_loss: 0.0400 - val_mean_absolute_error: 0.1830\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0135 - mean_absolute_error: 0.1008 - val_loss: 0.0395 - val_mean_absolute_error: 0.1816\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0134 - mean_absolute_error: 0.1002 - val_loss: 0.0390 - val_mean_absolute_error: 0.1801\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0132 - mean_absolute_error: 0.0997 - val_loss: 0.0386 - val_mean_absolute_error: 0.1787\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0131 - mean_absolute_error: 0.0991 - val_loss: 0.0381 - val_mean_absolute_error: 0.1772\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0130 - mean_absolute_error: 0.0985 - val_loss: 0.0377 - val_mean_absolute_error: 0.1757\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0128 - mean_absolute_error: 0.0980 - val_loss: 0.0373 - val_mean_absolute_error: 0.1742\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0127 - mean_absolute_error: 0.0974 - val_loss: 0.0368 - val_mean_absolute_error: 0.1727\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0126 - mean_absolute_error: 0.0968 - val_loss: 0.0364 - val_mean_absolute_error: 0.1712\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0124 - mean_absolute_error: 0.0963 - val_loss: 0.0360 - val_mean_absolute_error: 0.1696\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0123 - mean_absolute_error: 0.0957 - val_loss: 0.0356 - val_mean_absolute_error: 0.1681\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0122 - mean_absolute_error: 0.0951 - val_loss: 0.0351 - val_mean_absolute_error: 0.1665\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0121 - mean_absolute_error: 0.0945 - val_loss: 0.0347 - val_mean_absolute_error: 0.1650\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0119 - mean_absolute_error: 0.0939 - val_loss: 0.0343 - val_mean_absolute_error: 0.1634\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0118 - mean_absolute_error: 0.0934 - val_loss: 0.0340 - val_mean_absolute_error: 0.1618\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0117 - mean_absolute_error: 0.0928 - val_loss: 0.0336 - val_mean_absolute_error: 0.1603\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0115 - mean_absolute_error: 0.0922 - val_loss: 0.0332 - val_mean_absolute_error: 0.1587\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0114 - mean_absolute_error: 0.0917 - val_loss: 0.0328 - val_mean_absolute_error: 0.1571\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0113 - mean_absolute_error: 0.0911 - val_loss: 0.0325 - val_mean_absolute_error: 0.1555\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0112 - mean_absolute_error: 0.0906 - val_loss: 0.0321 - val_mean_absolute_error: 0.1539\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0111 - mean_absolute_error: 0.0900 - val_loss: 0.0318 - val_mean_absolute_error: 0.1523\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0109 - mean_absolute_error: 0.0895 - val_loss: 0.0314 - val_mean_absolute_error: 0.1507\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0108 - mean_absolute_error: 0.0890 - val_loss: 0.0311 - val_mean_absolute_error: 0.1490\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0107 - mean_absolute_error: 0.0885 - val_loss: 0.0308 - val_mean_absolute_error: 0.1474\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0106 - mean_absolute_error: 0.0880 - val_loss: 0.0305 - val_mean_absolute_error: 0.1459\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0105 - mean_absolute_error: 0.0876 - val_loss: 0.0302 - val_mean_absolute_error: 0.1445\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0104 - mean_absolute_error: 0.0871 - val_loss: 0.0299 - val_mean_absolute_error: 0.1435\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0102 - mean_absolute_error: 0.0866 - val_loss: 0.0296 - val_mean_absolute_error: 0.1433\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0101 - mean_absolute_error: 0.0862 - val_loss: 0.0293 - val_mean_absolute_error: 0.1432\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0100 - mean_absolute_error: 0.0857 - val_loss: 0.0290 - val_mean_absolute_error: 0.1432\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0099 - mean_absolute_error: 0.0853 - val_loss: 0.0288 - val_mean_absolute_error: 0.1431\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0098 - mean_absolute_error: 0.0848 - val_loss: 0.0285 - val_mean_absolute_error: 0.1430\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0097 - mean_absolute_error: 0.0844 - val_loss: 0.0283 - val_mean_absolute_error: 0.1430\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 426us/step - loss: 0.0096 - mean_absolute_error: 0.0839 - val_loss: 0.0281 - val_mean_absolute_error: 0.1429\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0095 - mean_absolute_error: 0.0834 - val_loss: 0.0279 - val_mean_absolute_error: 0.1428\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0094 - mean_absolute_error: 0.0830 - val_loss: 0.0276 - val_mean_absolute_error: 0.1427\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.0093 - mean_absolute_error: 0.0825 - val_loss: 0.0274 - val_mean_absolute_error: 0.1427\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0092 - mean_absolute_error: 0.0821 - val_loss: 0.0273 - val_mean_absolute_error: 0.1426\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0092 - mean_absolute_error: 0.0817 - val_loss: 0.0271 - val_mean_absolute_error: 0.1425\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0091 - mean_absolute_error: 0.0812 - val_loss: 0.0269 - val_mean_absolute_error: 0.1424\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0090 - mean_absolute_error: 0.0808 - val_loss: 0.0267 - val_mean_absolute_error: 0.1423\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0089 - mean_absolute_error: 0.0804 - val_loss: 0.0266 - val_mean_absolute_error: 0.1422\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0088 - mean_absolute_error: 0.0799 - val_loss: 0.0265 - val_mean_absolute_error: 0.1422\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 430us/step - loss: 0.0087 - mean_absolute_error: 0.0795 - val_loss: 0.0263 - val_mean_absolute_error: 0.1421\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0087 - mean_absolute_error: 0.0791 - val_loss: 0.0262 - val_mean_absolute_error: 0.1420\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0086 - mean_absolute_error: 0.0787 - val_loss: 0.0261 - val_mean_absolute_error: 0.1419\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0085 - mean_absolute_error: 0.0783 - val_loss: 0.0260 - val_mean_absolute_error: 0.1418\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0084 - mean_absolute_error: 0.0779 - val_loss: 0.0259 - val_mean_absolute_error: 0.1417\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0084 - mean_absolute_error: 0.0775 - val_loss: 0.0258 - val_mean_absolute_error: 0.1416\n",
      "Epoch 148/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 402us/step - loss: 0.0083 - mean_absolute_error: 0.0771 - val_loss: 0.0257 - val_mean_absolute_error: 0.1415\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 418us/step - loss: 0.0082 - mean_absolute_error: 0.0767 - val_loss: 0.0256 - val_mean_absolute_error: 0.1414\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0082 - mean_absolute_error: 0.0763 - val_loss: 0.0256 - val_mean_absolute_error: 0.1413\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0081 - mean_absolute_error: 0.0759 - val_loss: 0.0255 - val_mean_absolute_error: 0.1412\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0081 - mean_absolute_error: 0.0756 - val_loss: 0.0255 - val_mean_absolute_error: 0.1411\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0080 - mean_absolute_error: 0.0752 - val_loss: 0.0254 - val_mean_absolute_error: 0.1410\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0080 - mean_absolute_error: 0.0749 - val_loss: 0.0254 - val_mean_absolute_error: 0.1408\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0079 - mean_absolute_error: 0.0745 - val_loss: 0.0253 - val_mean_absolute_error: 0.1407\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0078 - mean_absolute_error: 0.0742 - val_loss: 0.0253 - val_mean_absolute_error: 0.1406\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 440us/step - loss: 0.0078 - mean_absolute_error: 0.0738 - val_loss: 0.0253 - val_mean_absolute_error: 0.1405\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0078 - mean_absolute_error: 0.0735 - val_loss: 0.0253 - val_mean_absolute_error: 0.1404\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0077 - mean_absolute_error: 0.0732 - val_loss: 0.0253 - val_mean_absolute_error: 0.1402\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 407us/step - loss: 0.0077 - mean_absolute_error: 0.0729 - val_loss: 0.0253 - val_mean_absolute_error: 0.1401\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0076 - mean_absolute_error: 0.0726 - val_loss: 0.0252 - val_mean_absolute_error: 0.1400\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0076 - mean_absolute_error: 0.0723 - val_loss: 0.0252 - val_mean_absolute_error: 0.1399\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 409us/step - loss: 0.0075 - mean_absolute_error: 0.0720 - val_loss: 0.0253 - val_mean_absolute_error: 0.1397\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0075 - mean_absolute_error: 0.0717 - val_loss: 0.0253 - val_mean_absolute_error: 0.1396\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0075 - mean_absolute_error: 0.0714 - val_loss: 0.0253 - val_mean_absolute_error: 0.1394\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0074 - mean_absolute_error: 0.0712 - val_loss: 0.0253 - val_mean_absolute_error: 0.1393\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0074 - mean_absolute_error: 0.0709 - val_loss: 0.0253 - val_mean_absolute_error: 0.1392\n",
      "Epoch 168/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0074 - mean_absolute_error: 0.0707 - val_loss: 0.0253 - val_mean_absolute_error: 0.1390\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0073 - mean_absolute_error: 0.0704 - val_loss: 0.0253 - val_mean_absolute_error: 0.1389\n",
      "Epoch 170/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0073 - mean_absolute_error: 0.0702 - val_loss: 0.0254 - val_mean_absolute_error: 0.1387\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0073 - mean_absolute_error: 0.0699 - val_loss: 0.0254 - val_mean_absolute_error: 0.1386\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0073 - mean_absolute_error: 0.0697 - val_loss: 0.0254 - val_mean_absolute_error: 0.1384\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0072 - mean_absolute_error: 0.0695 - val_loss: 0.0254 - val_mean_absolute_error: 0.1383\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0072 - mean_absolute_error: 0.0693 - val_loss: 0.0254 - val_mean_absolute_error: 0.1381\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0072 - mean_absolute_error: 0.0690 - val_loss: 0.0255 - val_mean_absolute_error: 0.1380\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0072 - mean_absolute_error: 0.0688 - val_loss: 0.0255 - val_mean_absolute_error: 0.1378\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0071 - mean_absolute_error: 0.0686 - val_loss: 0.0255 - val_mean_absolute_error: 0.1377\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0071 - mean_absolute_error: 0.0684 - val_loss: 0.0255 - val_mean_absolute_error: 0.1376\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0071 - mean_absolute_error: 0.0683 - val_loss: 0.0256 - val_mean_absolute_error: 0.1376\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0071 - mean_absolute_error: 0.0681 - val_loss: 0.0256 - val_mean_absolute_error: 0.1375\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0071 - mean_absolute_error: 0.0679 - val_loss: 0.0256 - val_mean_absolute_error: 0.1375\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0071 - mean_absolute_error: 0.0677 - val_loss: 0.0257 - val_mean_absolute_error: 0.1375\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0070 - mean_absolute_error: 0.0676 - val_loss: 0.0257 - val_mean_absolute_error: 0.1375\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0070 - mean_absolute_error: 0.0674 - val_loss: 0.0257 - val_mean_absolute_error: 0.1375\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0070 - mean_absolute_error: 0.0672 - val_loss: 0.0257 - val_mean_absolute_error: 0.1375\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 407us/step - loss: 0.0070 - mean_absolute_error: 0.0671 - val_loss: 0.0257 - val_mean_absolute_error: 0.1375\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0070 - mean_absolute_error: 0.0669 - val_loss: 0.0258 - val_mean_absolute_error: 0.1375\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0070 - mean_absolute_error: 0.0668 - val_loss: 0.0258 - val_mean_absolute_error: 0.1375\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0069 - mean_absolute_error: 0.0666 - val_loss: 0.0258 - val_mean_absolute_error: 0.1376\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0069 - mean_absolute_error: 0.0665 - val_loss: 0.0258 - val_mean_absolute_error: 0.1376\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0069 - mean_absolute_error: 0.0664 - val_loss: 0.0258 - val_mean_absolute_error: 0.1376\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0069 - mean_absolute_error: 0.0662 - val_loss: 0.0259 - val_mean_absolute_error: 0.1377\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0069 - mean_absolute_error: 0.0661 - val_loss: 0.0259 - val_mean_absolute_error: 0.1377\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0069 - mean_absolute_error: 0.0660 - val_loss: 0.0259 - val_mean_absolute_error: 0.1377\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0069 - mean_absolute_error: 0.0658 - val_loss: 0.0259 - val_mean_absolute_error: 0.1377\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0069 - mean_absolute_error: 0.0657 - val_loss: 0.0259 - val_mean_absolute_error: 0.1378\n",
      "Epoch 197/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 380us/step - loss: 0.0068 - mean_absolute_error: 0.0656 - val_loss: 0.0259 - val_mean_absolute_error: 0.1378\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 402us/step - loss: 0.0068 - mean_absolute_error: 0.0655 - val_loss: 0.0259 - val_mean_absolute_error: 0.1378\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0068 - mean_absolute_error: 0.0654 - val_loss: 0.0259 - val_mean_absolute_error: 0.1378\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0068 - mean_absolute_error: 0.0653 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0068 - mean_absolute_error: 0.0652 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0068 - mean_absolute_error: 0.0651 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0068 - mean_absolute_error: 0.0649 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0068 - mean_absolute_error: 0.0648 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0068 - mean_absolute_error: 0.0648 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0068 - mean_absolute_error: 0.0647 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0068 - mean_absolute_error: 0.0646 - val_loss: 0.0260 - val_mean_absolute_error: 0.1379\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 389us/step - loss: 0.0067 - mean_absolute_error: 0.0645 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 392us/step - loss: 0.0067 - mean_absolute_error: 0.0644 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0067 - mean_absolute_error: 0.0643 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0067 - mean_absolute_error: 0.0642 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0067 - mean_absolute_error: 0.0641 - val_loss: 0.0260 - val_mean_absolute_error: 0.1378\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0067 - mean_absolute_error: 0.0640 - val_loss: 0.0260 - val_mean_absolute_error: 0.1377\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0067 - mean_absolute_error: 0.0640 - val_loss: 0.0260 - val_mean_absolute_error: 0.1377\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0067 - mean_absolute_error: 0.0639 - val_loss: 0.0260 - val_mean_absolute_error: 0.1377\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0067 - mean_absolute_error: 0.0638 - val_loss: 0.0259 - val_mean_absolute_error: 0.1376\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0067 - mean_absolute_error: 0.0637 - val_loss: 0.0259 - val_mean_absolute_error: 0.1376\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0067 - mean_absolute_error: 0.0636 - val_loss: 0.0259 - val_mean_absolute_error: 0.1375\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0067 - mean_absolute_error: 0.0636 - val_loss: 0.0259 - val_mean_absolute_error: 0.1375\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0067 - mean_absolute_error: 0.0635 - val_loss: 0.0259 - val_mean_absolute_error: 0.1374\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0066 - mean_absolute_error: 0.0634 - val_loss: 0.0259 - val_mean_absolute_error: 0.1374\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0066 - mean_absolute_error: 0.0633 - val_loss: 0.0259 - val_mean_absolute_error: 0.1373\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0066 - mean_absolute_error: 0.0633 - val_loss: 0.0259 - val_mean_absolute_error: 0.1373\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0066 - mean_absolute_error: 0.0632 - val_loss: 0.0259 - val_mean_absolute_error: 0.1372\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0066 - mean_absolute_error: 0.0631 - val_loss: 0.0259 - val_mean_absolute_error: 0.1372\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0066 - mean_absolute_error: 0.0631 - val_loss: 0.0258 - val_mean_absolute_error: 0.1371\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0066 - mean_absolute_error: 0.0630 - val_loss: 0.0258 - val_mean_absolute_error: 0.1370\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 409us/step - loss: 0.0066 - mean_absolute_error: 0.0629 - val_loss: 0.0258 - val_mean_absolute_error: 0.1370\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0066 - mean_absolute_error: 0.0629 - val_loss: 0.0258 - val_mean_absolute_error: 0.1369\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0066 - mean_absolute_error: 0.0628 - val_loss: 0.0258 - val_mean_absolute_error: 0.1368\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0066 - mean_absolute_error: 0.0627 - val_loss: 0.0258 - val_mean_absolute_error: 0.1368\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0066 - mean_absolute_error: 0.0627 - val_loss: 0.0258 - val_mean_absolute_error: 0.1367\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0066 - mean_absolute_error: 0.0626 - val_loss: 0.0257 - val_mean_absolute_error: 0.1366\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0066 - mean_absolute_error: 0.0625 - val_loss: 0.0257 - val_mean_absolute_error: 0.1365\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0065 - mean_absolute_error: 0.0625 - val_loss: 0.0257 - val_mean_absolute_error: 0.1365\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0065 - mean_absolute_error: 0.0624 - val_loss: 0.0257 - val_mean_absolute_error: 0.1364\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0065 - mean_absolute_error: 0.0624 - val_loss: 0.0257 - val_mean_absolute_error: 0.1363\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0065 - mean_absolute_error: 0.0623 - val_loss: 0.0256 - val_mean_absolute_error: 0.1362\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 415us/step - loss: 0.0065 - mean_absolute_error: 0.0623 - val_loss: 0.0256 - val_mean_absolute_error: 0.1361\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 409us/step - loss: 0.0065 - mean_absolute_error: 0.0622 - val_loss: 0.0256 - val_mean_absolute_error: 0.1361\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0065 - mean_absolute_error: 0.0621 - val_loss: 0.0256 - val_mean_absolute_error: 0.1360\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0065 - mean_absolute_error: 0.0621 - val_loss: 0.0256 - val_mean_absolute_error: 0.1359\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0065 - mean_absolute_error: 0.0620 - val_loss: 0.0256 - val_mean_absolute_error: 0.1358\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0065 - mean_absolute_error: 0.0620 - val_loss: 0.0255 - val_mean_absolute_error: 0.1357\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0065 - mean_absolute_error: 0.0619 - val_loss: 0.0255 - val_mean_absolute_error: 0.1356\n",
      "Epoch 246/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 398us/step - loss: 0.0065 - mean_absolute_error: 0.0619 - val_loss: 0.0255 - val_mean_absolute_error: 0.1355\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0065 - mean_absolute_error: 0.0618 - val_loss: 0.0255 - val_mean_absolute_error: 0.1354\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0065 - mean_absolute_error: 0.0618 - val_loss: 0.0255 - val_mean_absolute_error: 0.1354\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0065 - mean_absolute_error: 0.0617 - val_loss: 0.0254 - val_mean_absolute_error: 0.1353\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0064 - mean_absolute_error: 0.0616 - val_loss: 0.0254 - val_mean_absolute_error: 0.1352\n",
      "Epoch 251/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0064 - mean_absolute_error: 0.0616 - val_loss: 0.0254 - val_mean_absolute_error: 0.1351\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0064 - mean_absolute_error: 0.0615 - val_loss: 0.0254 - val_mean_absolute_error: 0.1350\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0064 - mean_absolute_error: 0.0615 - val_loss: 0.0254 - val_mean_absolute_error: 0.1349\n",
      "Epoch 254/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0064 - mean_absolute_error: 0.0614 - val_loss: 0.0253 - val_mean_absolute_error: 0.1348\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0064 - mean_absolute_error: 0.0614 - val_loss: 0.0253 - val_mean_absolute_error: 0.1347\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0064 - mean_absolute_error: 0.0613 - val_loss: 0.0253 - val_mean_absolute_error: 0.1346\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0064 - mean_absolute_error: 0.0613 - val_loss: 0.0253 - val_mean_absolute_error: 0.1345\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0064 - mean_absolute_error: 0.0612 - val_loss: 0.0252 - val_mean_absolute_error: 0.1344\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0064 - mean_absolute_error: 0.0612 - val_loss: 0.0252 - val_mean_absolute_error: 0.1343\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0064 - mean_absolute_error: 0.0611 - val_loss: 0.0252 - val_mean_absolute_error: 0.1342\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0064 - mean_absolute_error: 0.0611 - val_loss: 0.0252 - val_mean_absolute_error: 0.1341\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0064 - mean_absolute_error: 0.0610 - val_loss: 0.0252 - val_mean_absolute_error: 0.1340\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0064 - mean_absolute_error: 0.0610 - val_loss: 0.0251 - val_mean_absolute_error: 0.1339\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0064 - mean_absolute_error: 0.0609 - val_loss: 0.0251 - val_mean_absolute_error: 0.1339\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 435us/step - loss: 0.0064 - mean_absolute_error: 0.0609 - val_loss: 0.0251 - val_mean_absolute_error: 0.1338\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0063 - mean_absolute_error: 0.0608 - val_loss: 0.0251 - val_mean_absolute_error: 0.1337\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0063 - mean_absolute_error: 0.0608 - val_loss: 0.0250 - val_mean_absolute_error: 0.1336\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0063 - mean_absolute_error: 0.0608 - val_loss: 0.0250 - val_mean_absolute_error: 0.1335\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0063 - mean_absolute_error: 0.0607 - val_loss: 0.0250 - val_mean_absolute_error: 0.1334\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0063 - mean_absolute_error: 0.0607 - val_loss: 0.0250 - val_mean_absolute_error: 0.1333\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0063 - mean_absolute_error: 0.0606 - val_loss: 0.0249 - val_mean_absolute_error: 0.1332\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 377us/step - loss: 0.0063 - mean_absolute_error: 0.0606 - val_loss: 0.0249 - val_mean_absolute_error: 0.1331\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0063 - mean_absolute_error: 0.0605 - val_loss: 0.0249 - val_mean_absolute_error: 0.1330\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 387us/step - loss: 0.0063 - mean_absolute_error: 0.0605 - val_loss: 0.0249 - val_mean_absolute_error: 0.1329\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0063 - mean_absolute_error: 0.0604 - val_loss: 0.0248 - val_mean_absolute_error: 0.1328\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 406us/step - loss: 0.0063 - mean_absolute_error: 0.0604 - val_loss: 0.0248 - val_mean_absolute_error: 0.1327\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 433us/step - loss: 0.0063 - mean_absolute_error: 0.0603 - val_loss: 0.0248 - val_mean_absolute_error: 0.1326\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0063 - mean_absolute_error: 0.0603 - val_loss: 0.0248 - val_mean_absolute_error: 0.1325\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0063 - mean_absolute_error: 0.0602 - val_loss: 0.0248 - val_mean_absolute_error: 0.1324\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0063 - mean_absolute_error: 0.0602 - val_loss: 0.0247 - val_mean_absolute_error: 0.1323\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0062 - mean_absolute_error: 0.0601 - val_loss: 0.0247 - val_mean_absolute_error: 0.1322\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 407us/step - loss: 0.0062 - mean_absolute_error: 0.0601 - val_loss: 0.0247 - val_mean_absolute_error: 0.1321\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0062 - mean_absolute_error: 0.0601 - val_loss: 0.0247 - val_mean_absolute_error: 0.1320\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0062 - mean_absolute_error: 0.0600 - val_loss: 0.0246 - val_mean_absolute_error: 0.1319\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0062 - mean_absolute_error: 0.0600 - val_loss: 0.0246 - val_mean_absolute_error: 0.1318\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 386us/step - loss: 0.0062 - mean_absolute_error: 0.0599 - val_loss: 0.0246 - val_mean_absolute_error: 0.1316\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0062 - mean_absolute_error: 0.0599 - val_loss: 0.0246 - val_mean_absolute_error: 0.1315\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 381us/step - loss: 0.0062 - mean_absolute_error: 0.0598 - val_loss: 0.0245 - val_mean_absolute_error: 0.1314\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 383us/step - loss: 0.0062 - mean_absolute_error: 0.0598 - val_loss: 0.0245 - val_mean_absolute_error: 0.1313\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0062 - mean_absolute_error: 0.0597 - val_loss: 0.0245 - val_mean_absolute_error: 0.1312\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0062 - mean_absolute_error: 0.0597 - val_loss: 0.0244 - val_mean_absolute_error: 0.1311\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0062 - mean_absolute_error: 0.0596 - val_loss: 0.0244 - val_mean_absolute_error: 0.1310\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0062 - mean_absolute_error: 0.0596 - val_loss: 0.0244 - val_mean_absolute_error: 0.1309\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 422us/step - loss: 0.0062 - mean_absolute_error: 0.0596 - val_loss: 0.0244 - val_mean_absolute_error: 0.1308\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 383us/step - loss: 0.0062 - mean_absolute_error: 0.0595 - val_loss: 0.0243 - val_mean_absolute_error: 0.1307\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0061 - mean_absolute_error: 0.0595 - val_loss: 0.0243 - val_mean_absolute_error: 0.1306\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0061 - mean_absolute_error: 0.0594 - val_loss: 0.0243 - val_mean_absolute_error: 0.1305\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 388us/step - loss: 0.0061 - mean_absolute_error: 0.0594 - val_loss: 0.0243 - val_mean_absolute_error: 0.1304\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0061 - mean_absolute_error: 0.0593 - val_loss: 0.0242 - val_mean_absolute_error: 0.1303\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0061 - mean_absolute_error: 0.0593 - val_loss: 0.0242 - val_mean_absolute_error: 0.1302\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit(train_X, train_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAADFCAYAAAB5PKoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdab1e6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train mean of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.720 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict(train_X)\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 4.832 RMSE\n",
      "Real\t:\n",
      " [[  0.        ]\n",
      " [ 19.39999962]\n",
      " [ 19.60000038]\n",
      " [ 19.79999924]],\n",
      "Predict\t:\n",
      " [[  9.75561047]\n",
      " [ 18.03530121]\n",
      " [ 23.42274857]\n",
      " [ 26.57158852]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in test_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[-1], test_Y_hat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder-Decoder 2 (Seq2Seq) - It can be used for Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['lstm_many_to_many_1'](lstm_many_to_many_1.jpg)\n",
    "!['lstm_seq_to_seq_1'](lstm_encdec_2.png)\n",
    "!['lstm_seq_to_seq_1'](seq2seq_1.png)\n",
    "!['Use of a summary state in the encoder-decoder architecture'](lstm_attention_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Embedding`:  \n",
    "> `(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 4, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 4, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_decoder_input_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, timestepX, ndimX = train_X.shape\n",
    "_, timestepY, ndimY = train_decoder_target_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "num_encoder_tokens = ndimX\n",
    "latent_dim = 4\n",
    "num_decoder_tokens = ndimY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Encoder_LSTM/kernel:0' shape=(1, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/recurrent_kernel:0' shape=(4, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Encoder_LSTM/bias:0' shape=(16,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/kernel:0' shape=(1, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/recurrent_kernel:0' shape=(4, 16) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_LSTM/bias:0' shape=(16,) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/kernel:0' shape=(4, 1) dtype=float32_ref>,\n",
      " <tf.Variable 'Decoder_Output/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Input (InputLayer)      (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Input (InputLayer)      (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (LSTM)             [(None, 4), (None, 4 96          Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)             [(None, None, 4), (N 96          Decoder_Input[0][0]              \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Output (Dense)          (None, None, 1)      5           Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 197\n",
      "Trainable params: 197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, ndimX), name='Encoder_Input')\n",
    "encoder = LSTM(latent_dim, return_state=True, name='Encoder_LSTM')\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='Decoder_Input')\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='sigmoid', name='Decoder_Output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "pprint(model.weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52 samples, validate on 13 samples\n",
      "Epoch 1/300\n",
      "52/52 [==============================] - 0s 423us/step - loss: 0.0311 - mean_absolute_error: 0.1569 - val_loss: 0.1015 - val_mean_absolute_error: 0.3040\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0309 - mean_absolute_error: 0.1565 - val_loss: 0.1015 - val_mean_absolute_error: 0.3040\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0308 - mean_absolute_error: 0.1560 - val_loss: 0.1016 - val_mean_absolute_error: 0.3039\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 413us/step - loss: 0.0306 - mean_absolute_error: 0.1556 - val_loss: 0.1016 - val_mean_absolute_error: 0.3038\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0305 - mean_absolute_error: 0.1552 - val_loss: 0.1016 - val_mean_absolute_error: 0.3037\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 395us/step - loss: 0.0303 - mean_absolute_error: 0.1548 - val_loss: 0.1016 - val_mean_absolute_error: 0.3036\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 405us/step - loss: 0.0302 - mean_absolute_error: 0.1544 - val_loss: 0.1015 - val_mean_absolute_error: 0.3034\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 399us/step - loss: 0.0300 - mean_absolute_error: 0.1540 - val_loss: 0.1015 - val_mean_absolute_error: 0.3033\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0299 - mean_absolute_error: 0.1536 - val_loss: 0.1014 - val_mean_absolute_error: 0.3031\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0297 - mean_absolute_error: 0.1532 - val_loss: 0.1013 - val_mean_absolute_error: 0.3029\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0296 - mean_absolute_error: 0.1528 - val_loss: 0.1012 - val_mean_absolute_error: 0.3026\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 435us/step - loss: 0.0295 - mean_absolute_error: 0.1525 - val_loss: 0.1011 - val_mean_absolute_error: 0.3024\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 417us/step - loss: 0.0293 - mean_absolute_error: 0.1521 - val_loss: 0.1010 - val_mean_absolute_error: 0.3021\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 418us/step - loss: 0.0292 - mean_absolute_error: 0.1518 - val_loss: 0.1008 - val_mean_absolute_error: 0.3018\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 410us/step - loss: 0.0291 - mean_absolute_error: 0.1514 - val_loss: 0.1007 - val_mean_absolute_error: 0.3015\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 419us/step - loss: 0.0290 - mean_absolute_error: 0.1511 - val_loss: 0.1005 - val_mean_absolute_error: 0.3012\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 394us/step - loss: 0.0288 - mean_absolute_error: 0.1507 - val_loss: 0.1003 - val_mean_absolute_error: 0.3009\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0287 - mean_absolute_error: 0.1504 - val_loss: 0.1001 - val_mean_absolute_error: 0.3005\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 400us/step - loss: 0.0286 - mean_absolute_error: 0.1500 - val_loss: 0.0999 - val_mean_absolute_error: 0.3002\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0285 - mean_absolute_error: 0.1497 - val_loss: 0.0997 - val_mean_absolute_error: 0.2998\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 421us/step - loss: 0.0284 - mean_absolute_error: 0.1494 - val_loss: 0.0995 - val_mean_absolute_error: 0.2994\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - 0s 413us/step - loss: 0.0282 - mean_absolute_error: 0.1490 - val_loss: 0.0992 - val_mean_absolute_error: 0.2990\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 409us/step - loss: 0.0281 - mean_absolute_error: 0.1487 - val_loss: 0.0990 - val_mean_absolute_error: 0.2986\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 397us/step - loss: 0.0280 - mean_absolute_error: 0.1484 - val_loss: 0.0987 - val_mean_absolute_error: 0.2982\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 416us/step - loss: 0.0279 - mean_absolute_error: 0.1480 - val_loss: 0.0985 - val_mean_absolute_error: 0.2978\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 418us/step - loss: 0.0278 - mean_absolute_error: 0.1477 - val_loss: 0.0982 - val_mean_absolute_error: 0.2973\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 417us/step - loss: 0.0277 - mean_absolute_error: 0.1474 - val_loss: 0.0979 - val_mean_absolute_error: 0.2968\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 404us/step - loss: 0.0276 - mean_absolute_error: 0.1471 - val_loss: 0.0976 - val_mean_absolute_error: 0.2964\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0274 - mean_absolute_error: 0.1468 - val_loss: 0.0973 - val_mean_absolute_error: 0.2959\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 403us/step - loss: 0.0273 - mean_absolute_error: 0.1464 - val_loss: 0.0970 - val_mean_absolute_error: 0.2954\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 408us/step - loss: 0.0272 - mean_absolute_error: 0.1461 - val_loss: 0.0966 - val_mean_absolute_error: 0.2949\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 423us/step - loss: 0.0271 - mean_absolute_error: 0.1458 - val_loss: 0.0963 - val_mean_absolute_error: 0.2944\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 401us/step - loss: 0.0270 - mean_absolute_error: 0.1455 - val_loss: 0.0960 - val_mean_absolute_error: 0.2939\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0269 - mean_absolute_error: 0.1452 - val_loss: 0.0956 - val_mean_absolute_error: 0.2933\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 424us/step - loss: 0.0268 - mean_absolute_error: 0.1448 - val_loss: 0.0953 - val_mean_absolute_error: 0.2928\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0267 - mean_absolute_error: 0.1445 - val_loss: 0.0949 - val_mean_absolute_error: 0.2922\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0266 - mean_absolute_error: 0.1442 - val_loss: 0.0945 - val_mean_absolute_error: 0.2917\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0265 - mean_absolute_error: 0.1439 - val_loss: 0.0942 - val_mean_absolute_error: 0.2911\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0263 - mean_absolute_error: 0.1435 - val_loss: 0.0938 - val_mean_absolute_error: 0.2905\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0262 - mean_absolute_error: 0.1432 - val_loss: 0.0934 - val_mean_absolute_error: 0.2899\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0261 - mean_absolute_error: 0.1429 - val_loss: 0.0930 - val_mean_absolute_error: 0.2893\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0260 - mean_absolute_error: 0.1426 - val_loss: 0.0926 - val_mean_absolute_error: 0.2887\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0259 - mean_absolute_error: 0.1422 - val_loss: 0.0922 - val_mean_absolute_error: 0.2881\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0258 - mean_absolute_error: 0.1419 - val_loss: 0.0918 - val_mean_absolute_error: 0.2875\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0257 - mean_absolute_error: 0.1416 - val_loss: 0.0914 - val_mean_absolute_error: 0.2868\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0256 - mean_absolute_error: 0.1412 - val_loss: 0.0909 - val_mean_absolute_error: 0.2862\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0254 - mean_absolute_error: 0.1409 - val_loss: 0.0905 - val_mean_absolute_error: 0.2855\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0253 - mean_absolute_error: 0.1406 - val_loss: 0.0901 - val_mean_absolute_error: 0.2849\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0252 - mean_absolute_error: 0.1402 - val_loss: 0.0896 - val_mean_absolute_error: 0.2842\n",
      "Epoch 50/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 383us/step - loss: 0.0251 - mean_absolute_error: 0.1399 - val_loss: 0.0892 - val_mean_absolute_error: 0.2835\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0250 - mean_absolute_error: 0.1395 - val_loss: 0.0887 - val_mean_absolute_error: 0.2828\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0249 - mean_absolute_error: 0.1392 - val_loss: 0.0883 - val_mean_absolute_error: 0.2821\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0248 - mean_absolute_error: 0.1388 - val_loss: 0.0878 - val_mean_absolute_error: 0.2814\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0246 - mean_absolute_error: 0.1385 - val_loss: 0.0873 - val_mean_absolute_error: 0.2807\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0245 - mean_absolute_error: 0.1381 - val_loss: 0.0869 - val_mean_absolute_error: 0.2800\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0244 - mean_absolute_error: 0.1378 - val_loss: 0.0864 - val_mean_absolute_error: 0.2792\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0243 - mean_absolute_error: 0.1374 - val_loss: 0.0859 - val_mean_absolute_error: 0.2785\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0242 - mean_absolute_error: 0.1370 - val_loss: 0.0854 - val_mean_absolute_error: 0.2777\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0240 - mean_absolute_error: 0.1367 - val_loss: 0.0849 - val_mean_absolute_error: 0.2770\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0239 - mean_absolute_error: 0.1363 - val_loss: 0.0844 - val_mean_absolute_error: 0.2762\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0238 - mean_absolute_error: 0.1359 - val_loss: 0.0839 - val_mean_absolute_error: 0.2754\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0237 - mean_absolute_error: 0.1356 - val_loss: 0.0834 - val_mean_absolute_error: 0.2747\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0236 - mean_absolute_error: 0.1352 - val_loss: 0.0829 - val_mean_absolute_error: 0.2739\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0234 - mean_absolute_error: 0.1348 - val_loss: 0.0824 - val_mean_absolute_error: 0.2731\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0233 - mean_absolute_error: 0.1344 - val_loss: 0.0818 - val_mean_absolute_error: 0.2723\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0232 - mean_absolute_error: 0.1340 - val_loss: 0.0813 - val_mean_absolute_error: 0.2714\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0231 - mean_absolute_error: 0.1336 - val_loss: 0.0808 - val_mean_absolute_error: 0.2706\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0229 - mean_absolute_error: 0.1332 - val_loss: 0.0803 - val_mean_absolute_error: 0.2698\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0228 - mean_absolute_error: 0.1328 - val_loss: 0.0797 - val_mean_absolute_error: 0.2690\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0227 - mean_absolute_error: 0.1324 - val_loss: 0.0792 - val_mean_absolute_error: 0.2681\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 391us/step - loss: 0.0225 - mean_absolute_error: 0.1320 - val_loss: 0.0787 - val_mean_absolute_error: 0.2673\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0224 - mean_absolute_error: 0.1316 - val_loss: 0.0781 - val_mean_absolute_error: 0.2664\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0223 - mean_absolute_error: 0.1312 - val_loss: 0.0776 - val_mean_absolute_error: 0.2656\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0222 - mean_absolute_error: 0.1308 - val_loss: 0.0770 - val_mean_absolute_error: 0.2647\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0220 - mean_absolute_error: 0.1303 - val_loss: 0.0765 - val_mean_absolute_error: 0.2638\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0219 - mean_absolute_error: 0.1299 - val_loss: 0.0759 - val_mean_absolute_error: 0.2629\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0218 - mean_absolute_error: 0.1295 - val_loss: 0.0754 - val_mean_absolute_error: 0.2620\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0216 - mean_absolute_error: 0.1291 - val_loss: 0.0748 - val_mean_absolute_error: 0.2612\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0215 - mean_absolute_error: 0.1286 - val_loss: 0.0743 - val_mean_absolute_error: 0.2603\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0213 - mean_absolute_error: 0.1282 - val_loss: 0.0737 - val_mean_absolute_error: 0.2594\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0212 - mean_absolute_error: 0.1277 - val_loss: 0.0732 - val_mean_absolute_error: 0.2584\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0211 - mean_absolute_error: 0.1273 - val_loss: 0.0726 - val_mean_absolute_error: 0.2575\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0209 - mean_absolute_error: 0.1268 - val_loss: 0.0720 - val_mean_absolute_error: 0.2566\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0208 - mean_absolute_error: 0.1264 - val_loss: 0.0715 - val_mean_absolute_error: 0.2557\n",
      "Epoch 85/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0207 - mean_absolute_error: 0.1259 - val_loss: 0.0709 - val_mean_absolute_error: 0.2548\n",
      "Epoch 86/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0205 - mean_absolute_error: 0.1254 - val_loss: 0.0704 - val_mean_absolute_error: 0.2538\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0204 - mean_absolute_error: 0.1250 - val_loss: 0.0698 - val_mean_absolute_error: 0.2529\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0202 - mean_absolute_error: 0.1245 - val_loss: 0.0693 - val_mean_absolute_error: 0.2520\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0201 - mean_absolute_error: 0.1240 - val_loss: 0.0687 - val_mean_absolute_error: 0.2510\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0200 - mean_absolute_error: 0.1235 - val_loss: 0.0682 - val_mean_absolute_error: 0.2501\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0198 - mean_absolute_error: 0.1231 - val_loss: 0.0676 - val_mean_absolute_error: 0.2492\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0197 - mean_absolute_error: 0.1226 - val_loss: 0.0671 - val_mean_absolute_error: 0.2482\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0196 - mean_absolute_error: 0.1221 - val_loss: 0.0666 - val_mean_absolute_error: 0.2473\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0194 - mean_absolute_error: 0.1216 - val_loss: 0.0660 - val_mean_absolute_error: 0.2463\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0193 - mean_absolute_error: 0.1211 - val_loss: 0.0655 - val_mean_absolute_error: 0.2454\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 390us/step - loss: 0.0191 - mean_absolute_error: 0.1206 - val_loss: 0.0650 - val_mean_absolute_error: 0.2445\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0190 - mean_absolute_error: 0.1201 - val_loss: 0.0644 - val_mean_absolute_error: 0.2435\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0189 - mean_absolute_error: 0.1196 - val_loss: 0.0639 - val_mean_absolute_error: 0.2426\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 351us/step - loss: 0.0187 - mean_absolute_error: 0.1191 - val_loss: 0.0634 - val_mean_absolute_error: 0.2416\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0186 - mean_absolute_error: 0.1186 - val_loss: 0.0629 - val_mean_absolute_error: 0.2407\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0184 - mean_absolute_error: 0.1181 - val_loss: 0.0624 - val_mean_absolute_error: 0.2398\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0183 - mean_absolute_error: 0.1176 - val_loss: 0.0619 - val_mean_absolute_error: 0.2388\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0182 - mean_absolute_error: 0.1171 - val_loss: 0.0614 - val_mean_absolute_error: 0.2379\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0180 - mean_absolute_error: 0.1165 - val_loss: 0.0609 - val_mean_absolute_error: 0.2370\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0179 - mean_absolute_error: 0.1160 - val_loss: 0.0604 - val_mean_absolute_error: 0.2360\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0177 - mean_absolute_error: 0.1155 - val_loss: 0.0600 - val_mean_absolute_error: 0.2351\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0176 - mean_absolute_error: 0.1150 - val_loss: 0.0595 - val_mean_absolute_error: 0.2342\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0175 - mean_absolute_error: 0.1145 - val_loss: 0.0590 - val_mean_absolute_error: 0.2333\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0173 - mean_absolute_error: 0.1140 - val_loss: 0.0586 - val_mean_absolute_error: 0.2324\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0172 - mean_absolute_error: 0.1135 - val_loss: 0.0582 - val_mean_absolute_error: 0.2315\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0171 - mean_absolute_error: 0.1130 - val_loss: 0.0577 - val_mean_absolute_error: 0.2306\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 343us/step - loss: 0.0169 - mean_absolute_error: 0.1125 - val_loss: 0.0573 - val_mean_absolute_error: 0.2297\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0168 - mean_absolute_error: 0.1119 - val_loss: 0.0569 - val_mean_absolute_error: 0.2289\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0167 - mean_absolute_error: 0.1114 - val_loss: 0.0565 - val_mean_absolute_error: 0.2280\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0166 - mean_absolute_error: 0.1109 - val_loss: 0.0561 - val_mean_absolute_error: 0.2271\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.0164 - mean_absolute_error: 0.1104 - val_loss: 0.0557 - val_mean_absolute_error: 0.2263\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0163 - mean_absolute_error: 0.1099 - val_loss: 0.0553 - val_mean_absolute_error: 0.2254\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0162 - mean_absolute_error: 0.1094 - val_loss: 0.0549 - val_mean_absolute_error: 0.2246\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0161 - mean_absolute_error: 0.1089 - val_loss: 0.0546 - val_mean_absolute_error: 0.2237\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0159 - mean_absolute_error: 0.1084 - val_loss: 0.0542 - val_mean_absolute_error: 0.2229\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 370us/step - loss: 0.0158 - mean_absolute_error: 0.1079 - val_loss: 0.0539 - val_mean_absolute_error: 0.2221\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0157 - mean_absolute_error: 0.1075 - val_loss: 0.0535 - val_mean_absolute_error: 0.2213\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 372us/step - loss: 0.0156 - mean_absolute_error: 0.1070 - val_loss: 0.0532 - val_mean_absolute_error: 0.2205\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0155 - mean_absolute_error: 0.1065 - val_loss: 0.0529 - val_mean_absolute_error: 0.2197\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0154 - mean_absolute_error: 0.1060 - val_loss: 0.0526 - val_mean_absolute_error: 0.2189\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0153 - mean_absolute_error: 0.1055 - val_loss: 0.0523 - val_mean_absolute_error: 0.2182\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0151 - mean_absolute_error: 0.1051 - val_loss: 0.0520 - val_mean_absolute_error: 0.2174\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0150 - mean_absolute_error: 0.1046 - val_loss: 0.0517 - val_mean_absolute_error: 0.2167\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.0149 - mean_absolute_error: 0.1042 - val_loss: 0.0514 - val_mean_absolute_error: 0.2159\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0148 - mean_absolute_error: 0.1037 - val_loss: 0.0512 - val_mean_absolute_error: 0.2152\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0147 - mean_absolute_error: 0.1033 - val_loss: 0.0509 - val_mean_absolute_error: 0.2145\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0146 - mean_absolute_error: 0.1029 - val_loss: 0.0506 - val_mean_absolute_error: 0.2138\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0145 - mean_absolute_error: 0.1024 - val_loss: 0.0504 - val_mean_absolute_error: 0.2131\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0144 - mean_absolute_error: 0.1020 - val_loss: 0.0502 - val_mean_absolute_error: 0.2124\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0143 - mean_absolute_error: 0.1016 - val_loss: 0.0499 - val_mean_absolute_error: 0.2117\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0143 - mean_absolute_error: 0.1012 - val_loss: 0.0497 - val_mean_absolute_error: 0.2111\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0142 - mean_absolute_error: 0.1008 - val_loss: 0.0495 - val_mean_absolute_error: 0.2104\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0141 - mean_absolute_error: 0.1004 - val_loss: 0.0493 - val_mean_absolute_error: 0.2098\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0140 - mean_absolute_error: 0.1000 - val_loss: 0.0491 - val_mean_absolute_error: 0.2091\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0139 - mean_absolute_error: 0.0996 - val_loss: 0.0489 - val_mean_absolute_error: 0.2085\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0138 - mean_absolute_error: 0.0993 - val_loss: 0.0487 - val_mean_absolute_error: 0.2079\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0138 - mean_absolute_error: 0.0989 - val_loss: 0.0485 - val_mean_absolute_error: 0.2073\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0137 - mean_absolute_error: 0.0985 - val_loss: 0.0484 - val_mean_absolute_error: 0.2067\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0136 - mean_absolute_error: 0.0982 - val_loss: 0.0482 - val_mean_absolute_error: 0.2061\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0135 - mean_absolute_error: 0.0979 - val_loss: 0.0480 - val_mean_absolute_error: 0.2055\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0135 - mean_absolute_error: 0.0975 - val_loss: 0.0479 - val_mean_absolute_error: 0.2049\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0134 - mean_absolute_error: 0.0972 - val_loss: 0.0477 - val_mean_absolute_error: 0.2044\n",
      "Epoch 148/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 383us/step - loss: 0.0133 - mean_absolute_error: 0.0969 - val_loss: 0.0476 - val_mean_absolute_error: 0.2038\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0133 - mean_absolute_error: 0.0966 - val_loss: 0.0474 - val_mean_absolute_error: 0.2033\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0132 - mean_absolute_error: 0.0963 - val_loss: 0.0473 - val_mean_absolute_error: 0.2027\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0131 - mean_absolute_error: 0.0960 - val_loss: 0.0472 - val_mean_absolute_error: 0.2022\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 373us/step - loss: 0.0131 - mean_absolute_error: 0.0957 - val_loss: 0.0470 - val_mean_absolute_error: 0.2017\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0130 - mean_absolute_error: 0.0954 - val_loss: 0.0469 - val_mean_absolute_error: 0.2012\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0130 - mean_absolute_error: 0.0951 - val_loss: 0.0468 - val_mean_absolute_error: 0.2007\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 368us/step - loss: 0.0129 - mean_absolute_error: 0.0949 - val_loss: 0.0467 - val_mean_absolute_error: 0.2002\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0129 - mean_absolute_error: 0.0946 - val_loss: 0.0465 - val_mean_absolute_error: 0.1997\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0128 - mean_absolute_error: 0.0944 - val_loss: 0.0464 - val_mean_absolute_error: 0.1992\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0128 - mean_absolute_error: 0.0941 - val_loss: 0.0463 - val_mean_absolute_error: 0.1988\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0127 - mean_absolute_error: 0.0939 - val_loss: 0.0462 - val_mean_absolute_error: 0.1983\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0127 - mean_absolute_error: 0.0936 - val_loss: 0.0461 - val_mean_absolute_error: 0.1978\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0126 - mean_absolute_error: 0.0934 - val_loss: 0.0460 - val_mean_absolute_error: 0.1974\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0126 - mean_absolute_error: 0.0932 - val_loss: 0.0459 - val_mean_absolute_error: 0.1969\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0125 - mean_absolute_error: 0.0930 - val_loss: 0.0458 - val_mean_absolute_error: 0.1965\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0125 - mean_absolute_error: 0.0928 - val_loss: 0.0457 - val_mean_absolute_error: 0.1961\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0125 - mean_absolute_error: 0.0926 - val_loss: 0.0456 - val_mean_absolute_error: 0.1957\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0124 - mean_absolute_error: 0.0924 - val_loss: 0.0455 - val_mean_absolute_error: 0.1952\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0124 - mean_absolute_error: 0.0922 - val_loss: 0.0454 - val_mean_absolute_error: 0.1948\n",
      "Epoch 168/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0123 - mean_absolute_error: 0.0920 - val_loss: 0.0453 - val_mean_absolute_error: 0.1944\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0123 - mean_absolute_error: 0.0919 - val_loss: 0.0452 - val_mean_absolute_error: 0.1940\n",
      "Epoch 170/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0123 - mean_absolute_error: 0.0917 - val_loss: 0.0452 - val_mean_absolute_error: 0.1936\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0122 - mean_absolute_error: 0.0915 - val_loss: 0.0451 - val_mean_absolute_error: 0.1933\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0122 - mean_absolute_error: 0.0914 - val_loss: 0.0450 - val_mean_absolute_error: 0.1929\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0122 - mean_absolute_error: 0.0912 - val_loss: 0.0449 - val_mean_absolute_error: 0.1925\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0121 - mean_absolute_error: 0.0911 - val_loss: 0.0448 - val_mean_absolute_error: 0.1921\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0121 - mean_absolute_error: 0.0909 - val_loss: 0.0447 - val_mean_absolute_error: 0.1918\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0121 - mean_absolute_error: 0.0908 - val_loss: 0.0447 - val_mean_absolute_error: 0.1914\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0121 - mean_absolute_error: 0.0906 - val_loss: 0.0446 - val_mean_absolute_error: 0.1911\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 393us/step - loss: 0.0120 - mean_absolute_error: 0.0905 - val_loss: 0.0445 - val_mean_absolute_error: 0.1907\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0120 - mean_absolute_error: 0.0904 - val_loss: 0.0444 - val_mean_absolute_error: 0.1904\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0120 - mean_absolute_error: 0.0903 - val_loss: 0.0443 - val_mean_absolute_error: 0.1900\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0120 - mean_absolute_error: 0.0901 - val_loss: 0.0443 - val_mean_absolute_error: 0.1897\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0119 - mean_absolute_error: 0.0900 - val_loss: 0.0442 - val_mean_absolute_error: 0.1894\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0119 - mean_absolute_error: 0.0899 - val_loss: 0.0441 - val_mean_absolute_error: 0.1891\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0119 - mean_absolute_error: 0.0898 - val_loss: 0.0440 - val_mean_absolute_error: 0.1887\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0119 - mean_absolute_error: 0.0897 - val_loss: 0.0440 - val_mean_absolute_error: 0.1884\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0118 - mean_absolute_error: 0.0896 - val_loss: 0.0439 - val_mean_absolute_error: 0.1881\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0118 - mean_absolute_error: 0.0895 - val_loss: 0.0438 - val_mean_absolute_error: 0.1878\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0118 - mean_absolute_error: 0.0894 - val_loss: 0.0437 - val_mean_absolute_error: 0.1875\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 371us/step - loss: 0.0118 - mean_absolute_error: 0.0893 - val_loss: 0.0437 - val_mean_absolute_error: 0.1872\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0118 - mean_absolute_error: 0.0892 - val_loss: 0.0436 - val_mean_absolute_error: 0.1869\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0117 - mean_absolute_error: 0.0891 - val_loss: 0.0435 - val_mean_absolute_error: 0.1866\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0117 - mean_absolute_error: 0.0890 - val_loss: 0.0434 - val_mean_absolute_error: 0.1863\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0117 - mean_absolute_error: 0.0889 - val_loss: 0.0433 - val_mean_absolute_error: 0.1860\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0117 - mean_absolute_error: 0.0888 - val_loss: 0.0433 - val_mean_absolute_error: 0.1858\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0117 - mean_absolute_error: 0.0887 - val_loss: 0.0432 - val_mean_absolute_error: 0.1855\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0116 - mean_absolute_error: 0.0887 - val_loss: 0.0431 - val_mean_absolute_error: 0.1852\n",
      "Epoch 197/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 375us/step - loss: 0.0116 - mean_absolute_error: 0.0886 - val_loss: 0.0430 - val_mean_absolute_error: 0.1849\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0116 - mean_absolute_error: 0.0885 - val_loss: 0.0430 - val_mean_absolute_error: 0.1847\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 396us/step - loss: 0.0116 - mean_absolute_error: 0.0884 - val_loss: 0.0429 - val_mean_absolute_error: 0.1844\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0116 - mean_absolute_error: 0.0884 - val_loss: 0.0428 - val_mean_absolute_error: 0.1841\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0115 - mean_absolute_error: 0.0883 - val_loss: 0.0427 - val_mean_absolute_error: 0.1839\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0115 - mean_absolute_error: 0.0882 - val_loss: 0.0427 - val_mean_absolute_error: 0.1836\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0115 - mean_absolute_error: 0.0882 - val_loss: 0.0426 - val_mean_absolute_error: 0.1834\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0115 - mean_absolute_error: 0.0881 - val_loss: 0.0425 - val_mean_absolute_error: 0.1831\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0115 - mean_absolute_error: 0.0880 - val_loss: 0.0424 - val_mean_absolute_error: 0.1829\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0115 - mean_absolute_error: 0.0880 - val_loss: 0.0424 - val_mean_absolute_error: 0.1826\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0115 - mean_absolute_error: 0.0879 - val_loss: 0.0423 - val_mean_absolute_error: 0.1824\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0114 - mean_absolute_error: 0.0878 - val_loss: 0.0422 - val_mean_absolute_error: 0.1821\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0114 - mean_absolute_error: 0.0878 - val_loss: 0.0422 - val_mean_absolute_error: 0.1819\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0114 - mean_absolute_error: 0.0877 - val_loss: 0.0421 - val_mean_absolute_error: 0.1817\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0114 - mean_absolute_error: 0.0877 - val_loss: 0.0420 - val_mean_absolute_error: 0.1814\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0114 - mean_absolute_error: 0.0876 - val_loss: 0.0419 - val_mean_absolute_error: 0.1812\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0114 - mean_absolute_error: 0.0876 - val_loss: 0.0419 - val_mean_absolute_error: 0.1810\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0113 - mean_absolute_error: 0.0875 - val_loss: 0.0418 - val_mean_absolute_error: 0.1807\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.0113 - mean_absolute_error: 0.0874 - val_loss: 0.0417 - val_mean_absolute_error: 0.1805\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0113 - mean_absolute_error: 0.0874 - val_loss: 0.0416 - val_mean_absolute_error: 0.1803\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0113 - mean_absolute_error: 0.0873 - val_loss: 0.0416 - val_mean_absolute_error: 0.1801\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0113 - mean_absolute_error: 0.0873 - val_loss: 0.0415 - val_mean_absolute_error: 0.1799\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0113 - mean_absolute_error: 0.0872 - val_loss: 0.0414 - val_mean_absolute_error: 0.1796\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0113 - mean_absolute_error: 0.0872 - val_loss: 0.0413 - val_mean_absolute_error: 0.1794\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0113 - mean_absolute_error: 0.0871 - val_loss: 0.0413 - val_mean_absolute_error: 0.1792\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0112 - mean_absolute_error: 0.0871 - val_loss: 0.0412 - val_mean_absolute_error: 0.1790\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0112 - mean_absolute_error: 0.0871 - val_loss: 0.0411 - val_mean_absolute_error: 0.1788\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 342us/step - loss: 0.0112 - mean_absolute_error: 0.0870 - val_loss: 0.0410 - val_mean_absolute_error: 0.1786\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 380us/step - loss: 0.0112 - mean_absolute_error: 0.0870 - val_loss: 0.0410 - val_mean_absolute_error: 0.1784\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 348us/step - loss: 0.0112 - mean_absolute_error: 0.0869 - val_loss: 0.0409 - val_mean_absolute_error: 0.1782\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0112 - mean_absolute_error: 0.0869 - val_loss: 0.0408 - val_mean_absolute_error: 0.1780\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0112 - mean_absolute_error: 0.0868 - val_loss: 0.0408 - val_mean_absolute_error: 0.1778\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0112 - mean_absolute_error: 0.0868 - val_loss: 0.0407 - val_mean_absolute_error: 0.1776\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 375us/step - loss: 0.0111 - mean_absolute_error: 0.0867 - val_loss: 0.0406 - val_mean_absolute_error: 0.1774\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0111 - mean_absolute_error: 0.0867 - val_loss: 0.0405 - val_mean_absolute_error: 0.1772\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 354us/step - loss: 0.0111 - mean_absolute_error: 0.0867 - val_loss: 0.0405 - val_mean_absolute_error: 0.1770\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 376us/step - loss: 0.0111 - mean_absolute_error: 0.0866 - val_loss: 0.0404 - val_mean_absolute_error: 0.1768\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 378us/step - loss: 0.0111 - mean_absolute_error: 0.0866 - val_loss: 0.0403 - val_mean_absolute_error: 0.1766\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0111 - mean_absolute_error: 0.0866 - val_loss: 0.0403 - val_mean_absolute_error: 0.1764\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0111 - mean_absolute_error: 0.0865 - val_loss: 0.0402 - val_mean_absolute_error: 0.1762\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0111 - mean_absolute_error: 0.0865 - val_loss: 0.0401 - val_mean_absolute_error: 0.1760\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0111 - mean_absolute_error: 0.0864 - val_loss: 0.0400 - val_mean_absolute_error: 0.1758\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0110 - mean_absolute_error: 0.0864 - val_loss: 0.0400 - val_mean_absolute_error: 0.1757\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0110 - mean_absolute_error: 0.0864 - val_loss: 0.0399 - val_mean_absolute_error: 0.1755\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0110 - mean_absolute_error: 0.0863 - val_loss: 0.0398 - val_mean_absolute_error: 0.1753\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0110 - mean_absolute_error: 0.0863 - val_loss: 0.0398 - val_mean_absolute_error: 0.1751\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0110 - mean_absolute_error: 0.0862 - val_loss: 0.0397 - val_mean_absolute_error: 0.1749\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 414us/step - loss: 0.0110 - mean_absolute_error: 0.0862 - val_loss: 0.0396 - val_mean_absolute_error: 0.1747\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0110 - mean_absolute_error: 0.0862 - val_loss: 0.0396 - val_mean_absolute_error: 0.1746\n",
      "Epoch 246/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 395us/step - loss: 0.0110 - mean_absolute_error: 0.0861 - val_loss: 0.0395 - val_mean_absolute_error: 0.1744\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0110 - mean_absolute_error: 0.0861 - val_loss: 0.0394 - val_mean_absolute_error: 0.1742\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0109 - mean_absolute_error: 0.0861 - val_loss: 0.0394 - val_mean_absolute_error: 0.1740\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0109 - mean_absolute_error: 0.0860 - val_loss: 0.0393 - val_mean_absolute_error: 0.1739\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 359us/step - loss: 0.0109 - mean_absolute_error: 0.0860 - val_loss: 0.0392 - val_mean_absolute_error: 0.1737\n",
      "Epoch 251/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0109 - mean_absolute_error: 0.0860 - val_loss: 0.0392 - val_mean_absolute_error: 0.1735\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0109 - mean_absolute_error: 0.0859 - val_loss: 0.0391 - val_mean_absolute_error: 0.1734\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0109 - mean_absolute_error: 0.0859 - val_loss: 0.0390 - val_mean_absolute_error: 0.1732\n",
      "Epoch 254/300\n",
      "52/52 [==============================] - 0s 362us/step - loss: 0.0109 - mean_absolute_error: 0.0859 - val_loss: 0.0390 - val_mean_absolute_error: 0.1730\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0109 - mean_absolute_error: 0.0859 - val_loss: 0.0389 - val_mean_absolute_error: 0.1729\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 385us/step - loss: 0.0109 - mean_absolute_error: 0.0858 - val_loss: 0.0388 - val_mean_absolute_error: 0.1727\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0109 - mean_absolute_error: 0.0858 - val_loss: 0.0388 - val_mean_absolute_error: 0.1725\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 358us/step - loss: 0.0108 - mean_absolute_error: 0.0858 - val_loss: 0.0387 - val_mean_absolute_error: 0.1724\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 365us/step - loss: 0.0108 - mean_absolute_error: 0.0857 - val_loss: 0.0386 - val_mean_absolute_error: 0.1722\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0108 - mean_absolute_error: 0.0857 - val_loss: 0.0386 - val_mean_absolute_error: 0.1720\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 341us/step - loss: 0.0108 - mean_absolute_error: 0.0857 - val_loss: 0.0385 - val_mean_absolute_error: 0.1719\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0108 - mean_absolute_error: 0.0856 - val_loss: 0.0385 - val_mean_absolute_error: 0.1717\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0108 - mean_absolute_error: 0.0856 - val_loss: 0.0384 - val_mean_absolute_error: 0.1716\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 345us/step - loss: 0.0108 - mean_absolute_error: 0.0856 - val_loss: 0.0383 - val_mean_absolute_error: 0.1714\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 357us/step - loss: 0.0108 - mean_absolute_error: 0.0855 - val_loss: 0.0383 - val_mean_absolute_error: 0.1712\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 379us/step - loss: 0.0108 - mean_absolute_error: 0.0855 - val_loss: 0.0382 - val_mean_absolute_error: 0.1711\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 384us/step - loss: 0.0108 - mean_absolute_error: 0.0855 - val_loss: 0.0382 - val_mean_absolute_error: 0.1709\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0108 - mean_absolute_error: 0.0855 - val_loss: 0.0381 - val_mean_absolute_error: 0.1708\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0107 - mean_absolute_error: 0.0854 - val_loss: 0.0380 - val_mean_absolute_error: 0.1706\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0107 - mean_absolute_error: 0.0854 - val_loss: 0.0380 - val_mean_absolute_error: 0.1705\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0107 - mean_absolute_error: 0.0854 - val_loss: 0.0379 - val_mean_absolute_error: 0.1703\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0107 - mean_absolute_error: 0.0853 - val_loss: 0.0379 - val_mean_absolute_error: 0.1702\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 363us/step - loss: 0.0107 - mean_absolute_error: 0.0853 - val_loss: 0.0378 - val_mean_absolute_error: 0.1700\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0107 - mean_absolute_error: 0.0853 - val_loss: 0.0377 - val_mean_absolute_error: 0.1699\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 347us/step - loss: 0.0107 - mean_absolute_error: 0.0853 - val_loss: 0.0377 - val_mean_absolute_error: 0.1697\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 349us/step - loss: 0.0107 - mean_absolute_error: 0.0852 - val_loss: 0.0376 - val_mean_absolute_error: 0.1696\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 360us/step - loss: 0.0107 - mean_absolute_error: 0.0852 - val_loss: 0.0376 - val_mean_absolute_error: 0.1694\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 366us/step - loss: 0.0107 - mean_absolute_error: 0.0852 - val_loss: 0.0375 - val_mean_absolute_error: 0.1693\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 382us/step - loss: 0.0107 - mean_absolute_error: 0.0851 - val_loss: 0.0374 - val_mean_absolute_error: 0.1691\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0107 - mean_absolute_error: 0.0851 - val_loss: 0.0374 - val_mean_absolute_error: 0.1690\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 352us/step - loss: 0.0106 - mean_absolute_error: 0.0851 - val_loss: 0.0373 - val_mean_absolute_error: 0.1689\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 411us/step - loss: 0.0106 - mean_absolute_error: 0.0851 - val_loss: 0.0373 - val_mean_absolute_error: 0.1687\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 367us/step - loss: 0.0106 - mean_absolute_error: 0.0850 - val_loss: 0.0372 - val_mean_absolute_error: 0.1686\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0106 - mean_absolute_error: 0.0850 - val_loss: 0.0372 - val_mean_absolute_error: 0.1684\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0106 - mean_absolute_error: 0.0850 - val_loss: 0.0371 - val_mean_absolute_error: 0.1683\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 355us/step - loss: 0.0106 - mean_absolute_error: 0.0850 - val_loss: 0.0371 - val_mean_absolute_error: 0.1682\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 374us/step - loss: 0.0106 - mean_absolute_error: 0.0849 - val_loss: 0.0370 - val_mean_absolute_error: 0.1680\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 353us/step - loss: 0.0106 - mean_absolute_error: 0.0849 - val_loss: 0.0369 - val_mean_absolute_error: 0.1679\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 369us/step - loss: 0.0106 - mean_absolute_error: 0.0849 - val_loss: 0.0369 - val_mean_absolute_error: 0.1677\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0106 - mean_absolute_error: 0.0848 - val_loss: 0.0368 - val_mean_absolute_error: 0.1676\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 361us/step - loss: 0.0106 - mean_absolute_error: 0.0848 - val_loss: 0.0368 - val_mean_absolute_error: 0.1675\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0106 - mean_absolute_error: 0.0848 - val_loss: 0.0367 - val_mean_absolute_error: 0.1673\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 344us/step - loss: 0.0105 - mean_absolute_error: 0.0848 - val_loss: 0.0367 - val_mean_absolute_error: 0.1672\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 364us/step - loss: 0.0105 - mean_absolute_error: 0.0847 - val_loss: 0.0366 - val_mean_absolute_error: 0.1671\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 379us/step - loss: 0.0105 - mean_absolute_error: 0.0847 - val_loss: 0.0366 - val_mean_absolute_error: 0.1669\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 346us/step - loss: 0.0105 - mean_absolute_error: 0.0847 - val_loss: 0.0365 - val_mean_absolute_error: 0.1668\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 398us/step - loss: 0.0105 - mean_absolute_error: 0.0847 - val_loss: 0.0365 - val_mean_absolute_error: 0.1667\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 356us/step - loss: 0.0105 - mean_absolute_error: 0.0846 - val_loss: 0.0364 - val_mean_absolute_error: 0.1665\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 350us/step - loss: 0.0105 - mean_absolute_error: 0.0846 - val_loss: 0.0364 - val_mean_absolute_error: 0.1664\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 351us/step - loss: 0.0105 - mean_absolute_error: 0.0846 - val_loss: 0.0363 - val_mean_absolute_error: 0.1663\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "EPOCH_NUM = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fitted = model.fit([train_X, train_decoder_input_Y], train_decoder_target_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=BATCH_SIZE,  # How many data to deal with at one epoch\n",
    "                   validation_split=0.2,\n",
    "                   verbose=1,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "source": [
    "_, timestepX, ndimX = seq_X.shape\n",
    "_, timestepY, ndimY = seq_Y.shape\n",
    "#_, ndimY = seq_Y.shape\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "# simple lstm network learning\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "2D: (batch_size, units)\n",
    "3D: (batch_size, timesteps, input_dim)\n",
    "\"\"\"\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=False,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "model.add(RepeatVector(timestepX))\n",
    "\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE,  # Network Node\n",
    "               input_shape=(timestepX, ndimX),  # Time-step, Feature Number\n",
    "               #dropout=.3,  # Drop-Out Ratio; Among the Input\n",
    "               recurrent_dropout=.3,  # Recurrent Drop-out Ratio; Among the Recurrent Network\n",
    "               return_sequences=True,  # If LSTM Returns the sequence;the same dimension of the input.\n",
    "               kernel_initializer=keras.initializers.Zeros(),\n",
    "               recurrent_initializer='zeros',\n",
    "               bias_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),\n",
    "               use_bias=True\n",
    "              ))\n",
    "\n",
    "# TimeDistributed to compare the predicted with the real one, sequence by sequence\n",
    "model.add(TimeDistributed(Dense(ndimY,  # Network Node\n",
    "                                input_shape=(ylen, ndimY),\n",
    "                                activation='linear'),))\n",
    "\n",
    "#model.add(Dense(ndimY,  # Network Node\n",
    "#                input_shape=(ylen, ndimY),  # Time-step, Feature Number\n",
    "#                activation='linear',\n",
    "#                kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None),\n",
    "#                bias_initializer=keras.initializers.Constant(value=0),))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "pprint(model.weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EPOCH_NUM = 100\n",
    "\n",
    "fitted = model.fit(seq_X, padded_seq_Y,\n",
    "                   epochs=EPOCH_NUM,     # How many times to run back_propagation\n",
    "                   batch_size=data_len,  # How many data to deal with at one epoch\n",
    "                   verbose=2,       # 1: progress bar, 2: one line per epoch\n",
    "                   #validation_data=(testX, testY),  # Validation set\n",
    "                   shuffle=False,\n",
    "                   callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pydemia/apps/anaconda3/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAADFCAYAAAB5PKoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j9WJ7FNb8JRgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefda7da7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "ax.plot(fitted.history['loss'], label='train')\n",
    "if 'val_loss' in fitted.history.keys():\n",
    "    ax.plot(fitted.history['val_loss'], label='validation')\n",
    "ax.legend()\n",
    "ax.set_xticks(np.arange(EPOCH_NUM))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 4.937 RMSE\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat_array = fitted.model.predict([train_X, train_decoder_input_Y])\n",
    "train_Y_real = np.array([scalerY.inverse_transform(Y) for Y  in train_Y])\n",
    "train_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in train_Y_hat_array])\n",
    "\n",
    "train_mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(train_Y_real, train_Y_hat)]\n",
    "train_score = np.mean(train_mse_array)\n",
    "print('Training Score: %.3f RMSE' % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_X = np.stack([np.arange(xlen) + 100,\n",
    "                  np.arange(xlen) + 101,\n",
    "                  np.arange(xlen) + 102]).T.astype('float32')\n",
    "print(new_X)\n",
    "\n",
    "test_X = scalerX.transform(new_X)\n",
    "test_X = test_X.reshape(-1, xlen, xfeature)\n",
    "print('X Shape:', test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ -1.21006191e-01,   1.76436994e-02,   4.40942764e-01,\n",
       "           1.41043104e-02],\n",
       "        [ -1.23298645e-01,   1.91401914e-02,   4.45899963e-01,\n",
       "           1.28465788e-02],\n",
       "        [ -1.25514656e-01,   2.05911603e-02,   4.50712204e-01,\n",
       "           1.16288671e-02],\n",
       "        [ -1.27657026e-01,   2.19977666e-02,   4.55385208e-01,\n",
       "           1.04500707e-02],\n",
       "        [ -1.29728496e-01,   2.33611483e-02,   4.59924966e-01,\n",
       "           9.30906553e-03],\n",
       "        [ -1.31731659e-01,   2.46824063e-02,   4.64336783e-01,\n",
       "           8.20477586e-03],\n",
       "        [ -1.33669078e-01,   2.59626433e-02,   4.68626082e-01,\n",
       "           7.13614002e-03],\n",
       "        [ -1.35543227e-01,   2.72029247e-02,   4.72797960e-01,\n",
       "           6.10210933e-03],\n",
       "        [ -1.37356460e-01,   2.84042861e-02,   4.76857275e-01,\n",
       "           5.10167331e-03],\n",
       "        [ -1.39111117e-01,   2.95677651e-02,   4.80808735e-01,\n",
       "           4.13383730e-03],\n",
       "        [ -1.40809342e-01,   3.06943450e-02,   4.84656900e-01,\n",
       "           3.19762807e-03],\n",
       "        [ -1.42453283e-01,   3.17850001e-02,   4.88406092e-01,\n",
       "           2.29212386e-03],\n",
       "        [ -1.44044995e-01,   3.28406692e-02,   4.92060393e-01,\n",
       "           1.41641009e-03],\n",
       "        [ -1.45586461e-01,   3.38622816e-02,   4.95623767e-01,\n",
       "           5.69568889e-04],\n",
       "        [ -1.47079542e-01,   3.48507389e-02,   4.99100119e-01,\n",
       "          -2.49243050e-04],\n",
       "        [ -1.48526043e-01,   3.58069018e-02,   5.02493024e-01,\n",
       "          -1.04087836e-03],\n",
       "        [ -1.49927750e-01,   3.67316231e-02,   5.05805910e-01,\n",
       "          -1.80614425e-03],\n",
       "        [ -1.51286304e-01,   3.76257263e-02,   5.09042084e-01,\n",
       "          -2.54584337e-03],\n",
       "        [ -1.52603328e-01,   3.84900235e-02,   5.12204766e-01,\n",
       "          -3.26074404e-03],\n",
       "        [ -1.53880343e-01,   3.93252745e-02,   5.15296996e-01,\n",
       "          -3.95159656e-03],\n",
       "        [ -1.55118838e-01,   4.01322655e-02,   5.18321633e-01,\n",
       "          -4.61912341e-03],\n",
       "        [ -1.56320229e-01,   4.09117006e-02,   5.21281421e-01,\n",
       "          -5.26402006e-03],\n",
       "        [ -1.57485873e-01,   4.16643061e-02,   5.24179041e-01,\n",
       "          -5.88697195e-03],\n",
       "        [ -1.58617064e-01,   4.23907638e-02,   5.27017057e-01,\n",
       "          -6.48862962e-03],\n",
       "        [ -1.59715071e-01,   4.30917591e-02,   5.29797733e-01,\n",
       "          -7.06964405e-03],\n",
       "        [ -1.60781026e-01,   4.37679328e-02,   5.32523453e-01,\n",
       "          -7.63062481e-03],\n",
       "        [ -1.61816165e-01,   4.44199219e-02,   5.35196424e-01,\n",
       "          -8.17217305e-03],\n",
       "        [ -1.62821501e-01,   4.50483412e-02,   5.37818670e-01,\n",
       "          -8.69487505e-03],\n",
       "        [ -1.63798124e-01,   4.56537679e-02,   5.40392339e-01,\n",
       "          -9.19928122e-03]], dtype=float32),\n",
       " array([[ -2.71095306e-01,   4.23558354e-02,   8.09026480e-01,\n",
       "           3.67603600e-02],\n",
       "        [ -2.77791917e-01,   4.61849570e-02,   8.23119342e-01,\n",
       "           3.36613804e-02],\n",
       "        [ -2.84368396e-01,   4.99417745e-02,   8.37034702e-01,\n",
       "           3.06326188e-02],\n",
       "        [ -2.90827572e-01,   5.36276363e-02,   8.50775898e-01,\n",
       "           2.76728421e-02],\n",
       "        [ -2.97172278e-01,   5.72439022e-02,   8.64346504e-01,\n",
       "           2.47807335e-02],\n",
       "        [ -3.03405285e-01,   6.07918724e-02,   8.77750039e-01,\n",
       "           2.19550245e-02],\n",
       "        [ -3.09529364e-01,   6.42728955e-02,   8.90989840e-01,\n",
       "           1.91944353e-02],\n",
       "        [ -3.15547228e-01,   6.76882789e-02,   9.04069543e-01,\n",
       "           1.64976660e-02],\n",
       "        [ -3.21461499e-01,   7.10392967e-02,   9.16992426e-01,\n",
       "           1.38634592e-02],\n",
       "        [ -3.27274919e-01,   7.43272603e-02,   9.29761827e-01,\n",
       "           1.12905391e-02],\n",
       "        [ -3.32989991e-01,   7.75534213e-02,   9.42381203e-01,\n",
       "           8.77763890e-03],\n",
       "        [ -3.38609219e-01,   8.07190090e-02,   9.54853654e-01,\n",
       "           6.32357225e-03],\n",
       "        [ -3.44135046e-01,   8.38252306e-02,   9.67182279e-01,\n",
       "           3.92712280e-03],\n",
       "        [ -3.49569976e-01,   8.68733078e-02,   9.79370356e-01,\n",
       "           1.58700906e-03],\n",
       "        [ -3.54916334e-01,   8.98644254e-02,   9.91421103e-01,\n",
       "          -6.97895885e-04],\n",
       "        [ -3.60176414e-01,   9.27997231e-02,   1.00333726e+00,\n",
       "          -2.92879529e-03],\n",
       "        [ -3.65352452e-01,   9.56803262e-02,   1.01512182e+00,\n",
       "          -5.10681421e-03],\n",
       "        [ -3.70446682e-01,   9.85073522e-02,   1.02677786e+00,\n",
       "          -7.23311491e-03],\n",
       "        [ -3.75461221e-01,   1.01281911e-01,   1.03830791e+00,\n",
       "          -9.30880941e-03],\n",
       "        [ -3.80398154e-01,   1.04005024e-01,   1.04971504e+00,\n",
       "          -1.13350023e-02],\n",
       "        [ -3.85259569e-01,   1.06677786e-01,   1.06100190e+00,\n",
       "          -1.33127701e-02],\n",
       "        [ -3.90047342e-01,   1.09301165e-01,   1.07217073e+00,\n",
       "          -1.52431484e-02],\n",
       "        [ -3.94763499e-01,   1.11876182e-01,   1.08322465e+00,\n",
       "          -1.71271861e-02],\n",
       "        [ -3.99409801e-01,   1.14403769e-01,   1.09416592e+00,\n",
       "          -1.89658757e-02],\n",
       "        [ -4.03988183e-01,   1.16884917e-01,   1.10499692e+00,\n",
       "          -2.07602475e-02],\n",
       "        [ -4.08500314e-01,   1.19320542e-01,   1.11572027e+00,\n",
       "          -2.25112550e-02],\n",
       "        [ -4.12947983e-01,   1.21711507e-01,   1.12633801e+00,\n",
       "          -2.42198575e-02],\n",
       "        [ -4.17332828e-01,   1.24058768e-01,   1.13685274e+00,\n",
       "          -2.58870013e-02],\n",
       "        [ -4.21656489e-01,   1.26363099e-01,   1.14726651e+00,\n",
       "          -2.75135804e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_value = encoder_model.predict(test_X)\n",
    "states_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq = np.zeros((1, timestepY, ndimY))\n",
    "target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.59817964],\n",
       "         [ 0.59973538],\n",
       "         [ 0.59055996],\n",
       "         [ 0.57424653]]], dtype=float32),\n",
       " array([[-0.13896747,  0.09012289,  0.08467691, -0.10543523]], dtype=float32),\n",
       " array([[-0.26892072,  0.17036155,  0.15765633, -0.21580139]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "states_value = [h, c]\n",
    "output_tokens, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Model within a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(latent_dim, test_X, timestepY, ndimY, encoder_inputs, encoder_states):\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    states_value = encoder_model.predict(test_X)\n",
    "    \n",
    "    samples, _, _ = test_X.shape\n",
    "    target_seq = np.zeros((samples, timestepY, ndimY))\n",
    "    \n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    states_value = [h, c]\n",
    "    output_tokens, h, c\n",
    "    \n",
    "    y_hat_array = output_tokens\n",
    "    \n",
    "    return y_hat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.59817964],\n",
       "        [ 0.59973538],\n",
       "        [ 0.59055996],\n",
       "        [ 0.57424653]],\n",
       "\n",
       "       [[ 0.60045111],\n",
       "        [ 0.60216534],\n",
       "        [ 0.59314299],\n",
       "        [ 0.57710046]],\n",
       "\n",
       "       [[ 0.60263914],\n",
       "        [ 0.60449618],\n",
       "        [ 0.59560835],\n",
       "        [ 0.57981175]],\n",
       "\n",
       "       [[ 0.60474807],\n",
       "        [ 0.60673338],\n",
       "        [ 0.59796351],\n",
       "        [ 0.58239031]],\n",
       "\n",
       "       [[ 0.60678154],\n",
       "        [ 0.60888219],\n",
       "        [ 0.60021555],\n",
       "        [ 0.58484554]],\n",
       "\n",
       "       [[ 0.60874319],\n",
       "        [ 0.61094767],\n",
       "        [ 0.60237074],\n",
       "        [ 0.58718586]],\n",
       "\n",
       "       [[ 0.61063659],\n",
       "        [ 0.61293429],\n",
       "        [ 0.60443509],\n",
       "        [ 0.58941901]],\n",
       "\n",
       "       [[ 0.61246496],\n",
       "        [ 0.61484635],\n",
       "        [ 0.60641414],\n",
       "        [ 0.59155214]],\n",
       "\n",
       "       [[ 0.61423129],\n",
       "        [ 0.61668777],\n",
       "        [ 0.60831296],\n",
       "        [ 0.59359175]],\n",
       "\n",
       "       [[ 0.61593854],\n",
       "        [ 0.61846244],\n",
       "        [ 0.61013633],\n",
       "        [ 0.59554386]],\n",
       "\n",
       "       [[ 0.61758941],\n",
       "        [ 0.62017375],\n",
       "        [ 0.61188859],\n",
       "        [ 0.59741408]],\n",
       "\n",
       "       [[ 0.61918652],\n",
       "        [ 0.6218251 ],\n",
       "        [ 0.61357379],\n",
       "        [ 0.5992074 ]],\n",
       "\n",
       "       [[ 0.62073237],\n",
       "        [ 0.62341946],\n",
       "        [ 0.61519581],\n",
       "        [ 0.60092866]],\n",
       "\n",
       "       [[ 0.62222922],\n",
       "        [ 0.62495971],\n",
       "        [ 0.61675805],\n",
       "        [ 0.6025821 ]],\n",
       "\n",
       "       [[ 0.62367922],\n",
       "        [ 0.62644863],\n",
       "        [ 0.61826384],\n",
       "        [ 0.60417169]],\n",
       "\n",
       "       [[ 0.62508452],\n",
       "        [ 0.62788874],\n",
       "        [ 0.61971623],\n",
       "        [ 0.60570127]],\n",
       "\n",
       "       [[ 0.62644708],\n",
       "        [ 0.62928236],\n",
       "        [ 0.62111807],\n",
       "        [ 0.60717416]],\n",
       "\n",
       "       [[ 0.6277687 ],\n",
       "        [ 0.6306318 ],\n",
       "        [ 0.62247205],\n",
       "        [ 0.60859364]],\n",
       "\n",
       "       [[ 0.62905109],\n",
       "        [ 0.63193905],\n",
       "        [ 0.62378055],\n",
       "        [ 0.60996264]],\n",
       "\n",
       "       [[ 0.63029611],\n",
       "        [ 0.63320631],\n",
       "        [ 0.62504596],\n",
       "        [ 0.61128396]],\n",
       "\n",
       "       [[ 0.63150519],\n",
       "        [ 0.63443512],\n",
       "        [ 0.62627041],\n",
       "        [ 0.61256003]],\n",
       "\n",
       "       [[ 0.6326797 ],\n",
       "        [ 0.63562757],\n",
       "        [ 0.62745595],\n",
       "        [ 0.61379331]],\n",
       "\n",
       "       [[ 0.63382137],\n",
       "        [ 0.63678503],\n",
       "        [ 0.62860453],\n",
       "        [ 0.61498612]],\n",
       "\n",
       "       [[ 0.63493121],\n",
       "        [ 0.63790917],\n",
       "        [ 0.62971777],\n",
       "        [ 0.61614043]],\n",
       "\n",
       "       [[ 0.63601077],\n",
       "        [ 0.63900155],\n",
       "        [ 0.63079751],\n",
       "        [ 0.61725813]],\n",
       "\n",
       "       [[ 0.637061  ],\n",
       "        [ 0.64006341],\n",
       "        [ 0.63184518],\n",
       "        [ 0.61834115]],\n",
       "\n",
       "       [[ 0.63808322],\n",
       "        [ 0.64109612],\n",
       "        [ 0.63286233],\n",
       "        [ 0.61939114]],\n",
       "\n",
       "       [[ 0.6390785 ],\n",
       "        [ 0.64210093],\n",
       "        [ 0.6338504 ],\n",
       "        [ 0.62040973]],\n",
       "\n",
       "       [[ 0.64004779],\n",
       "        [ 0.64307892],\n",
       "        [ 0.63481057],\n",
       "        [ 0.62139827]]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y_hat_array = inference_model(latent_dim, test_X, timestepY, ndimY, encoder_inputs, encoder_states)\n",
    "test_Y_hat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 7.190 RMSE\n",
      "Real\t:\n",
      " [[ 13.80000019]\n",
      " [ 14.        ]\n",
      " [ 14.19999981]],\n",
      "Predict\t:\n",
      " [[ 11.84395599]\n",
      " [ 11.87475967]\n",
      " [ 11.69308662]]\n"
     ]
    }
   ],
   "source": [
    "#test_Y_hat_array = fitted.model.predict(test_X)\n",
    "test_Y_real = np.array([scalerY.inverse_transform(Y) for Y in test_decoder_target_Y])\n",
    "test_Y_hat = np.array([scalerY.inverse_transform(Y_hat) for Y_hat in test_Y_hat_array])\n",
    "\n",
    "mse_array = [math.sqrt(mean_squared_error(Y_real, Y_hat)) for Y_real, Y_hat in zip(test_Y_real, test_Y_hat)]\n",
    "test_score = np.mean(mse_array)\n",
    "print('Test Score: %.3f RMSE' % test_score)\n",
    "print('Real\\t:\\n %s,\\nPredict\\t:\\n %s' % (test_Y_real[0][:ylen], test_Y_hat[0][:ylen]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __I’m given a big sequence (e.g. Time Series) and I split it into smaller sequences to construct my input matrix `X`. Is it possible that the LSTM may find dependencies between the sequences?__\n",
    "\n",
    "    No it’s not possible unless you go for the stateful LSTM.  \n",
    "    Most of the problems can be solved with stateless LSTM so if you go for the stateful mode, make sure you really need it.  \n",
    "    In stateless mode, long term memory does not mean that the LSTM will remember the content of the previous batches.\n",
    "    \n",
    "\n",
    "* __Why do we make the difference between stateless and stateful LSTM in Keras?__\n",
    "\n",
    "    A LSTM has cells and is therefore stateful by definition (not the same stateful meaning as used in Keras). Fabien Chollet gives this definition of statefulness:\n",
    "    > stateful: Boolean (default False).\n",
    "    > If `True`, the last state for each sample at index `i` in a batch will be used as initial state for the sample of index `i` in the following batch.\n",
    "\n",
    "    Said differently, whenever you train or test your LSTM, you first have to build your input matrix X\n",
    "    of shape `(nb_samples, timesteps, input_dim)` where your batch size divides `nb_samples`.  \n",
    "    For instance, if `nb_samples=1024` and `batch_size=64`, it means that your model will receive blocks of 64 samples,  \n",
    "    compute each output (whatever the number of `timesteps` is for every sample), average the gradients and propagate it to update the parameters vector.\n",
    "\n",
    "    By default, Keras shuffles (permutes) the samples in `X` and the dependencies between `Xi` and `Xi+1` are lost. Let’s assume there’s no shuffling in our explanation.\n",
    "\n",
    "    If the model is `stateless`, the cell states are reset at each sequence.  \n",
    "    _With the `stateful` model, **all the states are propagated to the next batch.**_  \n",
    "    It means that the state of the sample located at index `i`, `Xi` will be used in the computation of the sample `Xi+bs` in the next batch, where `bs` is the batch size (no `shuffling`).\n",
    "\n",
    "\n",
    "* __Why do Keras require the batch size in stateful mode?__\n",
    "\n",
    "    When the model is `stateless`, Keras allocates an array for the states of size `output_dim` (understand number of cells in your LSTM).  \n",
    "    At each sequence processing, this state array is reset.\n",
    "\n",
    "    __In Stateful model, Keras must propagate the previous states for each sample across the batches.__  \n",
    "    Referring to the explanation above, a sample at index `i` in batch `#1 (Xi+bs)` will know the states of the sample `i` in batch `#0 (Xi)`.  \n",
    "    In this case, the structure to store the states is of the shape `(batch_size, output_dim)`.  \n",
    "    This is the reason why you have to specify the batch size at the creation of the LSTM.  \n",
    "    If you don’t do so, Keras may raise an error to remind you: If a RNN is stateful, a complete `input_shape` must be provided (including batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "gist": {
   "data": {
    "description": "tensorflow/konlpy.ipynb",
    "public": false
   },
   "id": "5481ffd625eda4e9d4455a8d8b181ca6"
  },
  "kernelspec": {
   "display_name": "Tensorflow: Python3.6 (conda env)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "906px",
    "left": "0px",
    "right": "1789px",
    "top": "135px",
    "width": "259px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
